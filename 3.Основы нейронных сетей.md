# Основы нейронных сетей

## 1. Персептрон и решающее дерево

### Связь с методом главных компонент

Метод главных компонент ищет положение гиперплоскости, которая позволяет эффективно разделить датасет на два класса. Нормаль к этой гиперплоскости определяет главную компоненту.

Математически эта схема эквивалентна простейшему нейрону.

---

### Персептрон как простейший нейрон

Работа нейрона сводится к:

- скалярному произведению входного вектора и весов;
- добавлению смещения;
- проверке знака результата;
- применению ступенчатой функции Хэвисайда.

Правило выхода:

- результат > 0 → 1
- результат < 0 → 0

---

### Математическая модель нейрона

$$
y = \varphi(\langle \vec{w}, \vec{x} \rangle + b)
$$

где:
- $\vec{x}$ — входные параметры;
- $\vec{w}$ — веса нейрона;
- $b$ — смещение;
- $\varphi$ — функция активации.

---

### Геометрический смысл нейрона

Разделяющая гиперплоскость задаётся уравнением:

$$
\langle \vec{w}, \vec{x} \rangle + b = 0  
$$

Функция активации отображает расстояние точки до гиперплоскости (с учётом знака) в значение выхода.

Вектор $(\vec{w})$ является нормалью к гиперплоскости и направлен в сторону положительных значений выхода нейрона.

---

### Связь с решающим деревом

Такой нейрон эквивалентен однослойному решающему дереву с предварительным поворотом пространства признаков, аналогичным преобразованию методом главных компонент.

---

### Обучение персептрона

Параметры $(\vec{w})$ и $(b)$ подбираются по обучающим данным с использованием алгоритмов оптимизации.

При обучении одного нейрона:

- два коэффициента отвечают за направление нормали $(\vec{w})$;
- один коэффициент соответствует смещению $(b)$.

Для геометрической интерпретации веса нормируются по L2-норме, после чего соответствуют повороту разделяющей гиперплоскости.

---

### Итог
- персептрон реализует линейное разделение;
- соответствует элементарному решающему дереву;
- геометрически задаётся гиперплоскостью;
- является базовым элементом нейронных сетей.

---

## 2. Логические функции над логическими переменными

Показательным является использование персептрона для построения логических функций над логическими переменными. При получении коэффициентов нейрона удобно пользоваться таблицами истинности.

Рассмотрим функцию логического пересечения:
$$
X_1 \wedge X_2
$$
---

### Таблица истинности

| $X_1$ | $X_2$ | $X_1 \wedge X_2$ |
|------|------|------------------|
| 0    | 0    | 0                |
| 1    | 0    | 0                |
| 0    | 1    | 0                |
| 1    | 1    | 1                |

---

### Использование ступенчатой функции активации

Для ступенчатой функции активации справедливо:
- значение аргумента $> 0$ соответствует выходу $1$;
- значение аргумента $< 0$ соответствует выходу $0$.

Коэффициенты $W_1$, $W_2$ соответствуют входным переменным $X_1$, $X_2$.

---

### Построение системы неравенств

На основе таблицы истинности получаем систему условий:

$$
0 \cdot W_1 + 0 \cdot W_2 + B < 0
$$
$$
1 \cdot W_1 + 0 \cdot W_2 + B < 0
$$
$$
0 \cdot W_1 + 1 \cdot W_2 + B < 0
$$
$$
1 \cdot W_1 + 1 \cdot W_2 + B > 0
$$

---

### Упрощённая система

$$
B < 0
$$
$$
W_1 + B < 0
$$
$$
W_2 + B < 0
$$
$$
W_1 + W_2 + B > 0
$$

---

### Пример одного из решений

Одно из возможных решений системы:

$$
B = -1
$$
$$
W_1 = 0.6
$$
$$
W_2 = 0.6
$$

---

### Вывод

Нейрон с такими весами и ступенчатой функцией активации способен эффективно аппроксимировать логические функции над логическими переменными.

- элементарные логические операции (И, ИЛИ, НЕ) реализуются **одним нейроном**;
- для более сложных логических функций требуется **несколько нейронов**, объединённых в нейронную сеть.
---

## 3. Архитектура нейронной сети

Нейронная сеть строится из одиночных нейронов. Одиночный нейрон выполняет:
- взвешенное суммирование входных сигналов;
- вычитание смещения $d$;
- применение нелинейной функции активации к результату.

---

### Полносвязная нейронная сеть

В классической **полносвязной нейронной сети** обычно используется несколько скрытых слоёв нейронов.

Каждый нейрон слоя:
- получает на вход значения от **всех нейронов предыдущего слоя**;
- передаёт свой выход **всем нейронам следующего слоя**.

В конце сети располагается **выходной слой**, который формирует окончательное решение задачи.

---

### Сеть прямого распространения сигнала

Рассматриваемая архитектура является **нейронной сетью прямого распространения сигнала** (feed-forward network).

Она состоит из:
- **входного слоя** — принимает входные данные (рецепторы);
- **скрытых слоёв** — выполняют основные вычисления;
- **выходного слоя** — преобразует результат в требуемый формат.

Сигнал в такой сети распространяется **строго от входа к выходу**, без обратных связей.

---

### Гиперпараметры архитектуры

Архитектура нейронной сети определяется набором гиперпараметров, к которым относятся:
- количество слоёв сети;
- число нейронов в каждом слое;
- типы элементов сети;
- параметры функций активации.

Эти параметры:
- могут быть заданы заранее;
- либо подбираться отдельно в результате дополнительного поиска (spot-checking).

---

### Методы обучения нейронных сетей

Существует два основных подхода к обучению нейронных сетей.

#### Стохастические методы

К ним относятся:
- метод Монте-Карло;
- генетические алгоритмы.

Идея:
- случайным образом перебираются значения весовых коэффициентов;
- ищутся комбинации, обеспечивающие наилучшее предсказание целевых значений.

Недостаток:
- низкое быстродействие;
- при росте размерности задача становится вычислительно неэффективной.

---

#### Градиентные методы обучения

На более сложных сетях применяется:
- **метод градиентного спуска**;
- **метод обратного распространения ошибки**.

Условие применимости:
- вместо ступенчатой функции активации используется **гладкая функция**;
- производная функции активации существует и непрерывна.

Суть метода:
- вычисляются приращения всех весовых коэффициентов;
- параметры сети изменяются в направлении уменьшения ошибки;
- система последовательно движется к оптимальному разделению данных.

---

### Вывод

- нейронная сеть — композиция элементарных нейронов;
- архитектура определяет выразительную способность модели;
- для простых сетей возможны стохастические методы обучения;
- для сложных сетей используется градиентный спуск и backpropagation.
---

## 4. Функции активации

Более общим случаем нейрона является ситуация, когда результат скалярного произведения и смещения подаётся не на ступенчатую функцию Хэвисайда, а на некоторую функцию, называемую **функцией активации**.  
Она преобразует результат вычислений нейрона.

Если в качестве функции активации используется ступенька (равна 0 при отрицательном аргументе и 1 при положительном), то мы получаем **элементарный персептрон**.

---

### Общие требования к функциям активации

Функции активации могут быть достаточно произвольными, однако обычно от них требуется:

- **монотонность** — функция не убывает при росте аргумента;
- **нелинейность** (для многослойных сетей).

Монотонность связана с устойчивостью алгоритмов обучения.  
Нелинейность необходима, поскольку если все функции активации линейны, то последовательность нейронов можно заменить **одним нейроном**, и преимущество многослойных сетей исчезает.

---

### Линейные функции активации

Использование только линейной функции активации

$$
\varphi(x) = x
$$

не имеет смысла в многослойных сетях, так как несколько последовательно соединённых нейронов в этом случае эквивалентны одному.

Следовательно, **в многослойной нейронной сети обязательно должны присутствовать нелинейные функции активации**.

---

### Ступенчатая функция

Ступенчатая функция является исторически первой функцией активации и использовалась в персептронах.

$$
\varphi(x) =
\begin{cases}
1, & x > 0 \\
0, & x < 0 \\
\frac{1}{2}, & x = 0
\end{cases}
$$

Недостаток:
- функция **не является непрерывной**;
- не подходит для метода градиентного спуска.

---

### Сигмоида (логистическая функция)

Наиболее часто используемой функцией активации является **логистическая функция**:

$$
\varphi(x) = \frac{1}{1 + e^{-x}}
$$

Свойства:
- непрерывна;
- производная почти нигде не равна нулю;
- хорошо подходит для градиентных методов обучения.

---

### Сигмоида с температурой

Используется модификация сигмоиды с параметром температуры $T$:

$$
\varphi(x) = \frac{1}{1 + e^{-x / T}}
$$

Параметр $T$:
- позволяет сделать функцию более резкой;
- либо более размытой.

---

### Гиперболический тангенс

Гиперболический тангенс отличается от сигмоиды только сдвигом диапазона значений:

$$
\varphi(x) = \frac{2}{1 + e^{-2x}} - 1
$$

Свойства:
- нижняя асимптота равна $-1$;
- верхняя асимптота равна $1$.

---

### ReLU

Функция ReLU (Rectified Linear Unit):

$$
\varphi(x) =
\begin{cases}
x, & x > 0 \\
0, & x \le 0
\end{cases}
$$

Свойства:
- непрерывна;
- производная справа не равна нулю;
- слева производная равна нулю.

---

### Leaky ReLU

Модификация ReLU с ненулевым наклоном при отрицательных значениях:

$$
\varphi(x) =
\begin{cases}
x, & x > 0 \\
\alpha x, & x \le 0
\end{cases}
$$

где обычно:

$$
\alpha = 0.01
$$

Свойства:
- непрерывна;
- производная **никогда не равна нулю**.

---

### ELU

Экспоненциальная линейная функция:

$$
\varphi(x) =
\begin{cases}
x, & x > 0 \\
e^x - 1, & x \le 0
\end{cases}
$$

---

### SELU

Модификация ELU с дополнительным гиперпараметром:

$$
\varphi(x) =
\begin{cases}
x, & x > 0 \\
\alpha (e^x - 1), & x \le 0
\end{cases}
$$

где $\alpha$ — гиперпараметр.

---

### Softmax

В задачах **мультиклассовой классификации** используется функция Softmax.  
Это не функция активации одного нейрона, а функция активации **целого слоя**.

$$
\varphi_i(x) = \frac{e^{x_i}}{\sum_{j=1}^{N} e^{x_j}}
$$

Свойства:
- вход — вектор;
- выход — вектор вероятностей;
- используется нормировка между несколькими нейронами.

---

### Итог

- функция активации преобразует выход нейрона;
- в многослойных сетях обязательно используется нелинейность;
- разные функции активации применяются для разных задач;
- Softmax используется для мультиклассовой классификации.

---

## 5. Функция потерь

Для обучения нейронной сети необходимо определить, **при каких значениях коэффициентов сеть считается оптимальной**.  
Для этого вводится **функция потерь**, которая принимает минимальное значение при оптимальных параметрах сети.

Таким образом, задача обучения сводится к **поиску параметров**, при которых функция потерь минимальна.

---

### Назначение функции потерь

Функция потерь:
- численно оценивает ошибку прогноза;
- задаёт критерий оптимальности параметров сети;
- минимизируется в процессе обучения.

---

### Классы оптимизируемых функций в машинном обучении

Большинство функций в задачах обучения делятся на три категории.

#### 1. Функции потерь

Функции, позволяющие вычислить меру неточности алгоритма **на каждом конкретном примере**.

Свойства:
- обычно непрерывны;
- почти всюду дифференцируемы;
- позволяют применять градиентные методы оптимизации.

Именно **среднее значение функции потерь на датасете** оптимизируется при обучении сети.

---

#### 2. Метрики качества (метрики)

Функции, по которым анализируется качество **уже обученной модели**.

Особенности:
- обычно вычисляются на валидационном датасете;
- **не используются для поиска оптимума**;
- применяются для:
  - контроля переобучения;
  - критерия остановки;
  - сравнения моделей.

---

#### 3. Регуляризаторы

Дополнительные слагаемые в функции потерь, вводимые для:
- устранения неоднозначностей обучения;
- повышения устойчивости и скорости обучения;
- борьбы с переобучением.

Свойства:
- обычно не требуют информации о конкретных примерах;
- зависят от:
  - коэффициентов сети;
  - либо полученного решения.

---

### Оптимизация функции потерь

Обучение нейронной сети сводится к поиску параметров, минимизирующих функцию потерь.

Методы поиска:
- прямой расчёт;
- исчерпывающий перебор;
- метод Монте-Карло;
- генетические алгоритмы;
- деление отрезка пополам;
- метод Ньютона;
- **метод градиентного спуска**.

---

### Субъективность функции потерь

Функция потерь всегда:
- субъективна;
- привязана к конкретной задаче;
- отражает **стоимость ошибки** для исследователя.

Задача оптимизации — минимизировать именно эту выбранную функцию.

---

### Часто используемые функции потерь

#### Евклидово расстояние (MSE)

Наиболее простая и часто используемая функция потерь:

$$
d(\vec{x}, \vec{y}) = \sqrt{ \sum_{i=1}^{n} (x_i - y_i)^2 }
$$

Часто используется в квадрате (MSE).

---

#### Манхэттенское расстояние

Расстояние на квадратной сетке:

$$
d(\vec{x}, \vec{y}) = \sum_{i=1}^{n} |x_i - y_i|
$$

---

#### Расстояние Миньковского

Обобщение евклидова и манхэттенского расстояний:

$$
d(\vec{x}, \vec{y}) =
\left(
\sum_{i=1}^{n} |x_i - y_i|^p
\right)^{1/p}
$$

---

#### Расстояние Махалонобиса

Используется при коррелирующих координатах:

$$
d(\vec{x}, \vec{y}) =
\sqrt{(\vec{x} - \vec{y})^T S^{-1} (\vec{x} - \vec{y})}
$$

где $S$ — матрица ковариации.

---

#### Косинусное расстояние

Используется, когда важнее направление, а не длина вектора:

$$
d(\vec{x}, \vec{y}) =
\frac{\langle \vec{x}, \vec{y} \rangle}
{ \|\vec{x}\| \, \|\vec{y}\| }
$$

---

#### Intersection over Union (IoU)

Используется для связных множеств:

$$
d(X, Y) = \frac{X \wedge Y}{X \vee Y}
$$

где $X$, $Y$ — множества точек.

---

#### Dice

Используется для несвязных множеств:

$$
\text{Dice} =
\frac{ \sum (X_i Y_i) }
{ \sum X_i + \sum Y_i }
$$

---

#### Кросс-энтропия (CE)

Для дискретных событий:

$$
CE = - \sum_{x \in X} y_{orig}(x) \log \big( y_{pred}(x) \big)
$$

Для непрерывных событий:

$$
CE = - \int_{x \in X} y_{orig}(x) \log \big( y_{pred}(x) \big)\, dr(x)
$$

---

#### Бинарная кросс-энтропия (BCE)

Частный случай для двух классов:

$$
BCE = -
\big(
y_{orig} \log(y_{pred}) +
(1 - y_{orig}) \log(1 - y_{pred})
\big)
$$

---

### Замечания по использованию

- евклидово расстояние — наиболее часто используемое;
- при векторном выходе желательно **нормировать выходные значения**;
- выбор функции потерь зависит от задачи и предпочтений исследователя;
- функция потерь отражает важность различных типов ошибок.

---

### Итог

- функция потерь задаёт критерий оптимальности обучения;
- минимизация функции потерь — основная цель обучения сети;
- разные задачи требуют разных функций потерь;
- выбор функции потерь является субъективным и осознанным.
---

## 6. Обучение. Метод градиентного спуска

Количество свободных параметров в современных нейронных сетях может достигать

$$
10^6 \text{–} 10^9
$$

Для обучения таких моделей необходимо уметь вычислять зависимость выхода сети от **каждого параметра**.  
Для этого используются **метод градиентного спуска** и **метод обратного распространения ошибки**.

---

### Альтернативные методы оптимизации

Существует несколько способов оптимизации параметров решающей схемы.

#### Перебор по сетке значений
- параметры дискретизируются;
- перебираются все возможные комбинации.

Недостаток:
- крайне медленно работает при большой размерности.

---

#### Стохастические методы

К ним относятся:
- метод Монте-Карло;
- генетические алгоритмы.

Идея:
- параметры выбираются случайным образом;
- анализируется значение функции потерь.

Недостаток:
- быстрее перебора по сетке;
- но всё ещё недостаточно эффективно для больших сетей.

---

### Метод градиентного спуска

В методе градиентного спуска вычисляются **частные производные функции потерь** по каждому параметру сети.

Результат:
- формируется **градиент** — вектор приращений параметров;
- он показывает, на сколько нужно изменить каждый параметр.

Метод позволяет:
- итеративно улучшать параметры;
- находить приемлемое решение задачи оптимизации.

В настоящее время метод градиентного спуска является **основным методом обучения** нейронных сетей.

---

### Одномерный градиентный спуск

Самый простой случай — когда функция потерь зависит от **одного параметра** $w$.

Задача:
- найти точку минимума функции потерь;
- то есть точку, где производная равна нулю.

---

### Идея метода последовательных приближений

Пусть:
- зафиксирована точка $w_0$;
- функция $F(w)$ непрерывна в этой точке.

Тогда существует достаточно малое $\alpha$, при котором значение функции в точке

$$
w_1 = w_0 - \alpha \frac{dF(w)}{dw}\Big|_{w = w_0}
$$

меньше, чем в точке $w_0$.

---

### Итеративная формула градиентного спуска

В общем случае алгоритм имеет вид:

$$
w_{i+1} = w_i - \alpha \frac{\partial F(w)}{\partial w}\Big|_{w = w_i}
$$

где:
- $\alpha > 0$ — **скорость обучения**;
- $w_i$ — значение параметра на $i$-й итерации.

Обычно скорость обучения выбирается малой:

$$
\alpha \sim 10^{-3} \text{–} 10^{-5}
$$

---
### Итог

- градиентный спуск позволяет эффективно обучать модели с большим числом параметров;
- вычисляется градиент функции потерь по всем параметрам;
- параметры обновляются итеративно;
- одномерный случай является базой для понимания многомерного градиентного спуска;
- метод лежит в основе обучения современных нейронных сетей.

---

### Двумерный (многомерный) градиентный спуск

На практике число параметров нейронной сети велико, поэтому используется **многомерный градиентный спуск**.

Принцип его работы состоит в следующем:
- из текущей точки параметров делается шаг
- в направлении, **противоположном градиенту функции потерь**

Градиент указывает направление **наибольшего роста** функции, поэтому движение в
противоположную сторону приводит к уменьшению значения функции потерь.

---

### Роль скорости обучения

Важным параметром является длина шага.  
Если шаг слишком большой — минимум будет «перескочен».

Для масштабирования шага используется коэффициент $\alpha$, называемый
**скоростью обучения**.

Обычно:

$$
\alpha \sim 10^{-3} \text{–} 10^{-5}
$$

Градиент — это вектор частных производных, определяющий:
- направление максимального роста функции;
- амплитуду этого роста.

---

### Формулы многомерного градиентного спуска

Для двух параметров $w$ и $b$ алгоритм имеет вид:

$$
w_{i+1} = w_i - \alpha \frac{\partial F}{\partial w}\Big|_{w=w_i,\, b=b_i}
$$

$$
b_{i+1} = b_i - \alpha \frac{\partial F}{\partial b}\Big|_{w=w_i,\, b=b_i}
$$

Объединяя параметры в вектор

$$
\boldsymbol{\theta} = (w, b)
$$

получаем общий вид алгоритма:

$$
\boldsymbol{\theta}^{(t+1)} =
\boldsymbol{\theta}^{(t)} -
\alpha \nabla_{\boldsymbol{\theta}} F(\boldsymbol{\theta}^{(t)})
$$

---

### Метод обратного распространения ошибки

Основным способом вычисления градиента в нейронных сетях является
**метод обратного распространения ошибки (backpropagation)**.

---

### Градиент для одиночного нейрона (MSE)

Для одиночного нейрона с функцией потерь MSE:

$$
MSE(y) = L(y) = \sum \big( y - f(\langle \vec{w}, \vec{x} \rangle + b) \big)^2
$$

Производная функции потерь по выходу нейрона:

$$
\frac{dL}{df} =
2 \sum \big( y - f(\langle \vec{w}, \vec{x} \rangle + b) \big)
$$

Частные производные по параметрам:

$$
\frac{\partial L}{\partial w} =
\frac{\partial L}{\partial f} \cdot
\frac{\partial f}{\partial \text{arg}} \cdot
\frac{\partial \text{arg}}{\partial w}
$$

$$
\frac{\partial L}{\partial b} =
\frac{\partial L}{\partial f} \cdot
\frac{\partial f}{\partial \text{arg}} \cdot
\frac{\partial \text{arg}}{\partial b}
$$

Градиент нейрона — это **произведение производной функции потерь и производной функции активации**.

---

### Обратное распространение ошибки в сложной сети

Для сложной сети используется **правило цепочки**.

Интуитивно:
- если есть несколько путей распространения сигнала — производные **суммируются**;
- если элементы соединены последовательно — производные **перемножаются**.

Пример вычисления производной:

$$
\frac{\partial L}{\partial w_1} =
\left(
\frac{\partial L}{\partial y_1}
+
\frac{\partial L}{\partial y_2}
\cdot
\frac{\partial y_2}{\partial y_1}
\right)
\cdot
\frac{\partial y_1}{\partial x}
$$

где:
- $y_i$ — выходы нейронов;
- $w_1$ — параметр нейрона.

---

### Требования к функциям активации и потерь

Для корректной работы градиентного спуска требуется:
- дифференцируемость функции потерь почти везде;
- дифференцируемость функций активации почти везде.

Если производная:
- функции потерь,
- или функции активации

обращается в бесконечность или ноль, алгоритм становится некорректным.

---

### Связь с выбором функции активации

Из выражений для градиента следует:
- если $\frac{\partial f}{\partial x} = 0$, обучение останавливается;
- поэтому производная функции активации не должна обращаться в ноль, кроме точки минимума.

Это объясняет выбор функций активации:
- сигмоида;
- гиперболический тангенс;
- LeakyReLU.

---
### Итог

- градиентный спуск основан на движении против градиента функции потерь;
- backpropagation реализует вычисление градиента в сложных сетях;
- выбор функций активации напрямую влияет на сходимость обучения;
- функции активации и потерь должны подбираться совместно.
---

## 7. Улучшение скорости градиентного спуска. Оптимизаторы

Градиентный спуск является пошаговым алгоритмом, который стартует из некоторого начального состояния параметров и на каждом шаге изменяет их в направлении, пропорциональном градиенту функции потерь.

Ключевой вопрос — **как выбирать размер шага**, то есть скорость обучения $\alpha$.

---

### Проблема выбора скорости обучения

Скорость обучения:
- всегда положительна;
- не должна превышать единицу;
- традиционно выбирается достаточно малой, порядка

$$
\alpha \sim 10^{-5}
$$

В многомерном случае функция потерь часто имеет минимум в виде **узкого оврага**:
- по одним координатам она изменяется плавно;
- по другим — резко.

Проблемы:
- при слишком малой $\alpha$ обучение идёт очень медленно;
- при слишком большой $\alpha$ минимум постоянно «перескакивается».

---

### Ограничения классического градиентного спуска

Обычный градиентный спуск хорошо работает, если:
- минимум функции потерь примерно одинаков по всем координатам.

В случае узкого оврага:
- направление градиента резко меняется от итерации к итерации;
- возникает необходимость учитывать **предыдущие шаги обучения**.

---

### Идея улучшения градиентного спуска

Оптимальное решение:
- варьировать скорость обучения в зависимости от положения точки;
- ускоряться в гладких областях;
- замедляться в изрезанных областях.

Особенно важно:
- делать большие шаги в начале обучения;
- уменьшать шаг по мере приближения к минимуму.

---

## Стохастический градиентный спуск (SGD)

В методе SGD датасет разбивается на **батчи**, и градиент вычисляется:
- не по всему датасету;
- а по одному (часто случайному) батчу.

Это позволяет:
- уменьшить статистические вариации градиента;
- увеличить число итераций на одной эпохе обучения.

Алгоритм имеет вид:

$$
\boldsymbol{\theta}^{(t+1)} =
\boldsymbol{\theta}^{(t)} -
\alpha \nabla_{\boldsymbol{\theta}} f_j(\boldsymbol{\theta}^{(t)})
$$

где $f_j$ вычисляется по $j$-му батчу.

---

## Стохастический градиентный спуск с инерцией (SGDm)

В методе SGDm:
- градиент усредняется по батчам;
- дополнительно сглаживается по эпохам обучения.

Вводится вектор скорости:

$$
\vec{v}_{t+1} = \gamma \vec{v}_t + \nabla f(\boldsymbol{\theta}^{(t)})
$$

Обновление параметров:

$$
\boldsymbol{\theta}^{(t+1)} =
\boldsymbol{\theta}^{(t)} + \vec{v}_{t+1}
$$

Усреднение:
- устраняет резкие колебания градиента;
- направляет алгоритм вдоль оси оврага минимума.

---

## Метод Нестерова

Метод Нестерова отличается тем, что:
- градиент вычисляется **не в текущей точке**,
- а в точке, куда алгоритм придёт после шага.

Формулы обновления аналогичны SGDm, но градиент берётся с упреждением.

Следствие:
- более точный учёт формы минимума;
- более быстрая сходимость по сравнению с SGD.

---

## Метод адаптивного градиента (Adagrad)

Метод Adagrad масштабирует скорость обучения в зависимости от разброса градиента.

Обновление дисперсии градиента:

$$
g_{t+1} = \gamma g_t + \big( \nabla f_j(\boldsymbol{\theta}^{(t)}) \big)^2
$$

Обновление параметров:

$$
\boldsymbol{\theta}^{(t+1)} =
\boldsymbol{\theta}^{(t)} -
\frac{\alpha}{\sqrt{g_{t+1}} + \varepsilon}
\nabla f_j(\boldsymbol{\theta}^{(t)})
$$

Эффективная скорость обучения:

$$
\alpha_{\text{eff}} =
\frac{\alpha}{\sqrt{g_{t+1}} + \varepsilon}
$$

Недостаток:
- скорость обучения монотонно уменьшается.

---

## RMSprop

RMSprop устраняет недостаток Adagrad, используя **скользящее среднее**:

$$
g_{t+1} =
\gamma g_t +
(1 - \gamma)
\big( \nabla f_j(\boldsymbol{\theta}^{(t)}) \big)^2
$$

Обновление параметров:

$$
\boldsymbol{\theta}^{(t+1)} =
\boldsymbol{\theta}^{(t)} -
\frac{\alpha}{\sqrt{g_{t+1}} + \varepsilon}
\nabla f_j(\boldsymbol{\theta}^{(t)})
$$

Свойства:
- $\alpha$ уменьшается в зонах больших градиентов;
- $\alpha$ увеличивается вблизи минимума.

---

## Adadelta

Метод Adadelta объединяет идеи RMSprop и SGDm.

Используются:
- скользящее среднее квадратов градиента;
- скользящее среднее квадратов обновлений параметров.

Эффективная скорость обучения:

$$
\alpha_{\text{eff}} =
\frac{\sqrt{x_t + \varepsilon}}
{\sqrt{g_{t+1} + \varepsilon}}
$$

Это позволяет автоматически масштабировать шаг обучения.

---

## Adam

Метод Adam сочетает:
- усреднение градиента;
- усреднение дисперсии градиента.

Вводятся два параметра:
- $\gamma_1$ — масштаб усреднения градиента;
- $\gamma_2$ — масштаб усреднения дисперсии.

Формулы:

$$
m_{t+1} =
\gamma_1 m_t +
(1 - \gamma_1)
\nabla f_j(\boldsymbol{\theta}^{(t)})
$$

$$
g_{t+1} =
\gamma_2 g_t +
(1 - \gamma_2)
\big( \nabla f_j(\boldsymbol{\theta}^{(t)}) \big)^2
$$

Коррекция смещения:

$$
\hat{m}_{t+1} =
\frac{m_{t+1}}{1 - \gamma_1^{t+1}}, \quad
\hat{g}_{t+1} =
\frac{g_{t+1}}{1 - \gamma_2^{t+1}}
$$

Обновление параметров:

$$
\boldsymbol{\theta}^{(t+1)} =
\boldsymbol{\theta}^{(t)} -
\frac{\alpha \hat{m}_{t+1}}
{\sqrt{\hat{g}_{t+1}} + \varepsilon}
$$

---

## Сравнение методов оптимизации

Сравнение различных методов оптимизации показало, что **Adam**:
- обладает наилучшей скоростью сходимости;
- обеспечивает высокую скорость обучения сети.
---

### Итог

- классический градиентный спуск чувствителен к форме минимума;
- оптимизаторы адаптируют шаг обучения;
- Adam считается одним из наиболее эффективных методов;
- правильный выбор оптимизатора существенно ускоряет обучение нейронных сетей.
---

## 8. Метрика качества обучения

Метрика качества обучения нужна для того, чтобы:
- оценить, насколько близко полученное решение к идеальному;
- сравнить несколько решающих схем, оптимизированных разными способами;
- выбрать лучшую схему.

Метрик существует много, и для разных классов задач они различаются.

---

## Метрики для задач классификации

Наиболее хорошо метрики разработаны для **бинарной классификации**, когда на выходе два класса:
- негативный класс $0$
- позитивный класс $1$

В этом случае существуют:
- два типа ошибок;
- два типа правильных угадываний.

---

### Типы ошибок и правильных ответов

Определяются четыре значения:

- **FN (False Negative)** — ложно-негативный: должны предсказать $1$, предсказали $0$
- **FP (False Positive)** — ложно-позитивный: должны предсказать $0$, предсказали $1$
- **TP (True Positive)** — истинно-позитивный: должны предсказать $1$, предсказали $1$
- **TN (True Negative)** — истинно-негативный: должны предсказать $0$, предсказали $0$

Многие метрики бинарной классификации выражаются через эти четыре величины.

---

### Accuracy

Самая простая метрика — **доля правильных угадываний (accuracy)**.

Смысл:
- отношение числа правильно угаданных классов к общему числу попыток;
- максимум $1$ (всегда правильно), минимум $0$ (всегда неправильно).

Формула:

$$
ACCURACY =
\frac{TP + TN}{TP + TN + FP + FN}
$$

---

### Recall и Precision

**Recall (полнота)** — сколько позитивных объектов распознано правильно из всех реально позитивных:

$$
RECALL =
\frac{TP}{TP + FN}
$$

**Precision (точность)** — сколько из предсказанных позитивных действительно позитивные:

$$
PRECISION =
\frac{TP}{TP + FP}
$$

---

### Проблема дисбаланса классов

Метрики accuracy / precision / recall хорошо работают, когда количество примеров классов примерно одинаково.

При дисбалансе классов (например очень много нулей и очень мало единиц):
- accuracy может быть обманчивой;
- анализ по отдельности accuracy, precision и recall может не давать понимания качества.

Примерная иллюстрация:
- если модель всегда предсказывает столкновение, то на обучающем датасете может быть:
  - accuracy = 0
  - precision = 0
  - recall = 1

Из этого видно, что по отдельным метрикам качество можно интерпретировать неверно.

---

### Метрики для несбалансированных классов

Чтобы корректно работать при сильном дисбалансе, используют более устойчивые метрики.

#### F1-score

F1-score — гармоническое среднее precision и recall:

$$
F_1 =
\frac{2 \cdot PRECISION \cdot RECALL}{PRECISION + RECALL}
$$

Смысл:
- компромисс между precision и recall;
- позволяет частично компенсировать дисбаланс.

---

#### AURPC (AUC-RP)

AURPC — площадь под кривой precision–recall.

Смысл:
- на каждом этапе обучения вычисляются precision и recall;
- строится кривая (recall, precision);
- считается интеграл под этой кривой.

Рекомендуется для задач с несбалансированными классами.

---

### Confusion matrix (матрица ошибок)

В мультиклассовой классификации полезно анализировать **матрицу ошибок**:
- показывает, сколько объектов попало в каждую пару (истинный класс, предсказанный класс);
- позволяет выявить наиболее проблемные пары классов.

Для двух классов сводится к набору $TP, FP, FN, TN$.

---

## Метрики для других задач

Если задача — предсказать непрерывное значение, матрица ошибок не применяется.

Требуется мера расстояния между:
- требуемым значением;
- прогнозным значением.

В качестве таких метрик могут использоваться:
- средняя абсолютная ошибка;
- квадратичная ошибка;
- метрика Минковского;
- другие метрики над векторным пространством.

Также важно:
- часть метрик оптимизируется на максимум;
- часть — на минимум.

---

## Часто используемые метрики

### Accuracy

$$
ACCURACY =
\frac{TP + TN}{TP + TN + FP + FN}
$$

Ищется максимум.

---

### Recall

$$
RECALL =
\frac{TP}{TP + FN}
$$

Ищется максимум.

---

### Precision

$$
PRECISION =
\frac{TP}{TP + FP}
$$

Ищется максимум.

---

### F1 score

$$
F_1 =
\frac{2 \cdot PRECISION \cdot RECALL}{PRECISION + RECALL}
$$

Ищется максимум.

---

### AUC-RP (AURPC)

Площадь под кривой полнота–точность.  
Рекомендуется при несбалансированных классах. Ищется максимум.

---

### Mean Absolute Error (MAE)

Средняя сумма абсолютных отклонений:

$$
MAE =
\frac{1}{n}
\sum_{i=1}^{n}
|y_{pred} - y_{orig}|
$$

Ищется минимум.

---

### Mean Squared Error (MSE)

Средняя сумма квадратов отклонений:

$$
MSE =
\frac{1}{n}
\sum_{i=1}^{n}
(y_{pred} - y_{orig})^2
$$

Ищется минимум.

---

### Root Mean Squared Error (RMSE)

Корень из MSE:

$$
RMSE = \sqrt{MSE}
$$

Ищется минимум.

---

### Итог

- метрики нужны для оценки и сравнения обученных моделей;
- для бинарной классификации используются $TP, FP, FN, TN$;
- при дисбалансе классов предпочтительны F1-score и AURPC;
- для регрессии применяются метрики расстояния (MAE, MSE, RMSE и др.).
---

## 9. Разбиение доступных данных. Переобучение и недообучение

При машинном обучении доступный набор данных традиционно разбивается на **три выборки**:
- обучающую;
- валидационную;
- тестовую.

Каждая из них используется для своей цели.

---

## Виды выборок и их назначение

### Обучающая выборка (training set)

Используется для:
- поиска параметров обучаемой схемы;
- минимизации функции потерь;
- непосредственного обучения модели.

Именно на этой выборке:
- происходит градиентный спуск;
- обновляются веса нейронной сети.

---

### Валидационная выборка (validation set)

Используется для:
- настройки гиперпараметров сети;
- выбора архитектуры сети;
- отбора признаков;
- проверки критерия останова (early stopping);
- принятия решений, связанных с процессом обучения.

Особенности:
- **не используется для обновления весов**;
- применяется для контроля качества обучения;
- в качестве критерия обычно используется **метрика качества**.

---

### Тестовая выборка (test set)

Используется для:
- окончательной оценки качества работы алгоритма.

Ключевые свойства:
- полностью независима от обучения;
- **не влияет** на параметры, гиперпараметры и архитектуру сети;
- по ней не принимаются решения;
- используется только один раз, в самом конце.

В качестве критерия:
- применяется метрика качества.

---

## Основные проблемы обучения

При обучении решающих схем существуют две фундаментальные проблемы:
- **недообучение**;
- **переобучение**.

---

## Недообучение (underfitting)

Недообучение означает, что модель:
- не смогла выучить закономерности **даже на обучающем датасете**.

Основной признак:
- метрика качества на обучающей выборке плохая.

Основные причины недообучения:
- неудачный метод поиска параметров;
- плохо подобранные гиперпараметры;
- неудачная архитектура сети;
- некорректные критерии останова.

---

## Переобучение (overfitting)

Переобучение означает, что:
- модель хорошо работает на обучающей выборке;
- но существенно хуже работает на других данных (валидационной выборке).

Это говорит о том, что:
- модель подстроилась под особенности обучающего датасета;
- плохо обобщает новые данные.

Основные признаки переобучения:
- метрика на валидационной выборке существенно хуже, чем на обучающей;
- при увеличении числа итераций:
  - метрика на обучающей выборке улучшается;
  - метрика на валидационной выборке ухудшается.

---

## Причины переобучения

Основные причины:
- некорректные критерии останова;
- неудачный выбор гиперпараметров;
- чрезмерно сложная архитектура сети.

---

### Итог

- данные делятся на обучающую, валидационную и тестовую выборки;
- каждая выборка имеет строгое назначение;
- недообучение — плохое качество даже на обучающей выборке;
- переобучение — разрыв качества между обучающей и валидационной выборками;
- контроль этих эффектов является ключевой задачей обучения моделей.
---

## 10. Борьба с переобучением — критерии остановки

На некотором этапе обучения всегда возникает ситуация, когда:
- на обучающем датасете метрика качества продолжает улучшаться;
- на валидационных данных метрика качества начинает ухудшаться.

Такой эффект называется **переобучением**.

---

### Переобучение и ошибка обобщения

Переобучение напрямую связано с **ошибкой обобщения** — мерой того, насколько часто
обученный алгоритм ошибается на ранее неизвестных данных.

Цель обучения — минимизировать ошибку обобщения, а не только ошибку на обучающем
датасете.

---

## Основные методы борьбы с переобучением

### 1. Регуляризация

Регуляризация заключается в добавлении дополнительных слагаемых в функцию потерь,
которые:
- уменьшают зависимость между параметрами сети;
- снижают корреляцию весов;
- делают модель более устойчивой.

Особенности:
- является пассивным методом;
- **не даёт явного критерия остановки обучения**;
- снижает вероятность переобучения.

---

### 2. Достижение заданной точности

Обучение останавливается, если:
- достигнуто заранее заданное значение метрики качества.

Используется, когда заранее известно, какое качество считается достаточным.

---

### 3. Замедление скорости обучения

Обучение останавливается, если:
- изменение метрики качества от итерации к итерации становится меньше заданного порога.

Фактически:
- если обучение перестаёт давать значимый прирост качества.

---

### 4. Метод ранней остановки (Early Stopping)

Один из наиболее эффективных методов.

Суть метода:
- обучение прекращается, если функция потерь на валидационном датасете начинает
  возрастать;
- при этом функция потерь на обучающем датасете продолжает убывать.

Это означает начало переобучения.

---

### 5. Перекрёстная проверка (Cross-validation)

Метод заключается в следующем:
- данные разбиваются на $k$ частей;
- модель обучается на $k-1$ частях;
- оставшаяся часть используется для валидации;
- процедура повторяется $k$ раз.

В результате:
- каждая часть данных используется для валидации;
- получается усреднённая оценка качества модели;
- достигается более равномерное использование данных.

---

## Пример ранней остановки в TensorFlow

Ниже приведён пример реализации метода **early stopping** с использованием библиотеки
TensorFlow.

Используется классический датасет Фишера (IRIS).

```python
import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Fisher IRIS recognition dataset
iris = load_iris()

# Load data into a DataFrame
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df = df.astype(float)
df['label'] = iris.target
df['label'] = df.label.replace(dict(enumerate(iris.target_names)))

label = pd.get_dummies(df['label'], prefix='label')
df = pd.concat([df, label], axis=1)
df.drop(['label'], axis=1, inplace=True)

# get recognition data
X = df[[
    "sepal length (cm)",
    "sepal width (cm)",
    "petal length (cm)",
    "petal width (cm)"
]]
X = np.asarray(X)

y = df[[
    "label_setosa",
    "label_versicolor",
    "label_virginica"
]]
y = np.asarray(y)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20
)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

def create_model():
    model = Sequential([
        Dense(64, activation="relu", input_shape=(4,)),
        Dense(128, activation="relu"),
        Dense(128, activation="relu"),
        Dense(128, activation="relu"),
        Dense(64, activation="relu"),
        Dense(64, activation="relu"),
        Dense(64, activation="relu"),
        Dense(3, activation="softmax")
    ])
    return model

model = create_model()
model.compile(
    optimizer="adam",
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

# обучение без ранней остановки
history = model.fit(
    X_train, y_train,
    epochs=200,
    validation_split=0.25,
    batch_size=40,
    verbose=2
)

from tensorflow.keras.callbacks import EarlyStopping

# стандартная ранняя остановка
early_stopping = EarlyStopping()
history = model.fit(
    X_train, y_train,
    epochs=200,
    validation_split=0.25,
    batch_size=40,
    verbose=2,
    callbacks=[early_stopping]
)

# настраиваемая ранняя остановка
custom_early_stopping = EarlyStopping(
    monitor="val_accuracy",
    patience=8,
    min_delta=0.001,
    mode="max"
)

history = model.fit(
    X_train, y_train,
    epochs=200,
    validation_split=0.25,
    batch_size=40,
    verbose=2,
    callbacks=[custom_early_stopping]
)
```

---

### Итог

- переобучение проявляется ухудшением качества на валидационных данных;
- для борьбы используются как пассивные, так и активные методы;
- метод ранней остановки — один из наиболее практичных и эффективных;
- корректный контроль обучения критически важен для обобщающей способности модели.
---

## 11. Отбор признаков и функции регуляризации

Ещё одним способом оптимизации обучения является **отбор признаков**.

Отбор (или выделение) признаков — это процедура удаления входных параметров, которые
мало влияют на качество прогноза.

---

### Зачем нужен отбор признаков

Сокращение числа признаков необходимо по следующим причинам:

- **Значимость признаков**  
  Исходные датасеты часто содержат большое количество признаков, значения которых в
  пределах требуемой точности практически не влияют на результат.

- **Усложнение решающей схемы**  
  Увеличение числа входных признаков приводит к росту числа свободных параметров
  модели, что:
  - повышает вероятность переобучения;
  - увеличивает время обучения.

Таким образом, **снижение размерности задачи** является необходимым этапом подготовки
данных и существенно влияет на итоговый результат.

---

## Основные методы отбора признаков

Существует несколько базовых подходов к отбору значимых признаков.

---

### 1. Методы фильтрации

Методы фильтрации основаны на:
- теории вероятностей;
- статистических критериях.

Признаки ранжируются по степени их связи с целевой переменной (например, по корреляции).

Особенности:
- работают быстро;
- рассматривают признаки **изолированно**;
- не учитывают взаимное влияние признаков друг на друга.

Недостаток:
- точность моделей с таким отбором часто недостаточно высока.

Пример подобных методов рассматривался в главе о бинарных решающих деревьях.

---

### 2. Обёрточные методы (wrapper methods)

Обёрточные методы используют **саму модель прогноза** для оценки значимости признаков.

Идея:
- модель обучается на разных подмножествах признаков;
- выбирается набор, дающий наилучшее качество.

Основные стратегии:

#### Включение (forward selection)
- начинают с пустого множества признаков;
- постепенно добавляют признаки;
- процесс продолжается, пока качество улучшается.

#### Исключение (backward selection)
- начинают с полного набора признаков;
- постепенно удаляют признаки;
- процесс продолжается, пока качество не ухудшается.

Преимущества:
- учитывают взаимосвязи между признаками.

Недостатки:
- вычислительно более затратны;
- работают медленнее методов фильтрации.

---

### 3. Встроенные методы (embedded methods)

Встроенные методы:
- не разделяют отбор признаков и обучение модели;
- выделяют значимые признаки **в процессе обучения**.

Особенности:
- быстрее обёрточных методов;
- медленнее фильтрации.

Основным представителем этой группы является **регуляризация**.

---

## Регуляризация

Регуляризация заключается в добавлении дополнительных ограничений в функцию потерь,
которые:
- уменьшают зависимости между параметрами;
- сокращают число эффективно используемых переменных;
- снижают риск переобучения.

Обычно ограничения накладываются:
- на значения коэффициентов модели;
- через нормы $L_1$ или $L_2$.

---

### Общий вид регуляризованной функции потерь

Регуляризованная функция потерь имеет вид:

$$
D = d(\vec{x}, \vec{y}) + \lambda L
$$

где:
- $d(\vec{x}, \vec{y})$ — исходная (нерегуляризованная) функция потерь;
- $L$ — регуляризатор;
- $\lambda$ — коэффициент регуляризации.

---

### Регуляризация L2

$$
L = \|\vec{w}\|^2
$$

Смысл:
- минимизируется сумма квадратов весов;
- предпочтение отдаётся решениям с малыми значениями весов.

Используется для:
- сглаживания модели;
- уменьшения чувствительности к шуму.

---

### Регуляризация L1

$$
L = \|\vec{w}\|_1
$$

Смысл:
- минимизируется сумма модулей весов;
- многие веса становятся равными нулю.

Эффект:
- автоматический отбор признаков;
- уменьшение числа используемых параметров.

---

### Сглаживающие потери

В задачах 3DML используются дополнительные сглаживающие регуляризаторы, например:

$$
L = \sum (\cos \theta_{i+1})^2
$$

где $\theta$ — углы между полигонами меша.

Используются для:
- сглаживания геометрических поверхностей;
- устранения резких переходов.

---

### Итог

- отбор признаков снижает размерность задачи;
- уменьшает риск переобучения;
- ускоряет обучение;
- регуляризация является встроенным методом отбора признаков;
- L1 и L2 регуляризации применяются наиболее часто и решают разные задачи.
---

## 12. Spot-checking: поиск гиперпараметров и сравнение архитектур

При решении задач машинного обучения могут использоваться:
- различные архитектуры решающих схем;
- различные наборы гиперпараметров.

К гиперпараметрам относятся, например:
- условия обрезки решающих деревьев;
- параметры функций активации нейронов;
- количество слоёв в нейронной сети;
- типы элементов сети;
- другие настройки алгоритма обучения.

---

### Назначение spot-checking

Для автоматизации поиска оптимальной архитектуры и гиперпараметров используются
алгоритмы **spot-checking**.

Идея метода заключается в следующем:
- перебираются различные варианты решающих схем;
- перебираются различные комбинации гиперпараметров;
- все варианты оцениваются **на одном и том же обучающем и валидационном датасетах**.

Как правило:
- используемый датасет меньше полного объёма данных;
- цель — быстро сравнить большое число вариантов.

---

### Принцип работы

Процедура spot-checking включает:
1. формирование набора архитектур и гиперпараметров;
2. обучение каждой схемы;
3. вычисление выбранной метрики качества;
4. ранжирование вариантов по значению метрики.

Критерий выбора:
- чем лучше значение метрики качества, тем предпочтительнее вариант.

---

### Преимущества spot-checking

Spot-checking позволяет:
- быстро отбросить явно неэффективные архитектуры;
- исключить неудачные гиперпараметры;
- сузить пространство поиска;
- ускорить последующее обучение на полном датасете.

---

### Ограничения метода

- spot-checking не гарантирует нахождение глобального оптимума;
- качество оценки зависит от репрезентативности выбранного датасета;
- метод используется как предварительный этап подбора модели.

---

### Итог

- spot-checking — это перебор архитектур и гиперпараметров;
- сравнение проводится по выбранной метрике качества;
- используется обучающая и валидационная выборки;
- позволяет ускорить поиск эффективной модели;
- является практическим инструментом первичного анализа решений.
---

## 13. Обеспечение полноты датасета. Аугментация

При обучении решающих схем имеющийся обучающий датасет, как правило, **не покрывает
все возможные варианты входных данных**, на которых должна корректно работать система.

В реальных условиях возможны:
- искажения;
- помехи;
- наложения;
- изменения размеров;
- повороты;
- другие случайные преобразования.

Эти варианты могут отсутствовать в исходном обучающем датасете.

---

### Проблема неполного датасета

Обучать систему желательно на **полном датасете**, учитывающем все допустимые варианты
входных данных.

Если обучающий датасет неполон, то:
- при подаче на вход допустимых, но ранее не встречавшихся данных;
- решающая схема может работать некорректно.

Пример:
- при распознавании изображений;
- даже небольшое вращение изображения может привести к ошибке классификации,
  если такие варианты не встречались при обучении.

---

### Идея аугментации данных

Для повышения устойчивости системы обучающий датасет **искусственно расширяют**.

Это достигается путём:
- внесения контролируемых искажений;
- генерации новых примеров на основе уже существующих данных.

Полученный датасет:
- становится более широким;
- лучше покрывает возможные варианты входных данных;
- позволяет обучить более устойчивую модель.

---

### Определение аугментации

**Аугментация** — это способ обеспечения полноты датасета за счёт генерации
искусственных данных на основе существующих, с использованием заранее заданных
искажений.

Цель аугментации:
- приблизить обучающий датасет к реальному распределению данных;
- повысить устойчивость решающей схемы.

---

### Примеры аугментации для разных типов данных

#### Изображения

Аугментация может включать:
- геометрические искажения (повороты, масштабирование, проекции);
- изменения размеров;
- изменения яркости и цвета;
- изменения фона;
- добавление шумов;
- размытия, блики и другие помехи.

---

#### Тексты

Аугментация может включать:
- опечатки;
- изменения регистра букв;
- неверные или изменённые знаки препинания;
- вариации написания слов.

---

#### Аудиоданные

Аугментация может включать:
- изменения темпа речи;
- изменения громкости;
- добавление шумов;
- искажения тембра.

---

### Эффект аугментации

Использование аугментации:
- делает решающую систему более устойчивой;
- снижает чувствительность к типичным искажениям данных;
- уменьшает вероятность переобучения;
- повышает качество обобщения модели.

---

### Итог

- реальные данные всегда содержат искажения;
- обучающий датасет редко бывает полным;
- аугментация расширяет датасет искусственно;
- устойчивость модели напрямую зависит от используемых искажений;
- корректно подобранная аугментация существенно повышает качество обучения.
