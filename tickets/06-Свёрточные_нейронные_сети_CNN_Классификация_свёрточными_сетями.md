
---
## 6. Свёрточные нейронные сети (CNN). Классификация свёрточными сетями

Свёрточные нейронные сети (**Convolutional Neural Networks, CNN**) — это специализированный класс нейронных сетей, предназначенный для работы с данными, обладающими **пространственной структурой**. К таким данным относятся изображения, видео, карты признаков, а также некоторые типы сигналов.

Ключевая идея CNN заключается в том, что **локальные структуры важнее глобальных**, а одни и те же признаки могут встречаться в разных частях входных данных.

---

## 1. Историческая справка и мотивация

Идея свёрточных сетей была предложена **Яном Лекуном** в 1988 году при создании архитектуры **LeNet-5**, применявшейся для распознавания рукописных цифр.  
Широкое признание CNN получили после победы **AlexNet** в конкурсе ImageNet в 2012 году, где была показана принципиальная эффективность глубоких свёрточных сетей.

CNN вдохновлены биологией:
- в зрительной коре мозга есть нейроны, реагирующие на **локальные участки** поля зрения;
- рецептивные поля и иерархия признаков легли в основу архитектуры CNN.

---

## 2. Общая идея классификации свёрточной сетью

Задача классификации изображений формулируется так:

> по входному изображению определить, к какому классу оно относится.

Общая схема классификации:

```
Входное изображение  
→ [Свёртка + Активация + Пулинг] × N  
→ Flatten  
→ Полносвязные слои  
→ Softmax  
→ Класс
```

CNN автоматически выполняет:
- извлечение признаков;
- иерархическое усложнение признаков;
- классификацию на основе этих признаков.

---

## 3. Входные данные (Input Layer)

На вход CNN подаётся изображение в виде тензора.

Пример:
- размер изображения: 256 × 256;
- цветное изображение (RGB).

Форма входа:
```
[высота, ширина, каналы] = [256, 256, 3]
```

Перед подачей в сеть обычно выполняется:
- нормализация значений пикселей (например, в диапазон [0,1] или [-1,1]);
- иногда — стандартизация.

Это ускоряет обучение и улучшает сходимость градиентного спуска.

---

## 4. Извлечение признаков (Feature Extraction)

Эта часть является **ядром CNN** и состоит из повторяющихся блоков:

```
Свёртка → Функция активации → Пулинг
```

### 4.1. Свёрточный слой (Convolutional Layer)

**Назначение:** обнаружение локальных признаков.

Примеры признаков:
- на ранних слоях: края, линии, углы;
- на средних слоях: текстуры, простые формы;
- на глубоких слоях: сложные объекты (глаза, лица, детали).

#### Как работает свёртка

- по изображению «скользит» фильтр (ядро) размером, например, 3×3 или 5×5;
- фильтр умножается на соответствующий участок изображения;
- результаты суммируются → получается одно число;
- при всех положениях фильтра формируется **карта признаков (feature map)**.

Каждый фильтр обучается реагировать на **свой тип признака**.

Ключевые свойства:
- **локальные связи** (нейрон видит только часть изображения);
- **разделяемые веса** (один фильтр используется по всему изображению);
- резкое уменьшение числа параметров по сравнению с полносвязными сетями.

---

### 4.2. Функция активации

После свёртки применяется нелинейная функция активации, чаще всего **ReLU**:

```
ReLU(x) = max(0, x)
```

Назначение:
- внесение нелинейности;
- предотвращение вырождения сети в линейную модель;
- ускорение обучения.

---

### 4.3. Пулинг (Pooling Layer)

**Назначение:** уменьшение размерности карт признаков и повышение устойчивости сети.

Наиболее распространённый вариант — **MaxPooling**.

#### Пример: MaxPooling 2×2

- область 2×2 заменяется одним числом — максимумом;
- размер карты уменьшается в 2 раза по каждой оси.

Эффекты:
- снижение вычислительной сложности;
- устойчивость к небольшим сдвигам и искажениям;
- сохранение наиболее значимых признаков.

---

### 4.4. Иерархия признаков

По мере продвижения вглубь сети:
- уменьшается пространственное разрешение;
- увеличивается число каналов;
- признаки становятся более абстрактными.

Это отражает принцип:
> от простого к сложному.

---

## 5. Сглаживание (Flatten Layer)

После последнего свёрточного блока мы имеем набор 2D карт признаков.

**Flatten**:
- преобразует тензор вида `[H, W, C]` в одномерный вектор;
- подготавливает данные для полносвязных слоёв.

---

## 6. Классификация (Fully Connected Layers)

### 6.1. Полносвязные слои (Dense)

Назначение:
- объединение и интерпретация извлечённых признаков;
- обучение сложных нелинейных зависимостей между признаками.

Каждый нейрон связан со всеми нейронами предыдущего слоя.

---

### 6.2. Выходной слой (Output Layer)

- число нейронов = числу классов;
- применяется функция активации **Softmax**.

Softmax:
- преобразует логиты в вероятности;
- сумма вероятностей равна 1.

Предсказание:
- класс с максимальной вероятностью.

---

## 7. Обучение свёрточной сети

### 7.1. Прямой проход (Forward pass)

- изображение проходит через все слои;
- формируется прогноз вероятностей классов.

---

### 7.2. Функция потерь

Для классификации чаще всего используется:
- **кросс-энтропия**.

Она измеряет расхождение между:
- предсказанным распределением вероятностей;
- истинной меткой класса.

---

### 7.3. Обратное распространение ошибки (Backpropagation)

Алгоритм вычисляет:
- вклад каждого параметра (фильтра, веса) в общую ошибку;
- градиенты функции потерь по всем параметрам сети.

---

### 7.4. Оптимизаторы

Для обновления весов используются оптимизаторы:
- SGD;
- SGD с моментом;
- **Adam** (наиболее популярный для CNN).

Adam:
- адаптивно подбирает шаг обучения;
- ускоряет сходимость;
- устойчив к шумным градиентам.

---

## 8. Классические архитектуры CNN

### 8.1. LeNet-5 (1998)

- автор: Ян Лекун;
- задача: распознавание цифр;
- структура:  
  2 свёртки → 2 пулинга → 3 полносвязных слоя;
- первая успешная CNN.

---

### 8.2. AlexNet (2012)

- победа в ImageNet;
- глубокая архитектура (8 слоёв);
- ключевые инновации:
  - ReLU;
  - Dropout;
  - GPU-обучение.

---

### 8.3. VGG (2014)

- идея: много одинаковых свёрток 3×3;
- глубина: до 19 слоёв;
- высокая точность, но много параметров.

---

### 8.4. GoogLeNet / Inception (2014)

- параллельные свёртки разных размеров;
- эффективное использование параметров;
- высокая точность при меньшей сложности.

---

### 8.5. ResNet (2015)

- проблема: затухание градиента в глубоких сетях;
- решение: **остаточные связи**:
```
y = x + F(x)
```
- глубина: до 152 слоёв;
- стала стандартом для глубоких CNN.

---

## 9. Преимущества и недостатки CNN

### Преимущества
- высокая эффективность для изображений;
- автоматическое извлечение признаков;
- меньше параметров за счёт общих весов;
- хорошая обобщающая способность.

### Недостатки
- требуют больших объёмов данных;
- вычислительно затратны;
- менее эффективны для неструктурированных данных (например, чистого текста без преобразований).

---

## 10. Итог (экзаменационная формулировка)

- **Свёрточные нейронные сети** — это архитектуры, использующие локальные свёртки и общие веса для обработки пространственно структурированных данных.
- В задачах классификации CNN автоматически извлекают иерархию признаков и принимают решение через полносвязные слои.
- Обучение CNN осуществляется методом обратного распространения ошибки с использованием градиентных оптимизаторов.
- Архитектуры LeNet, AlexNet, VGG, GoogLeNet и ResNet отражают эволюцию CNN от простых моделей к глубоким и эффективным сетям.

---
## VIII. Классификация нейронной сетью  (дополнение к конспекту)

Этот раздел закрывает **классификацию нейросетью как технологию**, а не как частный трюк.  
Главная мысль: множество классов может быть большим (слова, буквы, токены), но оно **дискретно и конечно**, значит задача прогнозирования следующего элемента (слово/буква/N-грамма/слог) **редуцируется к классификации**.

---

## 0. Почему «прогноз» часто = «классификация»

Даже если классов очень много (десятки/сотни тысяч токенов), это всё равно:
- дискретное множество,
- конечное множество.

Поэтому задачи типа:
- предсказать следующую букву,
- предсказать следующее слово,
- выбрать N-грамму/слог,
сводятся к задаче:

> выбрать 1 класс из конечного набора вариантов.

---

## 1. Четыре опорных вопроса при построении классификатора нейросетью

При решении любой задачи классификации нейросетью обычно фиксируют 4 вещи:

### 1) Архитектура
Что именно соединено с чем:
- сколько слоёв,
- какие блоки (Dense/CNN/RNN и т.п.),
- как формируется решение.

Архитектура определяет **класс функций**, которые сеть вообще способна аппроксимировать.

---

### 2) Функции активации
Выбор активаций определяет:
- нелинейность модели,
- доступность градиентных методов,
- скорость/устойчивость обучения.

Ключевое требование (для градиентных методов):
- активации должны быть дифференцируемы (почти везде).

---

### 3) Функция потерь (loss)
Это функция, минимум которой означает «параметры оптимальны».  
Выбор loss — это выбор того, **что считается ошибкой** и насколько она «дорога».

---

### 4) Метрика
Нужна не всегда, а когда требуется:
- критерий остановки,
- сравнение моделей/архитектур.

При дисбалансе классов accuracy часто бесполезен → нужны F1 / AURPC.

---

## 2. Постановка задачи классификации

Дано:
- множество размеченных векторов (xᵢ, yᵢ),
- yᵢ — класс (метка).

Нужно:
- обучить решающую схему, которая по входному вектору x выдаёт класс y.

Эта постановка ранее встречалась в деревьях решений, здесь — нейросетевой вариант.

---

# 1. Двуклассовая классификация (Binary classification)

## 1.1. Архитектура

Если классы разделяются гиперплоскостью, то достаточно **одного нейрона**, потому что нейрон задаёт:
- линейную функцию,
- расстояние (со знаком) до разделяющей гиперплоскости.

Чтобы превратить это расстояние в «ответ 0/1», добавляют **функцию активации**.

В простейшем случае:
```

x → [нейрон] → sigmoid → p(class=1)

```

В более сложном бинарном случае:
- перед выходным нейроном стоит многослойная сеть,
- но финальный блок остаётся бинарным.

---

## 1.2. Функция активации: сигмоида

Выбирается функция, которая переводит выход в диапазон [0,1].  
Стандартный выбор — **логистическая функция (sigmoid)**:
$$
p = f(y) = 1 / (1 + e^(-y))
$$

Свойства, важные для обучения:
- дифференцируема,
- производная выражается через саму себя:
$$
df/dy = f(1 - f)
$$

Интерпретация:
- p ≈ вероятность принадлежности к классу 1,
- p близко к 0 → класс 0,
- p близко к 1 → класс 1.

---

## 1.3. Почему MSE неудобна для бинарной классификации с сигмоидой

Если взять MSE:
$$
MSE = Σ (f(yᵢ) - tᵢ)^2
$$

то производная содержит множитель от сигмоиды:
$$
dMSE/dy = 2 (f - t) * f(1 - f)
$$

Проблема:
- на «краях» сигмоиды ($p → 0$ или $p → 1$) множитель $f(1-f) → 0$,
- градиент становится маленьким даже при большой ошибке,
- обучение замедляется и фактически «застревает».

Это не запрет, а **плохая скорость сходимости** в задачах классификации.

---

## 1.4. Правильная связка: Sigmoid + BCE

Для бинарной классификации выгодна **бинарная кросс-энтропия (BCE)**:
$$
BCE(p, t) = - t log(p) - (1 - t) log(1 - p)
$$
где:
- $t ∈ {0,1}$ — истинная метка,
- $p = sigmoid(y)$ — прогноз.

Ключевое свойство (важнейшее):
$$
dBCE/dx = p - t
$$

То есть:
- чем дальше прогноз от цели, тем больше градиент,
- градиент не вырождается на «краях»,
- обучение идёт быстро и устойчиво.

Вывод:
> MSE не рекомендуется с сигмоидой, потому что даёт медленное обучение,  
> а BCE с сигмоидой обеспечивает быстрый градиентный спуск.

---

## 1.5. Выбор метрики в бинарной задаче

- Если классы сбалансированы → можно использовать accuracy.
- Если классы несбалансированы → accuracy становится обманчивой → нужны:
  - F1-score,
  - AURPC (area under recall-precision curve).

---

## 1.6. Итог бинарного классификатора

Бинарный классификатор нейросетью в простейшем виде:

- линейный нейрон (гиперплоскость),
- сигмоида → вероятность,
- BCE → loss,
- обучение градиентным спуском.

---

# 2. Многоклассовая классификация (Multiclass classification)

## 2.1. Архитектура

Если классов N, то можно взять N выходных нейронов, каждый из которых даёт «оценку» для своего класса.

На выходе необходимо получить:
- N вероятностей,
- сумма вероятностей = 1,
- каждый выход ∈ [0,1].

---

## 2.2. Почему нельзя просто N сигмоид

Если классы взаимоисключающие, то нужно:
- вероятность принадлежности ровно одному классу,
- сумма вероятностей должна быть 1.

Обычные независимые sigmoid этого не гарантируют.

---

## 2.3. Softmax как векторная активация

Поэтому используют **Softmax**, который нормирует выходы:

$$
SM_i = e^{y_i} / Σ_j e^{y_j}
$$
Смысл:
- $SM_i$ = вероятность i-го класса,
- $Σ SM_i$ = 1,
- все выходы в [0,1].

Softmax не локален:
- чтобы вычислить $SM_i$, нужно знать все $y_j$.

---

## 2.4. Функция потерь: Cross-Entropy (CE)

Многоклассовый аналог BCE — **кросс-энтропия (CE)**:
$$
CE(p, t) = - Σ_i (t_i * log(p_i))
$$
где:
-$t$ — one-hot вектор истинного класса (например для класса 2: [0,0,1,0,...]),
- $p_i = softmax(y)_i$ — вероятность i-го класса.

Ключевое свойство производной (главный аргумент выбора):
$$
∂CE/∂y_i = p_i - t_i
$$
Точно так же, как в бинарном случае:
- градиент равен «ошибка в вероятности»,
- не обнуляется вдали от решения,
- растёт при удалении от оптимума,
- обучение быстро сходится.

---

## 2.5. Итог многоклассового классификатора

Стандартная связка для многоклассовой классификации:

```
logits (y) → softmax → p  
loss = cross-entropy(p, t)
```

На предсказании:
- выбрать класс с максимальным p_i.

Это базовый шаблон практически для всех нейросетевых классификаторов.

---

## 3. Связь с задачами NLP (то, с чего начинался раздел)

Поскольку алфавит, словарь, набор токенов, N-грамм и т.п. конечен, то:

- предсказание следующего слова
- предсказание следующей буквы
- выбор токена из словаря

= **многоклассовая классификация**, где число классов равно размеру словаря.

Поэтому Softmax + CE — базовая схема языкового моделирования.

---
