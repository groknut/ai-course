
---
## 4. Токенизация. N-граммы и BPE

Эта тема отвечает на вопрос: **как превратить текст в дискретные элементы**, пригодные для построения признаков и обучения моделей.  
Ключевой переход:

**сырой текст → токены → числовое представление → модель**

---

## 1. Токенизация

### 1.1. Определение

**Токенизация** — это разбиение текста на элементы (**токены**), которые далее используются как:
- признаки (в Bag of Words / TF-IDF),
- элементы последовательности (в RNN/Transformer),
- элементы словаря (для эмбеддингов).

Токен — минимальная единица, с которой работает алгоритм на данном уровне:
- слово,
- символ,
- подслово (subword),
- n-грамма.

---

### 1.2. Зачем она нужна

Токенизация решает сразу несколько задач:

1) **Дискретизация текста**  
Алгоритму нужен конечный набор объектов (словарь), а не произвольная строка.

2) **Управление словарём и размерностью**  
Слишком большой словарь = разреженные признаки, память, переобучение.  
Слишком маленький = потеря смысла, ухудшение качества.

3) **Устойчивость к вариативности текста**  
Текстовые данные «шумные»: формы слов, опечатки, разные регистры, знаки препинания.

4) **Подготовка к векторизации**  
BoW/TF-IDF строятся на токенах. Эмбеддинги строятся по индексу токена.

---

### 1.3. Базовый пайплайн обработки текста

Типовая последовательность шагов:

1) Нормализация текста  
- приведение к нижнему регистру;
- стандартизация пробелов;
- обработка пунктуации (в зависимости от задачи).

2) Токенизация  
- выделение токенов (слова / подслова / символы / n-граммы).

3) (Опционально) нормализация токенов  
- лемматизация / стемминг (если используется);
- фильтрация стоп-слов (если нужно).

4) Векторизация  
- Bag of Words,
- TF-IDF,
- эмбеддинги.

---

### 1.4. Типы токенизации

#### A) Токенизация по словам (word-level)

Текст:
`"машинное обучение эффективно"`

Токены:
`["машинное", "обучение", "эффективно"]`

Плюсы:
- токен соответствует смысловой единице;
- удобно для BoW/TF-IDF и классических моделей.

Минусы:
- огромный словарь;
- разные формы слов считаются разными токенами;
- **OOV** (out-of-vocabulary): неизвестные слова вне словаря.

---

#### B) Токенизация по символам (char-level)

Текст:
`"кот"`

Токены:
`["к", "о", "т"]`

Плюсы:
- словарь минимален;
- **OOV практически исчезает** (символы известны почти всегда).

Минусы:
- последовательности длинные;
- смысл «распылён» по символам;
- модели труднее извлекать семантику.

---

#### C) Подсловная токенизация (subword-level)

Компромисс между словами и символами: токены — **части слова**.

Идея:
- редко встречающиеся слова раскладываются на части;
- часто встречающиеся части становятся отдельными токенами.

Классический представитель: **BPE**.

Плюсы:
- сильное снижение OOV;
- управляемый размер словаря;
- лучше переносится на новые слова и морфологию.

---

### 1.5. Проблема OOV и почему она принципиальна

Если словарь построен по обучающим данным, то на тесте неизбежно будут новые слова:
- имена,
- неологизмы,
- опечатки,
- редкие формы.

Word-level токенизация даёт:
- неизвестный токен `<UNK>` (теряется информация),
- или расширение словаря (растёт размерность, ломается обучение).

Subword (BPE) решает это структурно: новое слово разложится на известные подслова.

---

## 2. N-граммы

### 2.1. Определение

**N-грамма** — это последовательность из `N` подряд идущих токенов (обычно слов или символов).

- `N = 1` → unigram (униграммы)
- `N = 2` → bigram (биграммы)
- `N = 3` → trigram (триграммы)

---

### 2.2. Пример на словах

Текст:
`"машинное обучение эффективно"`

Unigram (1-граммы):
- `"машинное"`
- `"обучение"`
- `"эффективно"`

Bigram (2-граммы):
- `"машинное обучение"`
- `"обучение эффективно"`

Trigram (3-граммы):
- `"машинное обучение эффективно"`

---

### 2.3. Пример на символах

Слово: `"кот"`, 2-граммы по символам:
- `"ко"`
- `"от"`

Символьные n-граммы полезны, когда важно:
- учитывать морфологические куски,
- частично устойчиво переживать опечатки,
- моделировать «форму» слова.

---

### 2.4. Зачем нужны n-граммы

Главная причина: **Bag of Words на униграммах теряет порядок слов**.

BoW(uni) знает, что в тексте были слова A и B, но не знает:
- стояли ли они рядом,
- в каком порядке.

N-граммы вводят локальный контекст:
- фиксируют соседство,
- частично учитывают порядок.

---

### 2.5. N-граммы в Bag of Words / TF-IDF

Векторизация может строиться не по словам, а по n-граммам:

- BoW(unigram + bigram)
- TF-IDF(unigram + bigram)

Смысл:
- признаки становятся «фразовыми»,
- качество часто растёт (особенно в классификации текстов),
- но растёт размерность.

---

### 2.6. Цена n-грамм: рост пространства признаков

Если размер словаря слов `|V|`, то число возможных n-грамм в худшем случае порядка:

$$
|V|^N
$$

Практически:
- большинство n-грамм редкие → разреженность,
- рост памяти,
- рост времени обучения,
- повышенный риск переобучения.

---

### 2.7. Вывод по n-граммам

N-граммы — это:
- простой способ добавить контекст в классические модели,
- но они плохо масштабируются по размерности и данным.

Они особенно хороши:
- в BoW/TF-IDF задачах,
- при умеренном словаре и достаточном объёме данных.

---

## 3. BPE (Byte Pair Encoding)

### 3.1. Определение

**BPE (Byte Pair Encoding)** — алгоритм подсловной токенизации, в котором:
- текст сначала представляют как последовательность символов,
- затем **итеративно объединяют** самые частотные соседние пары символов/подслов,
- получая словарь подслов фиксированного размера.

Итог:
- токены — подслова (subwords),
- новые слова можно представить как комбинацию подслов.

---

### 3.2. Зачем нужен BPE

BPE решает две ключевые проблемы word-level:

1) **OOV (неизвестные слова)**  
Слово не обязательно должно быть целиком в словаре: достаточно, чтобы его части были в словаре.

2) **Размер словаря**  
Вместо сотен тысяч слов можно держать, например, 10k–50k подслов (контролируемо).

---

### 3.3. Идея BPE (механика)

1) Берём корпус и разрезаем слова на символы  
Например, добавляют маркер конца слова (в зависимости от реализации).

2) Считаем частоты всех соседних пар токенов (пока это символы)

3) Находим самую частую пару и **склеиваем** её в новый токен

4) Повторяем шаги 2–3 заданное число раз, пока не достигнем нужного размера словаря

---

### 3.4. Мини-пример (интуитивный)

Слова:
- `low`
- `lower`
- `lowest`

Старт (символы):
- `l o w`
- `l o w e r`
- `l o w e s t`

Частая пара: `l + o → lo`  
Потом часто будет: `lo + w → low`  
И т.д.

Итоговые подслова могут стать:
- `low`, `er`, `est`

Тогда:
- `lower = low + er`
- `lowest = low + est`

---

### 3.5. Свойства BPE

Плюсы:
- сильное снижение OOV;
- подслова хорошо ловят морфологию (корни/суффиксы);
- словарь контролируем;
- хорошо подходит для нейросетевых моделей и эмбеддингов.

Минусы/тонкости:
- токены становятся «техническими» (не всегда совпадают со словами);
- качество зависит от корпуса (частоты);
- разные реализации дают разные словари.

---

## 4. Сравнение подходов

### 4.1. Ключевые различия

| Подход | Единица | OOV | Словарь | Контекст |
|---|---|---:|---:|---:|
| Word-level | слово | высокий | большой | средний (через n-граммы) |
| Char-level | символ | почти нет | маленький | слабый, но устойчивый |
| N-граммы | фрагмент из N токенов | зависит | очень большой | локальный |
| **BPE** | подслово | низкий | управляемый | хороший компромисс |

---

## 5. Связь с моделями и векторизацией

### 5.1. Классический ML (BoW/TF-IDF)

- токенизация по словам или n-граммам;
- строим частотный вектор;
- обучаем классификатор/регрессор.

Смысл:
- токены = признаки.

---

### 5.2. Нейросети и эмбеддинги

- токены → индексы в словаре;
- индексы → embedding-векторы;
- дальше сеть (RNN/CNN/Transformer).

В нейросетевом NLP BPE особенно важен, потому что:
- уменьшает словарь,
- уменьшает OOV,
- улучшает переносимость на новые слова.

---

## 6. Итог (экзаменационная формулировка)

- **Токенизация** — разбиение текста на токены (слова/символы/подслова/фразы) для последующей векторизации и обучения.
- **N-граммы** — последовательности из N соседних токенов, добавляющие локальный контекст, но вызывающие рост размерности и разреженность.
- **BPE** — подсловная токенизация, которая итеративно объединяет частые пары символов/подслов, снижает OOV и контролирует размер словаря, поэтому широко используется в современных NLP-моделях.

---
