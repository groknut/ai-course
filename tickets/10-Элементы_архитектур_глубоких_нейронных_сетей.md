# 10. Элементы архитектур глубоких нейронных сетей
## 1. Блок распрямления (Flatten)

### Назначение блока Flatten
Блок **Flatten** используется для преобразования многомерных данных (матриц, тензоров)
в одномерный вектор.

Это необходимо, потому что:
- большинство **выходных задач** (классификация, регрессия)  
  требуют **векторного входа**;
- **полносвязные слои (Dense)** принимают на вход только векторы;
- предыдущие слои (Conv, Pool, RNN) работают с **матрицами или тензорами**.

Таким образом, Flatten выполняет **переход между разными типами архитектурных блоков**.

---

### Типичные входные данные

#### В задачах компьютерного зрения
Вход представляет собой **3D-тензор**:
- высота изображения
- ширина изображения
- количество каналов (RGB)

Пример:
```
[H, W, C] = [28, 28, 3]
```

#### В задачах обработки текста (NLP)
Вход представляет собой **матрицу эмбеддингов**:
- длина последовательности
- размерность эмбеддинга

Пример:
```
[sequence_length, embedding_dim] = [50, 300]
```

---

### Принцип работы Flatten

Flatten **не обучается** и **не содержит параметров**.  
Он выполняет только **изменение формы данных (reshape)**.

Математически:
```
X ∈ ℝ^{d1 × d2 × ... × dk}  
→  
x ∈ ℝ^{d1 · d2 · ... · dk}
```

Пример для изображения:
```
Input: [28, 28, 3]  
Output: [28 · 28 · 3] = [2352]
```

Все элементы просто **выстраиваются в один вектор** без изменения значений.

---

### Место Flatten в архитектуре сети

Наиболее типичное расположение:
```
Conv → ReLU → Pool → Conv → ReLU → Pool → Flatten → Dense → Output
```

Flatten **ставится между**:
- сверточными слоями и полносвязными слоями;
- рекуррентными слоями и классификатором;
- любыми матричными блоками и векторным выходом.

---

### Почему нельзя обойтись без Flatten

Полносвязный слой вычисляет:
```
y = W·x + b
```

где:
- `x` — **вектор**,
- `W` — матрица весов.

Если подать матрицу или тензор:
- операция становится некорректной;
- нарушается математическая модель слоя.

Flatten гарантирует **совместимость архитектурных блоков**.

---

### Свойства блока Flatten

**Преимущества**
- простота;
- отсутствие параметров;
- нулевая вычислительная сложность;
- необходим для большинства CNN-архитектур.

**Ограничения**
- полностью теряет пространственную структуру данных;
- после Flatten сеть перестает "понимать", где находился признак;
- поэтому Flatten **никогда не используется до конца извлечения признаков**.

---

### Важное архитектурное замечание
Flatten **не извлекает признаки**, а только подготавливает данные
для следующего этапа обработки.

Если применить Flatten слишком рано:
- сеть теряет локальные зависимости;
- резко падает качество обучения.

Поэтому Flatten всегда используется **после завершения feature extraction**.
## 2. Блоки случайного обнуления (DropOut)

### Назначение DropOut
Блоки **DropOut** предназначены для:
- уменьшения переобучения нейронной сети;
- стабилизации процесса обучения;
- снижения зависимости между нейронами и их весами.

DropOut является **методом регуляризации** и не изменяет архитектуру сети,
а влияет только на процесс обучения.

---

### Основная идея DropOut
Во время обучения нейронной сети:
- случайным образом **зануляется (обнуляется)** заданный процент выходов нейронов;
- обнуление происходит **на каждой итерации обучения по-новому**;
- нейроны временно исключаются из вычислений.

Таким образом, сеть **не может опираться на фиксированные комбинации признаков**.

---

### Механизм работы

Пусть выход слоя — вектор:
```
h = [h₁, h₂, ..., hₙ]
```

DropOut применяет случайную бинарную маску:
```
mᵢ ~ Bernoulli(p)
```

и формирует выход:
```
h'ᵢ = hᵢ · mᵢ
```

где:
- `p` — вероятность сохранения нейрона;
- `1-p` — вероятность его обнуления.

---

### DropOut как ансамблевый метод
Главная идея DropOut заключается в следующем:

> Вместо обучения **одной нейронной сети**
> происходит обучение **ансамбля сетей**, отличающихся структурой связей.

Каждая итерация обучения использует:
- **разный поднабор нейронов**;
- **разные комбинации связей**.

Это эквивалентно обучению большого ансамбля моделей
и последующему **усреднению их поведения**.

По смыслу DropOut является:
- аналогом **бэггинга**,
- но реализованным **внутри одной нейронной сети**.

---

### Интерпретация через ансамбли
- Исходная сеть без DropOut — одна модель.
- С DropOut — множество сетей, полученных
  случайным удалением связей.
- Обучение происходит одновременно на всех этих вариантах.

При этом:
- параметры весов **общие**;
- различается только активная структура сети.

---

### Поведение на обучении и инференсе

#### Во время обучения
- DropOut **включен**;
- нейроны случайно обнуляются;
- сеть вынуждена учиться устойчивым представлениям.

#### Во время работы обученной сети (inference)
- DropOut **отключается**;
- используется полная сеть;
- веса интерпретируются как **усреднённые по ансамблю**.

Таким образом, DropOut **не влияет на скорость работы** обученной модели.

---

### Влияние DropOut на переобучение
DropOut снижает вероятность:
- появления сильно коррелированных весов;
- доминирования отдельных нейронов;
- подгонки модели под шум обучающей выборки.

Он особенно эффективен:
- в полносвязных слоях;
- в глубоких сетях с большим числом параметров.

---

### Типичные значения параметра DropOut
На практике используют:
- `DropOut = 0.1–0.3` — слабая регуляризация;
- `DropOut = 0.5` — стандарт для Dense-слоев;
- `DropOut > 0.5` — редко, может приводить к недообучению.

---

### Ограничения и недостатки
- замедляет сходимость обучения;
- увеличивает дисперсию градиента;
- не всегда эффективен в сверточных слоях (часто заменяется BatchNorm).

---

### Архитектурное замечание
DropOut:
- **не извлекает признаки**;
- **не изменяет размерность данных**;
- влияет только на устойчивость обучения.

Типичное расположение:
```
Dense → ReLU → DropOut → Dense → Output
```
или
```
Conv → ReLU → Pool → Flatten → Dense → DropOut → Output
```
## 3. Батч-нормализация (Batch Normalization)

### Мотивация использования Batch Normalization
В задачах обучения нейронных сетей функция потерь часто имеет форму **вытянутого оврага**:
- по разным координатам (обучаемым параметрам сети) масштабы сильно различаются;
- градиентный спуск вынужден подбирать очень маленький шаг;
- адаптация направления и шага происходит медленно.

В такой ситуации оптимизация:
- становится нестабильной;
- сильно зависит от выбора скорости обучения;
- хуже масштабируется на глубокие сети.

Для ускорения и стабилизации обучения имеет смысл **нормализовать выходы нейронов** так,
чтобы их распределение было близко к сферическому:
- среднее значение ≈ 0;
- дисперсия ≈ 1.

Эта задача решается с помощью блока **Batch Normalization**.

---

### Определение Batch Normalization
**Batch Normalization** — это блок нейронной сети, выполняющий линейное преобразование
входного вектора таким образом, чтобы:
- каждая компонента имела нулевое среднее значение;
- каждая компонента имела единичную дисперсию.

Нормализация выполняется **по батчу** — подмножеству данных обучающей выборки.

---

### Этап 1. Нормализация по батчу
Пусть на вход блока подаётся вектор:
$$
x = (x₁, x₂, ..., xₙ)
$$
Для каждой компоненты `xᵢ` вычисляются:
- среднее значение ⟨xᵢ⟩ по батчу;
- стандартное отклонение σᵢ по батчу.

После этого выполняется нормализация:
$$
x̃ᵢ = (xᵢ − ⟨xᵢ⟩) / σᵢ
$$
В результате каждая компонента нормализованного вектора:
- имеет среднее значение, близкое к 0;
- имеет дисперсию, близкую к 1.

---

### Этап 2. Обучаемое линейное преобразование
После нормализации выполняется дополнительное линейное преобразование:
$$yᵢ = Aᵢ · x̃ᵢ + Bᵢ
$$
где:
- `Aᵢ` — коэффициент масштабирования;
- `Bᵢ` — коэффициент смещения.

Параметры `A` и `B`:
- **обучаемые**;
- подбираются в процессе градиентного спуска;
- позволяют сети восстанавливать оптимальный масштаб признаков.

Этот этап необходим, поскольку жёсткая нормализация может ухудшать выразительную
способность сети.

---

### Использование при обучении и валидации

#### Во время обучения
- средние значения ⟨xᵢ⟩ и дисперсии σᵢ считаются **по текущему батчу**;
- параметры `A` и `B` обновляются градиентным спуском.

#### Во время валидации и инференса
- используются **экспоненциально усреднённые** значения ⟨xᵢ⟩ и σᵢ;
- усреднение производится по эпохам обучения;
- применяется БИХ-фильтрация (экспоненциальное сглаживание).

Это обеспечивает стабильность работы модели на новых данных.

---

### Геометрическая интерпретация
Batch Normalization:
- компенсирует вытянутость оврага функции потерь;
- делает поверхность ошибки более симметричной;
- приближает форму минимума к сфере.

В результате:
- градиентный спуск становится более устойчивым;
- уменьшается чувствительность к выбору шага обучения;
- ускоряется сходимость.

---

### Влияние на обучение сети
Использование Batch Normalization приводит к:
- уменьшению корреляции между параметрами слоёв;
- более независимому обучению слоёв;
- снижению эффекта «внутреннего ковариационного сдвига».

Это особенно важно в глубоких нейронных сетях.

---

### Архитектурное расположение
Batch Normalization обычно располагается:
- после линейного преобразования;
- перед функцией активации.

Типовая схема:
```
Linear / Conv → BatchNorm → Activation
```

---

### Итог
Batch Normalization выполняет линейное преобразование входного вектора таким образом, чтобы:
- распределение выходных значений слоя было стабильным;
- оно слабо зависело от параметров предыдущего слоя;
- форма минимума функции потерь была менее вытянутой.

Это позволяет ускорить обучение, повысить устойчивость сети и упростить настройку гиперпараметров.
## 4. Блоки объединения (Pooling)

### Назначение блоков Pooling
При обработке данных в виде матриц (изображения, карты признаков) важной задачей является
**уменьшение их размерности**. Сокращение размеров матриц в процессе обучения позволяет:

- уменьшить количество свободных параметров сети;
- повысить устойчивость обучения;
- ускорить вычисления;
- снизить вероятность переобучения.

Одним из базовых способов уменьшения размерности является **пулинг (Pooling)**.

---

### Определение пулинга
**Пулинг** — это операция, которая:
- разбивает входную матрицу на подматрицы фиксированного размера;
- для каждой подматрицы вычисляет одно числовое значение;
- формирует новую матрицу меньшего размера из полученных значений.

Таким образом, происходит **агрегация локальной информации**.

---

### Основные типы пулинга

#### Max Pooling
Для каждой подматрицы выбирается **максимальный элемент**.

Используется чаще всего, поскольку:
- хорошо сохраняет наиболее выраженные признаки;
- повышает устойчивость к небольшим сдвигам входных данных;
- эффективно работает в сверточных сетях.

#### Min Pooling
Для каждой подматрицы выбирается **минимальный элемент**.

Используется редко, в специальных задачах.

#### Average Pooling (AvgPool)
Для каждой подматрицы вычисляется **среднее значение**.

Используется, когда:
- важна общая статистика области;
- требуется сгладить локальные шумы.

---

### Принцип работы пулинга
Пусть дана входная матрица размера `H × W`.

1. Выбирается размер окна пулинга, например `k × k`.
2. Матрица разбивается на неперекрывающиеся (или частично перекрывающиеся) подматрицы.
3. Для каждой подматрицы применяется выбранная операция (max, min, average).
4. Полученные значения формируют новую матрицу меньшего размера.

---

### Padding при пулинге
При необходимости по краям входной матрицы может быть добавлено дополнение нулями
(**padding**), чтобы:
- сохранить целочисленное число окон;
- контролировать размер выходной матрицы.

Padding позволяет корректно применять пулинг даже тогда, когда размеры матрицы не кратны
размеру окна.

---

### Иллюстрация Max Pooling
На Рис.41 показан принцип работы операции **Max Pooling**:

- входная матрица разбивается на окна фиксированного размера;
- из каждого окна выбирается максимальное значение;
- на выходе формируется уменьшенная матрица.

(Рис.41 — пояснение работы пулинга)

---

### Геометрический и смысловой эффект
Использование пулинга приводит к следующим эффектам:

- уменьшение пространственного разрешения карт признаков;
- сохранение наиболее значимых локальных признаков;
- повышение инвариантности к сдвигам и небольшим искажениям входных данных.

Это особенно важно при обработке изображений.

---

### Место в архитектуре нейронной сети
Блоки Pooling обычно используются:
- после сверточных слоёв;
- до следующего сверточного слоя или перед Flatten.

Типовая схема:
```
Conv → Activation → Pool → Conv → Activation → Pool → ...
```

---

### Итог
Блоки объединения (Pooling):
- уменьшают размерность данных;
- ускоряют обучение;
- повышают устойчивость сети;
- снижают количество параметров.

Пулинг является одним из ключевых элементов архитектуры сверточных нейронных сетей.
## 5. Блоки свёртки (Convolution, Conv) и сверточная сеть

### Назначение сверточных блоков
**Сверточные блоки (Conv)** предназначены для извлечения локальных признаков из
матричных данных с учётом их пространственной структуры и корреляций между элементами.

В отличие от пулинга, где выполняется простая агрегирующая операция (max, average),
свертка представляет собой **взвешенное суммирование элементов подматрицы** с набором
обучаемых коэффициентов.

---

### Математический смысл свёртки
Свертка имеет строгий математический смысл и определяется как интегральная операция:
$$
U_{выход}(R) = ∫ U_{вход}( R − r ) · K( r ) dr
$$
где:
- $U_{вход}$ — входная матрица (изображение, карта признаков);
- $U_{выход}$ — результат свёртки;
- $K( r )$ — ядро свёртки (фильтр);
- $R, r$ — координаты элементов матрицы и ядра.

В дискретном (матричном) виде эта операция сводится к:
- поэлементному умножению подматрицы входных данных на ядро;
- суммированию результатов.

Параметры ядра **K** являются **обучаемыми**, и их поиск эквивалентен решению задачи
линейной регрессии, рассмотренной ранее.

---

### Одномерная свёртка
В одномерном случае:
- ядро перемещается вдоль одного измерения;
- используется, например, при обработке временных рядов или последовательностей.

Схема работы одномерной свёртки показана на Рис.42.

(Рис.42 — схема работы сверточного элемента в одномерном случае)

---

### Двумерная свёртка
В двумерном случае:
- ядро имеет прямоугольную форму;
- перемещение происходит по двум направлениям (по строкам и столбцам);
- результатом является двумерная матрица — карта признаков.

Это основной режим работы свёрточных сетей при анализе изображений.

Схема работы двумерной свёртки приведена на Рис.43.

(Рис.43 — схема работы сверточного элемента в двумерном случае)

---

### Интерпретация свёртки
Сверточный слой:
- интегрирует информацию из локальной окрестности;
- учитывает корреляции между соседними элементами;
- выделяет характерные структуры (границы, текстуры, формы).

Благодаря этому свёртка эффективно используется для **извлечения признаков**, но не
для непосредственного принятия решения.

---

### Роль сверточных слоев в архитектуре
Как правило:
- сверточные слои используются для выделения признаков;
- окончательное решение принимается следующими слоями;
- чаще всего — полносвязной нейронной сетью.

Типовая структура сверточной сети:
```
Conv → Activation → Pool → Conv → Activation → Pool → Flatten → Dense → Output
```

Принципиальная схема сверточной сети для классификации показана на Рис.44.

(Рис.44 — принципиальная схема сверточной сети для решения задачи классификации)

---

### Основные параметры свёртки

#### Padding
При стандартной свёртке размер выходной матрицы **уменьшается** по сравнению со входной.
Для управления размером используется параметр **padding**.

Padding — это добавление по краям входной матрицы дополнительных ячеек (обычно нулей).

Назначение padding:
- сохранить исходный размер матрицы;
- увеличить размер выходной матрицы;
- корректно обрабатывать граничные элементы.

Иллюстрация padding приведена на Рис.45.

(Рис.45 — принцип работы padding)

---

#### Stride
**Stride** — это шаг смещения ядра при его перемещении по входной матрице.

Если stride > 1:
- ядро перемещается с пропуском элементов;
- размер выходной матрицы уменьшается;
- сокращается число вычислений.

Stride используется:
- для уменьшения размерности данных;
- для ускорения обучения;
- при анализе дальних зависимостей.

Иллюстрация работы stride показана на Рис.46.

(Рис.46 — принцип работы stride)

---

#### Dilation
**Dilation** — параметр, позволяющий учитывать дальние корреляции без увеличения числа
параметров.

Суть метода:
- ядро становится разреженным;
- между его ненулевыми элементами вставляются пропуски;
- свертка учитывает более удалённые элементы входной матрицы.

Преимущества dilation:
- увеличение эффективного радиуса восприятия;
- снижение вычислительной сложности;
- возможность моделирования дальних зависимостей.

Ядро при dilation содержит малое число ненулевых коэффициентов, остальные элементы —
фиктивные нули.

Иллюстрация ядра с dilation приведена на Рис.47.

(Рис.47 — пример разреженного ядра с dilation)

---
### Итог
Сверточные блоки:
- реализуют локальную линейную регрессию с общими весами;
- эффективно извлекают признаки из матричных данных;
- учитывают пространственные корреляции;
- являются основой сверточных нейронных сетей.

Параметры padding, stride и dilation позволяют гибко управлять:
- размером выходных данных;
- масштабом анализируемых корреляций;
- вычислительной сложностью сети.
# IX. Элементы архитектур глубоких нейронных сетей

Глубокие нейронные сети строятся из стандартных архитектурных блоков.
Каждый блок решает свою задачу:
- преобразование формы данных,
- стабилизация обучения,
- уменьшение размерности,
- извлечение признаков,
- ускорение и регуляризация обучения.

Ниже приведены основные элементы, используемые в современных CNN и DNN.

---

## 1. Блок распрямления (Flatten)

### Назначение
Блок Flatten преобразует многомерные данные (матрицы, тензоры) в одномерный вектор.

### Зачем нужен
- Большинство вычислений в нейросетях выполняется над матрицами и тензорами
- Однако задачи классификации требуют **векторного входа**
- Flatten служит мостом между:
  - сверточными / рекуррентными слоями
  - и полносвязными (Dense) слоями

### Примеры входных данных
- Изображения: тензор размера `[H, W, C]`
- Тексты: матрица `[длина_последовательности, размер_эмбеддинга]`

### Принцип работы
Flatten просто **переписывает элементы тензора в один длинный вектор**  
без изменения значений и без обучаемых параметров.

### Свойства
- Не содержит обучаемых параметров
- Не изменяет информацию, только форму данных
- Обычно ставится:
  Conv / Pool → **Flatten** → Dense

---

## 2. Блоки случайного обнуления (DropOut)

### Назначение
DropOut — метод регуляризации, уменьшающий переобучение.

### Основная идея
Во время обучения:
- случайно отключается (обнуляется) часть нейронов
- на каждой итерации используется **разная структура сети**

Фактически:
> вместо одной сети обучается ансамбль похожих сетей

### Математическая суть
Если p — вероятность сохранения нейрона, то:
- каждый выход слоя умножается на случайную маску Бернулли

### Эффект
- уменьшает корреляцию весов
- снижает переобучение
- имитирует бэггинг

### Важные особенности
- DropOut **работает только при обучении**
- При инференсе (prediction) DropOut отключается
- Веса сети интерпретируются как усреднённые по ансамблю

### Где используется
- чаще всего перед полносвязными слоями
- реже — в сверточных слоях

---

## 3. Батч-нормализация (Batch Normalization)

### Проблема, которую решает
Функция потерь часто имеет форму вытянутого оврага:
- разные параметры обучаются с разной скоростью
- градиентный спуск сходится медленно и нестабильно

### Идея BatchNorm
Привести выходы слоя к распределению:
- среднее ≈ 0
- дисперсия ≈ 1

Это делает поверхность функции потерь более "сферической".

### Формула нормализации
Для компоненты xᵢ внутри батча:
$~xᵢ = (xᵢ − ⟨xᵢ⟩) / σᵢ$

Затем применяется обучаемое линейное преобразование:
$yᵢ = A·~xᵢ + B$

где:
- A, B — обучаемые параметры
- ⟨xᵢ⟩, σᵢ — среднее и дисперсия по батчу

### Обучение и инференс
- При обучении: статистики считаются по батчу
- При валидации: используются скользящие средние по эпохам

### Эффекты
- ускоряет обучение
- уменьшает зависимость слоев друг от друга
- снижает чувствительность к инициализации
- частично работает как регуляризация

---

## 4. Блоки объединения (Pooling)

### Назначение
Уменьшение размерности матриц признаков.

### Зачем нужен
- уменьшение числа параметров
- ускорение обучения
- повышение устойчивости к сдвигам и шумам

### Принцип работы
Матрица разбивается на окна фиксированного размера,
из каждого окна вычисляется одно число.

### Основные типы
- MaxPool — максимум в окне
- AvgPool — среднее значение
- MinPool — минимум (редко используется)

### Пример
MaxPooling 2×2:
из каждого блока 2×2 берется максимальный элемент

### Свойства
- не содержит обучаемых параметров
- снижает пространственное разрешение
- увеличивает область контекста

---

## 5. Блоки свертки (Conv) и сверточные сети

### Назначение
Извлечение локальных и иерархических признаков из данных.

### Математический смысл
Свертка — это линейная операция:
$U_{out}(R) = ∫ U_{in}(R − r) · K(r) dr$

В дискретном виде — взвешенная сумма элементов подматрицы.

### Интерпретация
- ядро свертки K — обучаемый фильтр
- каждый фильтр ищет определённый паттерн
- поиск параметров = линейная регрессия

### 1D и 2D свертки
- 1D — временные ряды, текст
- 2D — изображения

### Роль в сети
- выделение контекстных признаков
- учет корреляций соседних элементов
- не используется напрямую для принятия решения

---

## Параметры свертки

### Padding
Добавление нулей по краям матрицы.
Используется для:
- сохранения размера
- увеличения выходной размерности

### Stride
Шаг смещения ядра.
Используется для:
- уменьшения размерности
- ускорения вычислений

### Dilation
Разреженная свертка.
Используется для:
- учета дальних корреляций
- увеличения receptive field без роста параметров

---
## Итоговая схема CNN
```
Вход → [Conv → Activation → Pool]×N → Flatten → Dense → Softmax / Sigmoid → Класс
```
