
---
## 5. Методы построения эмбеддинга для токенов в NLP

Данный билет посвящён **способам представления токенов текста в числовом (векторном) виде**, пригодном для использования в алгоритмах машинного обучения и нейронных сетях.  
Ключевая задача эмбеддингов — **отобразить дискретные текстовые токены в непрерывное векторное пространство**, в котором сохраняются статистические и/или семантические свойства языка.

Общий переход выглядит так:

```
текст → токены → числовые векторы (эмбеддинги) → модель
```

---

## 1. Что такое эмбеддинг токенов

**Эмбеддинг** — это отображение токенов (слов, подслов, символов) в векторы фиксированной размерности в ℝⁿ, где:

- каждый токен соответствует вектору;
- близкие по смыслу токены имеют близкие векторы;
- векторное пространство позволяет применять линейную алгебру и градиентные методы.

Эмбеддинги могут быть:
- **статическими** (один вектор на токен);
- **контекстными** (вектор зависит от окружения токена).

---

## 2. Частотные и дискретные эмбеддинги (классические методы)

### 2.1. One-Hot Encoding

**One-hot представление** — простейший способ кодирования токенов.

Идея:
- словарь размера |V|;
- каждый токен — вектор длины |V|;
- на позиции токена стоит 1, остальные координаты 0.

Пример (словарь из 5 слов):
```
"кот" → [0, 1, 0, 0, 0]
```

Свойства:
- простота реализации;
- отсутствие семантики;
- огромная размерность;
- сильная разреженность.

Используется как теоретическая база, на практике почти не применяется напрямую.

---

### 2.2. Bag of Words (BoW)

**Bag of Words** — частотное векторное представление текста.

Идея:
- токен ↔ координата вектора;
- значение — количество вхождений токена в документе.

Свойства:
- учитывает частоты;
- игнорирует порядок слов;
- размерность равна размеру словаря.

BoW можно рассматривать как **эмбеддинг документа**, но не отдельного токена в нейросетевом смысле.

---

### 2.3. TF-IDF

**TF-IDF (Term Frequency – Inverse Document Frequency)** — взвешенное частотное представление.

Формула:
- TF — частота токена в документе;
- IDF — обратная частота токена в корпусе.

Смысл:
- редкие, но значимые слова получают больший вес;
- частые «служебные» — меньший.

Свойства:
- лучше BoW для классификации;
- по-прежнему линейное и разреженное представление;
- не учитывает семантические связи между словами.

---

## 3. Нейросетевые эмбеддинги слов (Word Embeddings)

### 3.1. Общая идея распределённых представлений

**Распределённые эмбеддинги** — плотные векторы малой размерности (обычно 50–300), обучаемые на больших корпусах.

Ключевая идея:
> «Слова, встречающиеся в похожих контекстах, имеют похожие векторы»

---

### 3.2. Embedding layer в нейронных сетях

В нейросетях используется **embedding-слой**:

- вход: индекс токена;
- выход: вектор фиксированной размерности;
- веса слоя — матрица эмбеддингов.

Матрица имеет вид:
```
|V| × d
```
где:
- |V| — размер словаря,
- d — размерность эмбеддинга.

Эмбеддинги могут:
- обучаться с нуля вместе с моделью;
- инициализироваться предобученными весами.

---

### 3.3. Word2Vec (CBOW и Skip-gram)

**Word2Vec** — семейство моделей для обучения эмбеддингов слов.

Два варианта:
- **CBOW** — предсказывает слово по контексту;
- **Skip-gram** — предсказывает контекст по слову.

Свойства:
- статический эмбеддинг (одно слово — один вектор);
- хорошо отражает семантические связи;
- не учитывает многозначность слова.

---

## 4. Subword-эмбеддинги и FastText

### 4.1. Проблема OOV (out-of-vocabulary)

Классические word-эмбеддинги:
- не умеют работать с новыми словами;
- чувствительны к морфологии и опечаткам.

---

### 4.2. FastText

**FastText** расширяет Word2Vec, представляя слово как сумму эмбеддингов его **символьных n-грамм**.

Идея:
- слово разбивается на подстроки;
- каждая подстрока имеет свой вектор;
- вектор слова — сумма/среднее этих векторов.

Пример:
```
"learning" → , , , ...
```

Преимущества:
- устойчивость к OOV;
- учёт морфологии;
- особенно полезен для языков с богатым словоизменением.

---

## 5. Подсловные эмбеддинги и BPE

### 5.1. Связь BPE и эмбеддингов

**BPE (Byte Pair Encoding)** используется для построения **подсловных токенов**, после чего каждому подслову сопоставляется эмбеддинг.

Свойства:
- словарь фиксированного размера;
- новые слова разлагаются на известные подслова;
- эмбеддинг слова строится композиционно.

Подсловные эмбеддинги:
- уменьшают словарь;
- решают проблему OOV;
- широко применяются в современных NLP-моделях.

---

## 6. Контекстные эмбеддинги

### 6.1. Ограничение статических эмбеддингов

Один вектор на слово не различает контексты:
```

"ключ" (дверной) ≠ "ключ" (шифрования)

```

---

### 6.2. Контекстные представления

В современных моделях:
- эмбеддинг токена зависит от окружения;
- одно и то же слово получает разные векторы в разных предложениях.

Такие эмбеддинги строятся:
- рекуррентными сетями;
- трансформерами;
- предобученными языковыми моделями.

---

## 7. Предобученные эмбеддинги и Transfer Learning

Эмбеддинги могут быть:
- обучены на огромных корпусах;
- перенесены в прикладную задачу.

Подход:
1) загрузка предобученной embedding-матрицы;
2) дообучение или фиксация весов;
3) обучение задачи поверх них.

Это:
- ускоряет обучение;
- повышает качество при малых датасетах.

---

## 8. Сравнение методов эмбеддинга

| Метод | Тип | Семантика | OOV | Размерность |
|---|---|---|---|---|
| One-hot | дискретный | нет | нет | очень большая |
| BoW / TF-IDF | частотный | слабая | частично | большая |
| Word2Vec | статический | есть | нет | малая |
| FastText | subword | есть | да | малая |
| BPE + Embedding | subword | есть | да | малая |
| Контекстные | динамический | сильная | да | малая |

---

## 9. Итог (экзаменационно)

- **Эмбеддинги токенов** — это векторные представления текстовых единиц, позволяющие применять методы машинного обучения.
- Существуют частотные, статические и контекстные методы построения эмбеддингов.
- **Bag of Words и TF-IDF** — базовые частотные методы.
- **Word2Vec и FastText** — нейросетевые методы обучения плотных эмбеддингов.
- **BPE и subword-подходы** решают проблему OOV и снижают размер словаря.
- Современные NLP-системы используют подсловные и контекстные эмбеддинги в сочетании с transfer learning.

---
