# 9. Языковые модели и перенос обучения (Transfer Learning)

---

## 1. Что такое языковая модель (Language Model)

**Языковая модель (LM)** — это вероятностная модель, которая описывает,
насколько правдоподобна последовательность токенов в языке.

Формально языковая модель оценивает вероятность последовательности:

P(x₁, x₂, …, x_T)

Обычно это раскладывается по правилу цепочки:

P(x₁, x₂, …, x_T) = ∏ₜ P(xₜ | x₁, …, xₜ₋₁)

То есть:
> вероятность каждого следующего токена при условии всех предыдущих.

---

## 2. Зачем нужны языковые модели

Языковая модель **не решает прикладную задачу напрямую**, а изучает структуру языка:

Она учится:
- синтаксису (порядок слов),
- морфологии,
- семантике,
- статистическим закономерностям языка,
- контекстным зависимостям.

Поэтому LM используется как **универсальный источник знаний о языке**.

---

## 3. Типы языковых моделей

### 3.1 N-граммные модели (классические)

Основаны на ограниченном контексте:

P(xₜ | xₜ₋ₙ₊₁, …, xₜ₋₁)

Недостатки:
- экспоненциальный рост словаря,
- плохо работают с длинным контекстом,
- не обобщаются.

Используются исторически, сейчас — редко.

---

### 3.2 Нейросетевые языковые модели

#### RNN / LSTM Language Model

Идея:
- вход — предыдущие токены,
- выход — распределение вероятностей следующего токена.

Обучение:
- задача классификации следующего токена
- softmax + cross-entropy

Проблемы:
- медленное обучение,
- ограниченная длина контекста.

---

### 3.3 Transformer-based Language Models (современный стандарт)

Примеры:
- BERT
- GPT
- RoBERTa
- XLM-R

Ключевая идея:
- **self-attention** вместо рекурсии
- параллельная обработка
- глобальный контекст

---

## 4. Обучение языковой модели (Pretraining)

### 4.1 Самообучение (Self-supervised learning)

Языковая модель обучается **без ручной разметки**.

Примеры задач:
- предсказание следующего токена (GPT)
- восстановление замаскированных токенов (BERT)
- предсказание пропущенных фрагментов

Данные:
- огромные текстовые корпуса
- Википедия, книги, интернет

---

### 4.2 Пример: Masked Language Modeling (BERT)

1. Часть токенов маскируется (`[MASK]`)
2. Модель должна восстановить исходные токены
3. Оптимизируется cross-entropy

Результат:
- модель учится **двустороннему контексту**
- формирует контекстные эмбеддинги

---

## 5. Что такое перенос обучения (Transfer Learning)

**Перенос обучения** — это использование модели,
предобученной на одной задаче, для решения другой задачи.

В NLP:
- сначала обучают **языковую модель**
- затем используют её как основу для прикладных задач

---

## 6. Почему перенос обучения работает в NLP

Язык — общий для всех задач.

Если модель уже знает:
- структуру предложений,
- связи между словами,
- смысл слов в контексте,

то для новой задачи ей нужно:
- **не учить язык заново**,
- а только адаптироваться под конкретную цель.

---

## 7. Fine-tuning (дообучение)

### 7.1 Общая схема

1. Берётся предобученная языковая модель
2. Добавляется небольшой task-specific слой
3. Модель дообучается на размеченных данных задачи

Пример:
- POS-tagging
- NER
- классификация текста
- QA

---

### 7.2 Что именно переносится

Переносятся:
- веса эмбеддингов,
- attention-слои,
- представление контекста.

Дообучаются:
- все веса (fine-tuning целиком)
- или только верхние слои (partial freezing)

---

## 8. Примеры задач с переносом обучения

### 8.1 Классификация текста
- берётся embedding `[CLS]`
- добавляется Dense + Softmax
- обучается на размеченных классах

### 8.2 POS-tagging / NER
- берутся эмбеддинги всех токенов
- поверх — классификатор (или CRF)
- метка на каждый токен

### 8.3 Машинный перевод
- encoder-decoder архитектура
- encoder часто инициализируется предобученной LM

---

## 9. Отличие от обучения «с нуля»

| Обучение с нуля | Transfer Learning |
|-----------------|------------------|
| Нужно много данных | Можно мало данных |
| Долго обучается | Быстро сходится |
| Слабое обобщение | Хорошее обобщение |
| Низкая точность | Высокая точность |

---

## 10. Ограничения переноса обучения

- доменное несоответствие (domain shift)
- большие модели → ресурсоёмкость
- возможное переобучение при малых данных
- «чёрный ящик»

---

## 11. Итог

- Языковая модель — это модель вероятности текста
- Она обучается в self-supervised режиме
- Перенос обучения позволяет использовать знания языка
- Современный NLP почти полностью основан на pretraining + fine-tuning
- BERT/GPT-подходы — де-факто стандарт

---

