# Глава: Бинарная классификация и критерий Байеса

## 1. Общая постановка задачи классификации

**Классификация** — задача отнесения наблюдаемого объекта (вектора признаков) к одному из заранее известных классов.

**Бинарная классификация** — простейший случай, когда классов ровно два.

### Интуитивная модель

- Есть два источника (генератора) данных:
    - генератор 1 (условно _красный_)
    - генератор 2 (условно _чёрный_)
- Наблюдателю доступно **одно число**
- Требуется определить, какой генератор его породил

**Ключевая идея:**

> задача классификации = задача разделения множества наблюдений на подмножества по некоторому правилу

---

## 2. Простой случай: идеальная разделимость

Если классы **разделимы однозначным признаком**, задача решается тривиально.

**Пример:**
- один генератор выдаёт только чётные числа
- другой — только нечётные

**Решение:**
- проверка чётности полностью определяет класc

**Вывод:**
- существует правило, не допускающее ошибок
- функция классификации детерминирована

---

## 3. Классификация по порогу

Рассмотрим более реалистичный случай:

- оба генератора выдают случайные величины
- распределения имеют:
    - разные математические ожидания
    - ненулевые дисперсии

### 3.1. Сильно разнесённые распределения

Условия:
- разница средних значений велика
- дисперсии малы

**Следствие:**
- между распределениями существует «пустой» промежуток
- границу (порог) можно выбрать легко

Если дисперсии равны:
- оптимальный порог ≈ полусумма средних значений

---

### 3.2. Слабо разнесённые распределения

**Условия:**
- распределения перекрываются
- дисперсии велики и могут различаться

**Проблема:**
- любое положение порога приводит к ошибкам

**Ключевая идея:**

> необходимо выбрать такой порог, который минимизирует ошибку классификации

---

## 4. Ошибки классификации

Вводятся два типа ошибок:

- **Ошибка первого рода (E₁)**  
    число сгенерировано 1-м генератором, но отнесено ко 2-му
    
- **Ошибка второго рода (E₂)**  
    число сгенерировано 2-м генератором, но отнесено к 1-му
    

Ошибки могут иметь **разную стоимость** в зависимости от задачи.

---

## 5. Функция потерь и критерий Байеса

Для количественной оценки качества классификации вводится **функция потерь**.

### Байесовская функция потерь
$$
LOSS(L) = W₁ · P₁(x > L) + W₂ · P₂(x < L)
$$
где:

- L — положение порога
- P₁, P₂ — условные вероятности ошибок 1-го и 2-го рода
- W₁, W₂ — веса (стоимости) ошибок

**Цель:**
$$
L₀ = argminₗ LOSS(L)
$$
---

## 6. Оценка вероятностей ошибок P₁ и P₂

**Основной подход:**
- использовать размеченный датасет
- построить плотности распределений для каждого генератора

**Вероятность ошибки**
- вычисляется как интеграл плотности распределения
- пределы интегрирования задаются положением порога L

---

## 7. Выбор весов ошибок W₁ и W₂

### 7.1. Классический байесовский критерий

- W₁ = W₂ = 1
- все ошибки равнозначны

### 7.2. Подход идеального наблюдателя (Котельников)

- W₁ и W₂ равны частотам появления данных от соответствующих генераторов
- учитывает априорные вероятности классов

### 7.3. Критерий Неймана–Пирсона

- фиксируется ошибка первого рода (E₁)
- минимизируется ошибка второго рода (E₂)
- эффективен в задачах с несколькими признаками и границами

---

## 8. Поиск оптимального порога

Задача сводится к минимизации функции LOSS(L).

**Возможные методы:**
- перебор по сетке значений
- деление отрезка пополам
- метод Ньютона
- градиентный спуск
- статистические методы (Монте-Карло, генетические алгоритмы)

**В рамках данной главы:**

- используется простой перебор по сетке    

---

## 9. Ключевые выводы для экзамена

- классификация = разделение пространства признаков
- бинарная классификация — два класса
- при перекрытии распределений ошибки неизбежны
- оптимальный порог выбирается через минимизацию функции потерь
- критерий Байеса учитывает вероятности ошибок и их стоимость
- выбор весов определяет стратегию классификации

# Глава: Построение бинарного решающего дерева по действительным данным

## 1. Обобщение постановки задачи

**Главная идея:** бинарные решающие деревья можно строить не только для бинарных, но и для **действительных признаков**.

Входные данные:

- входной вектор:  
    a = (a₀, a₁, …, aₙ), где aᵢ ∈ ℝ
    
- целевая функция:  
    y ∈ {0,1}
    

Отличие от предыдущего случая — только в **способе разбиения**.

---

## 2. Ключевое отличие от бинарных признаков

В бинарном случае:
- проверка: aᵢ = 0 или aᵢ = 1

В действительном случае:
- проверка: aᵢ < L или aᵢ ≥ L
- необходимо выбрать **порог L**

**Следствие:**

> для каждого признака нужно искать не только координату i, но и оптимальное значение порога L

---

## 3. Критерий выбора порога

Используется подход, аналогичный **критерию Байеса**.

Интерпретация:
- значения aᵢ при y = 0 → первый «генератор»
- значения aᵢ при y = 1 → второй «генератор»

Задача:
- провести границу L так, чтобы суммарная ошибка классификации была минимальна

---

## 4. Функция потерь для действительного признака

Для фиксированной координаты aᵢ вводится функция потерь:
$$
LOSSᵢ(L) = W₁ · P(y = 0, aᵢ ≥ L) + W₂ · P(y = 1, aᵢ < L)
$$
Смысл:

- ошибка первого рода — перепутать y = 0 с y = 1
- ошибка второго рода — перепутать y = 1 с y = 0

**Оптимальный порог:**
$$
Lᵢ* = argminₗ LOSSᵢ(L)
$$
---

## 5. Оценка ошибок через распределения

Для каждой координаты aᵢ:
1. строятся два распределения:
    - распределение aᵢ | y = 0
    - распределение aᵢ | y = 1
	
2. ошибки считаются как площади под кривыми:
    - ошибка 1-го рода — часть распределения y = 0 по одну сторону порога
    - ошибка 2-го рода — часть распределения y = 1 по другую сторону порога

---

## 6. Выбор лучшего признака

После нахождения оптимального порога для каждого признака:

- вычисляется минимальная ошибка: `LOSSᵢ(Lᵢ*)`
- выбирается признак i, для которого эта ошибка **минимальна среди всех признаков**

**Результат:**
- найдено лучшее условие вида:

> aᵢ < Lᵢ* / aᵢ ≥ Lᵢ*

---

## 7. Рекурсивное построение дерева

Дальнейшие шаги полностью аналогичны бинарному случаю:
1. делим датасет по найденному признаку и порогу
2. для каждого поддатасета:
    - если y (почти) постоянно → лист
    - иначе → рекурсивное продолжение

Таким образом строится дерево, близкое к оптимальному:
- короткое
- интерпретируемое
- с минимальной суммарной ошибкой

---

## 8. Алгоритмическая схема (для экзамена)

Для каждого узла дерева:
1. перебрать все признаки aᵢ
2. для каждого aᵢ:
    - построить распределения при y = 0 и y = 1
    - найти оптимальный порог Lᵢ*
    - вычислить минимальную ошибку LOSSᵢ
3. выбрать признак с минимальной LOSSᵢ
4. разделить датасет по этому признаку
5. рекурсивно повторить

---

## 9. Практическая реализация

Алгоритм легко автоматизируется:
- функция 1: поиск оптимального порога и ошибки
- функция 2: разбиение датасета по признаку

Основной цикл:
- перебор признаков
- выбор лучшего
- разбиение

---

## 10. Ключевые выводы

- решающие деревья естественно обобщаются на действительные данные
- ключевая операция — поиск оптимального порога
- используется байесовская логика минимизации ошибок
- итоговое дерево эффективно и интерпретируемо
# Глава: Уменьшение размера решающих деревьев (Pruning)

## 1. Проблема роста и переобучения деревьев

**Главная идея:**  
решающее дерево стремится усложняться, чтобы идеально описать обучающие данные, что приводит к **переобучению**.

**Причины роста дерева:**
- граница разделения классов не параллельна координатным осям
- увеличение объёма обучающих данных
- использование всех признаков на каждом уровне

**Следствие:**
- дерево перестаёт выделять общие закономерности
- вместо этого «запоминает» датасет

---

## 2. Стратегии построения дерева

Существуют два подхода выбора признаков при ветвлении:

### Подход 1. Без повторного использования признаков

- каждый признак используется не более одного раза
- глубина дерева ограничена размерностью входного вектора
- дерево компактное, но может быть неточным
### Подход 2. Повторное использование признаков

- на каждом уровне перебираются все признаки
- глубина дерева не ограничена заранее
- высокая точность на обучающем датасете
- **высокий риск переобучения**

---

## 3. Идея обрезки дерева (pruning)

**Цель обрезки:**

> уменьшить размер дерева без ухудшения (или с минимальным ухудшением) качества прогноза

Основной принцип:

- остановить рост дерева **до** полного описания обучающих данных

---

## 4. Основные методы ограничения роста дерева

### 4.1. Ограничение глубины дерева

Параметр: `max_depth`

Суть:
- жёстко ограничивается максимальная глубина дерева
- при достижении глубины узел превращается в лист

Следствие:
- прогноз в листе = наиболее вероятное значение y

Пример:

```python
tree.DecisionTreeClassifier(max_depth=7)
```

---

### 4.2. Ограничение по асимметрии ветвления

Параметр: `min_weight_fraction_leaf`
Суть:
- если одно из направлений ветвления содержит слишком мало данных
- ветвление запрещается

Интерпретация:
- если, например, 90% объектов имеют y = 1
- прогноз сразу принимается равным 1

Пример:

```python
tree.DecisionTreeClassifier(min_weight_fraction_leaf=0.01)
```

Эффект:
- симметризация дерева
- устранение «тонких» ветвей

---

### 4.3. Ограничение по количеству объектов

Используются два параметра:

- `min_samples_split`
    - минимальное число объектов в узле для разрешения ветвления
- `min_samples_leaf`
    - минимальное число объектов в листе

Примеры:

```python
tree.DecisionTreeClassifier(min_samples_split=10000)
tree.DecisionTreeClassifier(min_samples_leaf=1000)
```

---

### 4.4. Ограничение по числу листьев

Параметр: `max_leaf_nodes`

Суть:
- напрямую ограничивает размер дерева
- контролирует сложность модели глобально

Пример:

```python
tree.DecisionTreeClassifier(max_leaf_nodes=20)
```

---

## 5. Гиперпараметры дерева

Все параметры ограничения роста называются **гиперпараметрами**.

Их свойства:
- не обучаются автоматически
- подбираются экспериментально
- оптимизируются по валидационному датасету

**Цель подбора:**

> максимальная обобщающая способность модели

---

## 6. Ключевые выводы для экзамена

- рост дерева без ограничений ведёт к переобучению
- pruning — обязательный этап практического использования деревьев
- существует несколько способов ограничения роста:
    - глубина
    - асимметрия ветвлений
    - минимальное число объектов
    - число листьев
- все ограничения задаются гиперпараметрами
- оптимальные значения подбираются по валидации