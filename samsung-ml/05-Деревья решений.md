
## Деревья решений

Деревья решения основаны на известной структуре данных - деревьях, которые по сути представляют собой последовательные инструкции с условиями. Например, в задаче согласования кредита может быть следующий алгоритм принятия решения:

1. Старше ли клиент 18 лет? Если да, то продолжаем, иначе отказываем в кредите.

2. Превышает ли его заработок 50 тысяч рублей? Если да, то продолжаем, иначе отказываем в кредите.

3. Были ли у клиента просроченные кредиты ранее? Если да, отказываем в кредите, иначе выдаем.

В листьях (терминальных, конечных узлах или конечных вершинах) деревьев стоят значения целевой функции (прогноз), а в узлах - условия перехода, определяющие, по какому из рёбер идти. Если речь идет о бинарных деревьях (каждый узел производит ветвление на две части), обычно, если условие в узле истинно, то происходит переход по левому ребру, если ложно, то по правому.

```python
# визуализировать дерево можно посредством библиотеки python-graphviz
from graphviz import Digraph
dot = Digraph(node_attr={'shape': 'box'})

dot.node('A', label='Клиент старше 18 лет?')
dot.node('B', label='Превышает ли его заработок 50 тысяч рублей?')
dot.node('C', label='Отказать')
dot.node('D', label='Были ли у клиента просроченные кредиты ранее?')
dot.node('E', label='Отказать')
dot.node('F', label='Отказать')
dot.node('G', label='Выдать')

dot.edge('A', 'B', label='да')
dot.edge('A', 'C', label='нет')
dot.edge('B', 'D', label='да')
dot.edge('B', 'E', label='нет')
dot.edge('D', 'F', label='да')
dot.edge('D', 'G', label='нет')

dot
```
В задачах машинного обучения чаще всего в вершинах прописываются максимально простые условия. Обычно это сравнение значения одного из признаков $x^{j}$ с некоторым заданным порогом $t$:

$$[x^{j} \leq t].$$

Если выполняется задача классификации, конечным прогнозом считается класс или распределение вероятностей классов. В случае регрессии прогноз в листе представляет собой вещественное число.

Большим плюсом деревьев считается тот факт, что они легко интерпретируемы. То есть вы всегда можете восстановить всю цепочку принятия решения алгоритмом, в отличии от любых более сложных методов, таких, как нейронные сети.

Главное же отрицательное качество дерево решения - они (деревья) очень легко переобучаются. Легко построить дерево, где каждый лист будет соответствовать одному объекту обучающей выборки. Оно будет идеально подогнано под обучающую выборку, давать стопроцентный ответ на ней, но при этом не станет восстанавливать оригинальных закономерностей, и качество ответов на новых данных окажется неудовлетворительным.

### Теория построения дерева решений

В машинном обучении деревья строятся от корня к листьям. Вначале выбирается корень и критерий, по которому выборка разбивается пополам (или на несколько частей). Затем то же самое делается для каждого из потомков этого корня и так далее до остановки. Задача на каждом этапе состоит в выборе способа разбиения каждого узла, то есть в выборе значения порога, с которым будет сравниваться значение одного из признаков в каждом узле.

Разбиениение происходит на основании заранее заданного функционала качества $Q(X, j, t)$. Находятся наилучшие значения $j$ и $t$ для выбора порога $[x^{j}<t]$. Параметры $j$ и $t$ можно выбирать перебором: признаков конечное число, а из всех возможных значений порога $t$ можно рассматривать только те, при которых получаются различные разбиения на две подвыборки. Таким образом, различных значений параметра $t$ будет столько же, сколько различных значений признака $x^{j}$ в обучающей выборке.

В каждой потенциальной вершине производится проверка, не выполнилось ли некоторое условие останова. Если оно выполнилось, разбиение прекращается, такая вершина будет содержать прогноз.

В задаче классификации это будет класс, к которому относится большая часть объектов из выборки в листе $X_{m}$

$$a_{m} = \text{argmax}_{y \in Y} \sum_{i \in X_{m}}[y_{i}=y]$$

или доля объектов определённого класса $k$, если требуется предсказать вероятности классов

$$a_{mk} = \frac{1}{|X_{m}|} \sum_{i \in X_{m}}[y_{i}=k].$$

В случае регрессии в качестве ответа можно давать средние значение по выборке по выборке в листе

$$a_{m} = \frac{1}{|X_{m}|} \sum_{i \in X_{m}}y_{i}.$$

За функционал качества при работе с деревом решений принимается функционал вида:

$$Q(X_{m}, j, t) = H(X_{m}) - \frac{|X_{l}|}{|X_{m}|}H(X_{l}) - \frac{|X_{r}|}{|X_{m}|}H(X_{r}),$$

где $X_{m}$ — множество объектов, попавших в лист на этом шаге, а $X_{l}$ и $X_{r}$ — множества, попадающие в левое и правое поддерево, соответственно, после гипотетического разбиения. $H(X)$ — _критерий информативности_. Он оценивает качество распределения объектов в подмножестве и тем меньше, чем меньше разнообразие ответов в $X$, соответственно, задача обучения состоит в его минимизации и максимизации $Q(X_{m}, j, t)$ на этом шаге.

### Критерий информативности
В случае регрессии разброс будет характретизваться дисперсией, поэтому критерий информативности будет записан в виде

$$H(X) = \frac{1}{X}\sum_{i\in X}(y_{i} - \bar{y}(X))^{2}.$$

В задаче классификации есть несколько способов определить критерий информативности.

Обозначим через $p_{k}$ долю объектов класса $k$ в выборке $X$:

$$p_{k} = \frac{1}{|X|}\sum_{i\in X}[y_{i} = k].$$

$p_{k}$ будет характеризовать вероятность выдачи класса $k$.

_Критерий Джини_, или _индекс Джини_, выглядит следующим образом:

$$H(X) = \sum^{K}_{k=1}p_{k}(1-p_{k}),$$

где $K$ — количество классов в наборе данных $X$.

Его минимум достигается, когда все объекты в подмножестве относятся к одному классу, а максимум — при равном содержании объектов всех классов. Критерий информативности Джини можно интерпретировать как вероятность ошибки случайного классификатора.

Ещё один критерий информативности — _энтропийный критерий_. Он также называется _энтропией Шеннона_ и записывается как

$$H(X) = - \sum^{K}_{k=1}p_{k}\text{log}_{2}p_{k}.$$

Минимум энтропии также достигается, когда все объекты относятся к одному классу, а максимум — при равномерном распределении. Прирост информации есть утрата неопределённости (=уменьшение энтропии). В формуле полагается, что $0\text{log}_{2}0=0.$

### Остановка построения дерева

_Критерии останова_ — это критерий, который показывает, надо ли остановить процесс построения дерева. Правильный выбор критериев останова, относящихся к росту дерева, может существенно повлиять на его качество. Существует множество возможных ограничений:

- Ограничение максимальной глубины дерева. Этот критерий считается довольно грубым, но хорошо зарекомендовавшим себя в построении композиций деревьев — когда несколько деревьев объединяются в один алгоритм.


- Ограничение максимального количества листьев.


- Ограничение минимального количества $n$ объектов в листе. Оно должно быть достаточным, чтобы построить надёжный прогноз.


- Останов, когда все объекты в листе относятся к одному классу.

- Требование улучшения функционала качества при разбиении на какую-то минимальную величину.

```python
import matplotlib.pyplot as plt
import random

from matplotlib.colors import ListedColormap
from sklearn import datasets

import numpy as np

# сгенерируем и визуализируем данные
classification_data, classification_labels = datasets.make_classification(n_features = 2, n_informative = 2,
                                                      n_classes = 2, n_redundant=0,
                                                      n_clusters_per_class=1, random_state=5)

colors = ListedColormap(['red', 'blue'])
light_colors = ListedColormap(['lightcoral', 'lightblue'])

plt.figure(figsize=(8,8))
plt.scatter(list(map(lambda x: x[0], classification_data)), list(map(lambda x: x[1], classification_data)),
              c=classification_labels, cmap=colors)
plt.show()
```

```python
# Реализуем класс узла

class Node:

    def __init__(self, index, t, true_branch, false_branch):
        self.index = index  # индекс признака, по которому ведётся сравнение с порогом в этом узле
        self.t = t  # значение порога
        self.true_branch = true_branch  # поддерево, удовлетворяющее условию в узле
        self.false_branch = false_branch  # поддерево, не удовлетворяющее условию в узле
```

```python
# Реализуем класс листа листа

class Leaf:

    def __init__(self, data, labels):
        self.data = data
        self.labels = labels  # y_true
        self.prediction = self.predict()  # y_pred

    def predict(self):
        # подсчёт количества объектов разных классов
        classes = {}  # сформируем словарь "класс: количество объектов"
        for label in self.labels:
            if label not in classes:
                classes[label] = 0
            classes[label] += 1
        #  найдём класс, количество объектов которого будет максимальным в этом листе, и вернём его
        prediction = max(classes, key=classes.get)
        return prediction
```

```python
# Расчёт критерия Джини

def gini(labels):
    #  подсчёт количества объектов разных классов
    classes = {}
    for label in labels:
        if label not in classes:
            classes[label] = 0
        classes[label] += 1

    #  расчёт критерия
    impurity = 1
    for label in classes:
        p = classes[label] / len(labels)
        impurity -= p ** 2

    return impurity
```

```python
# Расчёт качества

def quality(left_labels, right_labels, current_gini):

    # доля выборки, ушедшей в левое поддерево
    p = float(left_labels.shape[0]) / (left_labels.shape[0] + right_labels.shape[0])

    return current_gini - p * gini(left_labels) - (1 - p) * gini(right_labels)
```

```python
# Разбиение выборки на две части

def split(data, labels, index, t):

    left = np.where(data[:, index] <= t)
    right = np.where(data[:, index] > t)

    true_data = data[left]
    false_data = data[right]
    true_labels = labels[left]
    false_labels = labels[right]

    return true_data, false_data, true_labels, false_labels
```

```python
# Нахождение наилучшего разбиения

def find_best_split(data, labels):

    #  обозначим минимальное количество объектов в узле
    min_leaf = 5

    current_gini = gini(labels)

    best_quality = 0
    best_t = None
    best_index = None

    n_features = data.shape[1]

    for index in range(n_features):
        t_values = [row[index] for row in data]

        for t in t_values:
            true_data, false_data, true_labels, false_labels = split(data, labels, index, t)
            #  пропускаем разбиения, где в узле остаётся менее 5 объектов
            if len(true_data) < min_leaf or len(false_data) < min_leaf:
                continue

            current_quality = quality(true_labels, false_labels, current_gini)

            #  выбираем порог, на котором получается максимальный прирост качества
            if current_quality > best_quality:
                best_quality, best_t, best_index = current_quality, t, index

    return best_quality, best_t, best_index
```

```python
# Построение дерева посредством рекурсивной функции

def build_tree(data, labels):

    quality, t, index = find_best_split(data, labels)

    #  Базовый случай — прекращаем рекурсию, когда нет прироста в качества
    if quality == 0:
        return Leaf(data, labels)

    true_data, false_data, true_labels, false_labels = split(data, labels, index, t)

    # Рекурсивно строим два поддерева
    true_branch = build_tree(true_data, true_labels)
    false_branch = build_tree(false_data, false_labels)

    # Возвращаем класс узла со всеми поддеревьями, то есть целого дерева
    return Node(index, t, true_branch, false_branch)
```
```python
# Проход объекта по дереву для его классификации

def classify_object(obj, node):

    #  Останавливаем рекурсию, если достигли листа
    if isinstance(node, Leaf):
        answer = node.prediction
        return answer

    #print(obj, node.index)
    if obj[node.index] <= node.t:
        return classify_object(obj, node.true_branch)
    else:
        return classify_object(obj, node.false_branch)
```
```python
# Предсказание деревом для всего датасета

def predict(data, tree):

    classes = []
    for obj in data:
        prediction = classify_object(obj, tree)
        classes.append(prediction)
    return classes
```
```python
# Разобьём выборку на обучающую и тестовую

from sklearn import model_selection

train_data, test_data, train_labels, test_labels = model_selection.train_test_split(classification_data,classification_labels,test_size = 0.3, random_state = 1)
```
```python
# Построим дерево по обучающей выборке
my_tree = build_tree(train_data, train_labels)
```
```python
# Напечатаем ход нашего дерева
def print_tree(node, spacing=""):

    # Если лист, то выводим его прогноз
    if isinstance(node, Leaf):
        print(spacing + "Прогноз:", node.prediction)
        return

    # Выведем значение индекса и порога на этом узле
    print(spacing + 'Индекс', str(node.index))
    print(spacing + 'Порог', str(node.t))

    # Рекурсионный вызов функции на положительном поддереве
    print (spacing + '--> True:')
    print_tree(node.true_branch, spacing + "  ")

    # Рекурсионный вызов функции на положительном поддереве
    print (spacing + '--> False:')
    print_tree(node.false_branch, spacing + "  ")

print_tree(my_tree)
# Получим ответы для обучающей выборки
train_answers = predict(train_data, my_tree)
# Получим ответы для тестовой выборки
answers = predict(test_data, my_tree)
# Введём функцию подсчёта точности
def accuracy_metric(actual, predicted):
    correct = 0
    for i in range(len(actual)):
        if actual[i] == predicted[i]:
            correct += 1
    return correct / float(len(actual)) * 100.0
# Точность на обучающей выборке
train_accuracy = accuracy_metric(train_labels, train_answers)
train_accuracy
# Точность на тестовой выборке
test_accuracy = accuracy_metric(test_labels, answers)
test_accuracy
# Визуализируем дерево на графике

def get_meshgrid(data, step=.05, border=1.2):
    x_min, x_max = data[:, 0].min() - border, data[:, 0].max() + border
    y_min, y_max = data[:, 1].min() - border, data[:, 1].max() + border
    return np.meshgrid(np.arange(x_min, x_max, step), np.arange(y_min, y_max, step))

plt.figure(figsize = (16, 7))

# график обучающей выборки
plt.subplot(1,2,1)
xx, yy = get_meshgrid(train_data)
mesh_predictions = np.array(predict(np.c_[xx.ravel(), yy.ravel()], my_tree)).reshape(xx.shape)

#print(np.c_[xx.ravel(), yy.ravel()])

plt.pcolormesh(xx, yy, mesh_predictions, cmap = light_colors)
plt.scatter(train_data[:, 0], train_data[:, 1], c = train_labels, cmap = colors)
plt.title(f'Train accuracy={train_accuracy:.2f}')

# график тестовой выборки
plt.subplot(1,2,2)
plt.pcolormesh(xx, yy, mesh_predictions, cmap = light_colors)
plt.scatter(test_data[:, 0], test_data[:, 1], c = test_labels, cmap = colors)
plt.title(f'Test accuracy={test_accuracy:.2f}')
plt.show()
```
### своя реализация
```python
import numpy as np
from collections import Counter

# 1. Моя функция gini через sum(p*(1-p))
def gini(labels):
    """Индекс Джини - мера нечистоты данных"""
    if len(labels) == 0:
        return 0
    
    # Считаем сколько объектов каждого класса
    counts = Counter(labels)  # например {0: 3, 1: 1}
    n = len(labels)  # всего объектов
    
    # Вычисляем доли каждого класса
    gini_value = 0
    for count in counts.values():
        p = count / n  # доля класса
        gini_value += p * (1 - p)  # моя формула!
    
    return gini_value

# 2. Два типа узлов дерева
class Leaf:  # Лист - конечный узел
    def __init__(self, labels):
        # Выбираем самый частый класс
        counts = Counter(labels)
        self.prediction = max(counts, key=counts.get)

class Node:  # Узел - вопрос
    def __init__(self, feature_idx, threshold, left, right):
        self.feature_idx = feature_idx  # номер признака (0 или 1)
        self.threshold = threshold      # пороговое значение
        self.left = left                # левое поддерево (<= threshold)
        self.right = right              # правое поддерево (> threshold)

# 3. Построение дерева
def build_tree(X, y, min_leaf=5):
    """Строит дерево решений рекурсивно"""
    
    # Если мало данных или все одного класса - создаем лист
    if len(y) <= min_leaf or gini(y) == 0:
        return Leaf(y)
    
    best_gain = 0
    best_threshold = None
    best_feature = None
    
    # Ищем лучший способ разделить данные
    for feature_idx in range(X.shape[1]):  # по всем признакам
        for threshold in np.unique(X[:, feature_idx]):  # по всем значениям
            
            # Разделяем данные
            left_mask = X[:, feature_idx] <= threshold
            right_mask = ~left_mask
            
            # Пропускаем если одна из частей слишком мала
            if sum(left_mask) < min_leaf or sum(right_mask) < min_leaf:
                continue
            
            # Считаем насколько улучшилась чистота
            left_gini = gini(y[left_mask])
            right_gini = gini(y[right_mask])
            n_left, n_right = sum(left_mask), sum(right_mask)
            
            gain = gini(y) - (n_left/len(y))*left_gini - (n_right/len(y))*right_gini
            
            # Запоминаем лучшее разделение
            if gain > best_gain:
                best_gain, best_threshold, best_feature = gain, threshold, feature_idx
    
    # Если не нашли хорошего разделения - лист
    if best_gain == 0:
        return Leaf(y)
    
    # Рекурсивно строим левое и правое поддеревья
    left_mask = X[:, best_feature] <= best_threshold
    left_tree = build_tree(X[left_mask], y[left_mask], min_leaf)
    right_tree = build_tree(X[~left_mask], y[~left_mask], min_leaf)
    
    return Node(best_feature, best_threshold, left_tree, right_tree)

# 4. Предсказание
def predict_one(x, tree):
    """Предсказывает класс для одного объекта"""
    if isinstance(tree, Leaf):
        return tree.prediction
    
    if x[tree.feature_idx] <= tree.threshold:
        return predict_one(x, tree.left)
    else:
        return predict_one(x, tree.right)

def predict(X, tree):
    """Предсказывает классы для всех объектов"""
    return [predict_one(x, tree) for x in X]

# 5. Пример использования
if __name__ == "__main__":
    # Простые данные для примера
    X = np.array([
        [1, 2], [2, 3], [3, 1],  # класс 0
        [7, 8], [8, 6], [9, 9]   # класс 1
    ])
    y = np.array([0, 0, 0, 1, 1, 1])
    
    # Строим дерево
    tree = build_tree(X, y, min_leaf=2)
    
    # Предсказываем
    test_points = np.array([[2.5, 2], [8, 7]])
    predictions = predict(test_points, tree)
    
    print("Предсказания:", predictions)  # [0, 1]
```
