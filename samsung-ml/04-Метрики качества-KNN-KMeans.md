
## Оценка качества классификации

Оценка качества классификации, хоть и кажется простой задачей, имеет свои "подвохи".

### Очевидное решение

Самое просто решение, которое может прийти в голову - оценивать число верных ответов, то есть "правильных" классификаций. Такая метрика называется "точностью", а на сленге - "аккуратностью". Откуда возникло второе название вы сможете узнать чуть позже.

$$accuracy(a,x) = \frac{1}{l} \sum^{l}_{i=1}[a(x_{i})=y_{i}].$$

Тем не менее, такая метрика имеет некоторые недостатки, котороые хорошо заметны на несбалансированных выборках, то есть на таких выборках, в которых число объектов одного класса значительно больше, чем число объектов другого класса.

Например, предположим, что у вас в выборке 10000 объектов. Объектов, которые можно отнести к первому классу 99900. Объектов, которые можно отнести ко второму классу - 100. Пусть функция классификации относит все объекты к первому классу. Тогда очевидно, что тоность составит 0.999. Хотя на практике, такой классификатор не верно идентифицирует _каждый_ объект второго класса.

### Матрица ошибок (confusion matrix)

Матрица ошибок - классический способ представления результатов работы классификатора, особенно, бинарного классификатора. Матрица ошибок составляется следующим образом:

|  <empty>   | $$y = 1$$ | $$y = -1$$ |
--- | --- | ---
| __$$a(x) = 1$$__  |   TP    |   FP   |
| __$$a(x) = -1$$__ |   FN    |   TN   |

По горизонтали откладывается истинное значение класса объекта. По вертикали результат работы модели. На пересечении получается набор вариантов срабатывания, которые сейчас обозначены классическими для компьютерных наук терминами:

*   TP - истинно-положительное или истинно-верное срабатывание. "Положительный" объект был отнесён к "положительному" классу.
*   FP - ложно-положительное срабатывание. "Отрицательный" объект был отнесён к "положительному" классу.
*   FN - ложно-отрицательное срабатывание. "Положительный" объект был отнесён к "отрицательному" классу.
*   TN - истинно-отрицательное срабатывание. "Отрицательный" объект был отнесён к "отрицательному" классу.

Исходя из этой матрицы можно выделить два разных типа ошибок.

Исходя из этого, можно по-другому рассчитать "аккуратность":

$$accuracy(a,x) = \frac{TP+TN}{TP+TN+FP+FN} $$

Так как сумма в знаменателе всегда больше или равна нулю, а числитель не может быть меньше знаменателя, то значения метрики находится интервале от 0 до 1.

### Точность и полнота

Исходя из того, что ошибки могут быть двух типов (в случае бинарной классификации), а также с допущением, что мы выбираем* "положительный" класс для расчёта характеристик можно ввести две метрики,

Точность** (precision) представляет из себя долю истинных срабатываний от общего количества срабатываний. Она показывает, насколько можно доверять алгоритму классификации в случае срабатывания

$$precision(a, X) = \frac{TP}{TP+FP}.$$

Полнота (recall) считается как доля объектов, истинно относящихся к классу "+1", которые алгоритм отнес к этому классу

$$recall(a, X) = \frac{TP}{TP+FN}.$$

*исходя из этого важного отметить, что приведённые в этом разделе оценки зависят от "точки отсчёта", то есть от того, какой класс выбран в качестве основного для расчёта метрики.

**именно по-этому для той "точности", которую мы обсуждали ранее, и появилось сленговое название "аккуратность".

### Пример

Пусть у нас есть выборка из 100 объектов, из которых 50 относится к классу "+1" и 50 к классу "-1" и для этой работы с этой выборкой мы рассматриваем две модели: $a_{1}(x)$ с матрицей ошибок

|  <empty>   | $$y = +1$$ | $$y = -1$$ |
--- | --- | ---
| __$$a_{1}(x) = +1$$__  |   45    |   5   |
| __$$a_{1}(x) = -1$$__ |   5    |   45   |
    

и $a_{2}(x)$ с матрицей ошибок:


|  <empty>   | $$y = +1$$ | $$y = -1$$ |
--- | --- | ---
| __$$a_{2}(x) = +1$$__  |   12    |   8   |
| __$$a_{2}(x) = -1$$__ |   20    |   60   |

Рассчитаем значения всех ключевых харатеристик:

Для первого алгоритма

$$preсision(a_{1}, X)=P=0.8$$
$$recall(a_{1}, X)=R=0.8$$
$$accuracy(a_{1}, X) = A = 0.9$$

Для второго алгоритма

$$P(a_{2}, X)=0.6$$
$$R(a_{2}, X)=0.375$$
$$A(a_{2}, X)=0.72$$

Полнота и точность - разные характеристики, каждая из которых может требоваться при решении разных задач.
Например - максимизация полноты необходима в задаче интерскопического распознавания объектов (поиск опасных объектов при досмотре в аэропорту и прочее): каждый пропущенный объект будет стоить жизни, а каждый ложно найденный объект можно перепропрверить "руками" и "глазами".

### f-мера

Часто точность и полноту необходимо рассматривать в совокупности друг с другом. Наиболее часто используемой с такой целью метрикой является _F-мера_, которая представляет собой среднее гармоническое между точностью и полнотой

$$F = \frac{2 \cdot precision \cdot recall }{ presision + recall}.$$

```python
import matplotlib.pyplot as plt
import numpy as np
```

```python
A, B = np.meshgrid(np.linspace(0.01, 1, 100), np.linspace(0.01, 1, 100))

f_levels = np.empty_like(A)
for i in range(A.shape[0]):
    for j in range(A.shape[1]):
        f_levels[i, j] = 2 * A[i, j] * B[i, j] / (A[i, j] + B[i, j])
        #f_levels[i, j] = A[i, j] + B[i, j]

plt.figure(figsize=(6, 6))
plt.title('F-мера')
plt.xlabel('Точность')
plt.ylabel('Полнота')
plt.grid()
plt.contour(A, B, f_levels, levels=50)
plt.show()
```


## kNN

Рассмотрим ещё один классический алгоритм классификации - kNN. Расшифровывается его название как "k ближайших соседей (k nearest neighbours)".

### Общее описание алгоритма
Суть алгоритма заключается в принципе отнесения объекту к тому классу, представители которого преобладают рядом с ним. Упрощенно алгоритм классификации выглядит следующим образом:

- найти расстояние от объекта $u$ до каждого из объектов $x$ обучающей выборки;
- выбрать $k$ объектов, расстояние до которых минимально;
- отнести объект к классу, к которому относится большинство из выбранных $k$ ближайших соседей:

$$a(u) = \underset{y}{\text{argmax}}\sum_{i=1}^{k}[y_{u}^{(i)}=y],$$

то есть провести голосование.

Усовершенствованием алгоритма kNN является добавление весов (так называемое "взвешенное голосование"), зависящих от их порядкового номера или расстояния до классифицируемого объекта (чем ближе объект обучающей выборки, тем больше его вес).

От номера соседа $i$ веса можно определять как:

- $w(i) = q^{i}$,   $q \in (0,1)$;


- $w(i) = \frac{1}{i}$;


- $w(i) = \frac{1}{(i+a)^{b}}$;


- $w(i) = \frac{k + 1 - i}{k}$.


От расстояния $d$ веса можно определять как:

- $w(d) = q^{d}$,   $q \in (0,1)$;


- $w(d) = \frac{1}{(d+a)^{b}}$.

Существуют и другие способы вычисления весов.

Перед работой по алгоритму kNN стоит (хотя и не обязательно) проводить нормализацию признаков, так как они могут иметь разные единицы измерения и, соответственно, разный масштаб, что будет прямо влиять на вычисление расстояния.

### Расстояние в математике

Евклидова метрика

$$\rho(x, y) = \sqrt{\sum_{i=1}^{n}(x_{i}-y_{i})^{2}}$$

Манхэттенская метрика

$$\rho(x, y) = \sum_{i=1}^{n}|x_{i}-y_{i}|$$

Метрика Минковского:

$$\rho(x, y) = \left ( \sum_{i=1}^{n}|x_{i}-y_{i}|^{q} \right )^{\frac{1}{q}}.$$

При этом при $q = 1$ получаем манхэттенскую ($L_{1}$) метрику, при $q = 2$ - евклидову ($L_{2}$) метрику.

В метрических алгоритмах часто используются так называемые меры близости. В отличие от метрик, которые тем меньше, чем объекты более похожи, меры близости увеличиваются при увеличении похожести (близости) объектов.

Примером такой функции может быть _косинусное сходство (косинусная мера)_:

$$\text{cos}\theta = \frac{\left \langle x, y \right \rangle}{||x||\cdot||y||} = \frac{\sum_{i=1}^{n}x_{i}y_{i}}{\sqrt{\sum_{i=1}^{n}x_{i}^{2}}\sqrt{\sum_{i=1}^{n}y_{i}^{2}}}.$$

Формула нахождения косинусной меры похожа на коэффициент корреляции, который также может быть использован как мера близости и используется обычно в рекомендательных системах:

$$r = \frac{\sum_{i=1}^{n}((x_{i} - \bar{x})(y_{i} - \bar{y}))}{\sqrt{\sum_{i=1}^{n}(x_{i} - \bar{x})^{2}}\sqrt{\sum_{i=1}^{n}(y_{i} - \bar{y})^{2}}}.$$

### Практическая реализация

```python
import numpy as np
from sklearn import model_selection
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
```

```python
X, y = load_iris(return_X_y=True)

# Для возможности визуализации возьмем только первые два признака (всего в датасете их 4)
X = X[:, :2]
```
```python
cmap = ListedColormap(['red', 'green', 'blue'])
plt.figure(figsize=(7, 7))
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap)
plt.show()
```
```python
X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=1)
```
```python
def e_metrics(x, y):
    
    distance = 0
    for i in range(len(x)):
        distance += np.square(x[i] - y[i])
      
    return np.sqrt(distance)
```
```python
def knn(x_train, y_train, x_test, k):
    
    answers = []
    for x in x_test:
        test_distances = []
            
        for i in range(len(x_train)):
            
            # расчет расстояния от классифицируемого объекта до объекта обучающей выборки
            distance = e_metrics(x, x_train[i])
            
            # Записываем в список значение расстояния и ответа на объекте обучающей выборки
            test_distances.append((distance, y_train[i]))
        
        # создаем словарь со всеми возможными классами
        classes = {class_item: 0 for class_item in set(y_train)}
        
        # Сортируем список и среди первых k элементов подсчитаем частоту появления разных классов
        for d in sorted(test_distances)[0:k]:
            classes[d[1]] += 1
            
        # Записываем в список ответов наиболее часто встречающийся класс
        answers.append(sorted(classes, key=classes.get)[-1])
    return answers
```
```python
def accuracy(pred, y):
    return (sum(pred == y) / len(y))
```
```python
def get_graph(X_train, y_train, k):
    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA','#00AAFF'])

    h = .02

    # Расчет пределов графика
    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

    # Получим предсказания для всех точек
    Z = knn(X_train, y_train, np.c_[xx.ravel(), yy.ravel()], k)

    # Построим график
    Z = np.array(Z).reshape(xx.shape)
    plt.figure(figsize=(7,7))
    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)

    # Добавим на график обучающую выборку
    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap)
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.title(f"Трехклассовая kNN классификация при k = {k}")
    plt.show()
```
```python
k = 1

y_pred = knn(X_train, y_train, X_test, k)

print(f'Точность алгоритма при k = {k}: {accuracy(y_pred, y_test):.3f}')
```

```python
k = 5

y_pred = knn(X_train, y_train, X_test, k)

print(f'Точность алгоритма при k = {k}: {accuracy(y_pred, y_test):.3f}')

get_graph(X_train, y_train, k)
```

### Своя реализация
```python
def knn(x_train, y_train, x_test, k):
    answers = []
    for test_point in x_test:
        distances = [
            (e_metrics(test_point, train_point), label) for train_point, label in zip(x_train, y_train)
        ]

        k_nears = [label for _, label in sorted(distances)[:k]]

        from collections import Counter
        answers.append(
            Counter(k_nears).most_common(1)[0][0]
        )
    return answers
```

## Кластеризация
До этого мы рассматривали методы обучения с учителем, то есть задачи, в которых изначально есть размеченная обучающая выборка данных с известными ответами.

Теперь мы рассмотрим обучение без учителя (unsupervised learning) - то есть такое семейство методов, где в роли обучающей выборки выступает просто набор объектов  x1,...,xl , и он же выступает в роли тестовой выборки, а задача состоит в проставлении меток  y1,...,yl  так, что бы объекты с одной и той же меткой были "похожи", а с разными - нет. То есть все объекты в пространстве признаков нужно разделить на группы, найти структуру в данных.

Это и называется кластеризацией.

По-простому задачу кластеризации можно сформулировать так: имеется множество точек, которые скапливаются в сгустки, нужно найти возможность относить точки к тому или иному сгустку и предсказывать, в какой сгусток попадет новая точка.

### KMeans

Одним из самых простых и популярных алгоритмов кластеризации является алгоритм _K-means (K-средних)_. Заключается он в следующих шагах:

1. Выбрать количество кластеров $k$, на которые будут делиться данные.

2. Случайным образом выбрать в пространстве данных $k$ точек $c_{k}$ (центроидов) - центров будущих кластеров.

3. Для каждой точки из выборки посчитать, к какому из центроидов она ближе.

4. Переместить каждый центроид в центр выборки, отнесенной к этому центроиду, определив его как среднее арифметическое всех точек кластера:

$$c_{k} = \frac{\sum_{i=1}^{l}[a(x_{i})=k]x_{i}}{\sum_{i=1}^{l}[a(x_{i}) = k]}.$$


5. Повторить шаги 4-5 до сходимости алгоритма (обычно это оценивается по величине смещения центроида после каждого шага - сходимость означает непревышение смещения какого-то заданного значения).

Результат работы алгоритма значительно зависит от начального выбора центроидов. Существует много методик их выбора, наиболее удачным из которых считается k-means++. Он заключается в последовательном выборе начальных приближений так, что вероятность выбрать в качестве центроида следующую точку пропорциональна квадрату расстояния от нее до ближайшего центроида.

Проблемой метода является необходимость знать число кластеров, на которые будет делиться выборка. В случае, когда это число неизвестно, вариантом может быть последовательная кластеризация на разное число кластеров с последующим анализом качества работы алгоритма, например, по сумме квадратов внутрикластерных расстояний $$\sum_{k=1}^{K}\sum_{i \in C_{k}}\rho(x_{i}, c_{k})^{2}$$ - выбирается такое число кластеров, начиная с которого при увеличении количества кластеров функционал падает незначительно.

```python
import numpy as np
import matplotlib.pyplot as plt
```
```python
from sklearn.datasets import make_blobs
import random

X, y = make_blobs(n_samples=100, random_state=3)

plt.figure(figsize=(7,7))
plt.scatter(X[:, 0], X[:, 1])

plt.show()
```
```python
def kmeans(data, k, max_iterations, min_distance):

    # инициализируем центроиды как первые k элементов датасета
    centroids = [data[i] for i in range(k)]

    for _ in range(max_iterations):
        # Создадим словарь для классификации
        classes = {i: [] for i in range(k)}
        # классифицируем объекты по центроидам
        for x in data:
            # определим расстояния от объекта до каждого центроида
            distances = [e_metrics(x, centroid) for centroid in centroids]
            # отнесем объект к кластеру, до центроида которого наименьшее расстояние
            classification = distances.index(min(distances))
            classes[classification].append(x)

        # сохраним предыдущие центроиды в отдельный список для последующего сравнения сновыми
        old_centroids = centroids.copy()

        # пересчитаем центроиды как среднее по кластерам
        for classification in classes:
            centroids[classification] = np.average(classes[classification], axis=0)

        # сравним величину смещения центроидов с минимальной
        optimal = True
        for centroid in range(len(centroids)):
            if np.sum(abs((centroids[centroid] - old_centroids[centroid]) / old_centroids * 100)) > min_distance:
                optimal = False

        # если все смещения меньше минимального, останавливаем алгоритм
        if optimal:
            break

    return old_centroids, classes
```
```python
def visualize(centroids, classes):
    colors = ['r', 'g', 'b']

    plt.figure(figsize=(7,7))

    # нанесем на график центроиды
    for centroid in centroids:
        plt.scatter(centroid[0], centroid[1], marker='x', s=130, c='black')

    # нанесем объекты раскрашенные по классам
    for class_item in classes:
        for x in classes[class_item]:
            plt.scatter(x[0], x[1], color=colors[class_item])

    plt.show()
```
```python
# определим максимальное количество итераций
max_iterations = 1

# и минимальное расстояние между центроидами до пересчета и после него, при котором нужно остановить алгоритм
min_distance = 1e-4

# сразу определим известное нам количество кластеров
k = 3
centroids, clusters = kmeans(X, 3, max_iterations, min_distance)

visualize(centroids, clusters)
```

```python
def kmeans(data, k, max_iter=100, tol=1e-4):
    
    centroids = [data[i] for i in range(k)]

    for _ in range(max_iter):
        # 2. Классификация
        classes = {i: [] for i in range(k)}
        for x in data:
            distances = [euclide(x, c) for c in centroids]
            classes[np.argmin(distances)].append(x)
        
        # 3. Сохранение старых центроидов
        old_centroids = centroids.copy()
        
        # 4. Пересчёт центроидов
        centroids = [np.mean(classes[i], axis=0) for i in range(k)]
        
        # 5. Проверка сходимости (упрощённая)
        shift = sum(euclide(c, oc) for c, oc in zip(centroids, old_centroids))
        if shift < min_distance:
            break
    
    return centroids, classes

```

### Примеры решения задачи кластеризации разными методами

В модуле sklearn, одном из основных модулей, используемых для решения задач машинного обучения, по умолчанию реализовано большое количество различных методов кластерации.

Код, который [сравнивает](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html) между собой различные методы и показывает разницу представлен ниже.

```python
import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
X, y_true = make_blobs(n_samples = 400, centers = 4, cluster_std = 0.60, random_state = 0)
plt.scatter(X[:, 0], X[:, 1], s = 20);
plt.show()
kmeans = KMeans(n_clusters = 5)
kmeans.fit(X)
y_kmeans = kmeans.predict(X)
plt.scatter(X[:, 0], X[:, 1], c = y_kmeans, s = 20, cmap = 'summer')
centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c = 'blue', s = 100, alpha = 0.9);
plt.show()
```
