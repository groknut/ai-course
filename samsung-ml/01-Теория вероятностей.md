
## Теория вероятностей

**Случайное событие** - которое при определенных условиях может произойти или нет.


**Достоверное** - обязательно произойдет

**Невозможное** - не произойдет никогда

### Математическое ожидание
Взвешанное по вероятности появления значение случайной величины

$E[X] = Σ_i{p_i*x_i}$, где

$p_i$ - вроятность появления i-ого события \
$x_i$ - значение случайной величины на i-ом эксперименте

```python
m = sum(b) / len(b)
```

### Дисперсия

Дисперсия случайной величины — мера разброса значений случайной величины относительно её математического ожидания.

$ D[X] = E[(X-E[X])^2]$

```python
D = sum((np.array(b) - m)**2)/len(b)
```

### Стандартное отклонение

Дисперсию невозможно приложить к величинам реального мира, потому для величин реального мира принята использовать стандартное отклонение, то есть корень из дисперсии, делённый на число измерений в выборке (если точнее, на число измерений в выборке -1).

```python
std = (D * len(b) / (len(b)-1))**(1/2)
```

### Мода и медиана

**Мода** — значение во множестве наблюдений, которое встречается наиболее часто. \
**Медиана** - значение, которое в упорядоченной выборке находится ровно посередине.

```python
np.mode() # мода
scipy.stats.mode() # мода
np.median() # медиана
np.mean() # среднее
```

### Медиана или математическое ожидание?

Есть известная метафора "средняя температура по больнице". Она намекает на то, что считать среднее как минимум бесполезно, без опредеделения краевых значений функций. Наиболее часто проблема расчёта "среднего" встречается при расчёте заработной платы.

### Корреляция

Довольно часто встаёт задачу оценить меру статистической связанности двух величин между собой. Для решения этой задачи существует большое число различных инструментов, однако наиболее частым из них выступает анализ "корелляции". Это настолько употребимое в обиходе слово, что принято считать, что корелляция способна математически оценить любую связь двух величин. Однако, это не так.

Корелляция Пирсона считается следующим образом: \
 $r_{xy} = \frac{Σ(X-E[X])(Y-E[Y])}{\sqrt{Σ(X-E[X])^2 Σ(Y-E[Y])^2}}$

Выражение $Σ(X-E[X])(Y-E[Y])$ - называется ковариация. 

Корелляция - мера **линейной** связи между двумя случайными величинами. 

```python
np.corrcoef(d,e)
```
Вывод будет следующим
```bash
[[1. 0.]
 [0. 1.]]
```
Как интерпретировать полученный результат? \
Это значения все возможных вариантов расчёта корелляций. \
[[$r_{xx}$, $r_{xy}$ ] \
[$r_{yx}$, $r_{yy}$ ]]

Мы видим, что корелляция между e и d отсутствуют, однако очевидно, что: \
$d = e^2$.

То есть связь есть.

Но приведенный выше пример - это не самая большая беда при использовании корелляции. Гораздо хуже - нахождение ложных зависимостей, там где их нет. 

### Нормальное распределение

**Нормальное распределение**, также называемое распределением Гаусса — распределение вероятностей, которое в одномерном случае задаётся функцией плотности вероятности, совпадающей с функцией Гаусса.

Формула, задающая нормальное распределение вероятности: 
$ f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)}$, где \
${\sigma}$ - стандартное отклонение \
${\mu}$ - мат ожидание

Функция распределения вероятности при визуализации имеет известный вид "купола".

Нормальное распределение имеет огромное значение в математической статистики потому, что показано, что сумма большого числа случайных слабозависящих друг от друга и имеющих одинаковый масштаб величин будет нормально распределена. 

Эта "центральная предельная теорема" обычно сводится в обиходе к тому, что всё в мире распределено нормально. **Это не так**. 

Однако, так как обычно в различных используемых технологиях машинного обучения, для работы с моделями применяется большое число данных, можно предполагать, что они распределены нормально. Более того, работа с моделями машинног обучения требует нормально распределённх данных. 

### Решение ошибки первого рода

Для этого достаточно взять и проверить соответствие двух значений по критерию хи-квардарт. Мы не будем вдаваться в нюансы того, что это за критерий, однако отметим, что значению pval - определяет то, есть ли у нас ошибка первого рода или нет.

Чтобы результат был удовлетворительным, нам необходимо чтобы pvalue была, обычно, больше 0.05. Это означает, что с вероятность 95% разницы между Игнатием и Пафнутием нет.

```python
import statsmodels.stats.proportion as proportion
chi2stat, pval, table = proportion.proportions_chisquare([177,180],180)
pval
```
Pvalue показывает также вероятность ошибок первого рода - то есть вероятность того, что ложный объект (гипотезу) засчитают за истинный объект (гипотезу).

Однако проблема также может крыться и в ошибках второго рода: то есть в ошибках, когда истинный объект считают ложным. Проблема ошибок второго рода кроется в числе экспериментов.

То есть должно быть достаточно экспериментов, чтобы разница была заметна.

### Решение ошибки второго рода

Полученное значение показывает то, что на выборке в 180 дней вы лишь с 5% вероятность отличите Игнатия от Пафнутия. Нам же необходимо добиться минимум 80%, а то и больше.

```python
import statsmodels.stats.power as smp

chipo = smp.GofChisquarePower()
```

```python
chipo.solve_power(effect_size=np.sqrt(((137/180 - 180/180)**2)/(137/180)), nobs = 180, alpha = 0.05, power = None)
```

То есть чтобы отличить с 95% вероятностью Игнатия от Пафнутия с вероятность ошибки первого рода 5% нужно, чтобы Игнатий опоздал в 43 случаях. С вероятность 80% - в 34 случаях.

Такое применение математической статистики называется A/B тестированием. Им занимаются все аналитики данных.
