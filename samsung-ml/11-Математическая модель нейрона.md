
## Математическая модель нейрона

Нервная система человека - это самый большой обработчик информации в человеческом теле. \
Это не единственный обработчик информации но он самый большой и самый эффективный. Нервная система человека состоит из нескольких компонентов: \
Во-первых, мозг, который отвечает за обработку информации и принятие решений. \
Информация к мозгу поступает от органов чувств по нервным волокнам. Мозг обрабатывает эту информацию и принимает решение о том, как среагировать в данной ситуации. Это решение передаётся по нервным волокнам в обратную сторону к мышцам. Таким образом происходит обработка информации в человеческом теле. 

Эти компоненты отвечают за разные задачи. Однако, все эти компоненты состоят из примерно одинаковых клеток которые называются нейронами.

### Нейрон

Это компонента, из которой состоит вся нервная система человека. Почти все нейроны устроены примерно одинаково. \
Итак, у нейрона есть ядро нейрона, которое называется также телом нейрона. В теле нейрона накапливается электрический заряд. Из тела нейрона есть множество отростков: есть маленькие отростки, есть большой отросток. 

Маленькие отростки называются дендритами. По этим отросткам сигнал от других нейронов приходит к нашему нейрону. 

Есть большой отросток. Этот отросток называется аксоном. По аксону нейрон передаёт сигнал к другим нейронам. 

Место, где аксон другого нейрона соединяется с дендритами нашего нейрона,  называется "синапсом".

Синапс - это место соприкосновения дендрита и аксона. Синапс может быть сильным, а может быть слабым. Если синапс сильный, то сигнал, который передан по аксону нейрона практически полностью перейдёт к нашему нейрону. Если же синапс слабый, то практически никакой заряд не перетечёт от аксона другого нейрона к нашему нейрону.

Синапс может изменяться со временем. В зависимости от обстоятельств синапс может становиться сильнее, а может становиться слабее. Именно с настройкой синапса и связана тренировка биологической нейронной сети и, соответственно, нервной системы человека.



### мат модель

Тело нейрона, где накапливается заряд, заменяется на сумматор. Входы заменят нам дендриты и выход заменит нам аксон, по которому нейрон отправляет сигналы другим нейронам.

Функция активации - что нейрон делает с сигналами, которые в него приходят.

Мы должны ещё вспомнить о том, что у нейрона есть синапсы, то есть сила, с которой он соединён с другими нейронами. Силу синапсов мы будем моделировать при помощи синаптических весов, которые мы будем навешивать на входы в наш нейрон.


Так, математическая модель нейрона делает следующее. Пусть у нас есть три входа $ x_0 $, $ x_1 $, $ x_2 $. Они приходят в сумматор, где они суммируются, и получается суммарный сигнал $ z $. После этого мы берём некоторую функцию, которая называется **функцией активации** $ f $, и мы получаем выход из нейрона, который здесь мы будем называть $ y $.

В виде математического выражения, можно записать такую формулировку:

$y = f(x_0 + x_1 + x_2)$.

Однако мы здесь не учли, что есть ещё веса на входы в нейрон. Соответственно, поскольку синапс изменяется и обучение биологической нейронной сети сводится к тому, что настраиваются синапсы, мы введём некоторые настраиваемые веса, которые навесим на входы в нейрон. Их будем называть $ w_0, w_1, w_2 $. Тогда наше выражение примет вид:

$y = f\big( w_0 x_0 + w_1 x_1 + w_2 x_2 \big)$.

Итак, если мы внимательно присмотримся к тому, что у нас получилось, то здесь мы сможем наблюдать линейную функцию. Однако эта линейная функция ещё не совсем полна. Это означает, что до совсем линейной функции ей чего-то не хватает. И для того, чтобы описать все линейные функции, мы добавим **смещение**. Итак, $b$ — это ещё один настраиваемый параметр, который живёт внутри нейрона.
$y = f\big( w_0 x_0 + w_1 x_1 + w_2 x_2 + b \big)$.


**Результат работы нейрона** — это функция активации, взятая от суммы входов нейрона, перемноженных с весами, плюс смещение. При этом веса и смещение — это настраиваемые параметры. 

То, что здесь получилось, можно записать несколькими способами. Это можно записать в виде суммы:

$$ y = f\left( \sum_{i=0}^{2} w_i x_i + b \right) $$

Это обозначение означает, что мы суммируем все индексы. Кроме того, это можно записать и другим способом — через скалярные произведения. Потому, что то что здесь написано — это то же самое, что скалярное произведение $\mathbf{w}$ с $\mathbf{x}$.

В векторной форме:

$$ y = f\big( \mathbf{w} \cdot \mathbf{x} + b \big) $$

где $\mathbf{w} = (w_0, w_1, w_2)$, $\mathbf{x} = (x_0, x_1, x_2)$.

### Функция активации

Вернёмся к функции активации. В биологическом нейроне функция активации у нас пороговая. То есть это означает, что до тех пор, пока в нейроне не накопилось достаточное количество заряда, ничего не происходит, и когда там накопилось достаточное количество заряда, нейрон выстреливает какой-то сигнал. Это не единственная функция активации, которую мы можем применять в нейронных сетях, но пока что мы остановимся на этой функции.

Итак, мы с вами рассмотрели математическую модель нейрона. Кроме того, мы рассмотрели с вами самую простую функцию активации — **пороговую функцию**. У этой пороговой функции активации есть два сильно отличающихся друг от друга значения: $0$ и $1$.

Место, где функция активации изменяет своё значение с нуля на единицу, мы будем называть **разделяющей поверхностью**. Разделяющая поверхность находится там, где аргумент функции активации равен нулю. Именно в этом месте происходит смена значения функции активации.

Итак, как же выглядит разделяющая поверхность? Рассмотрим формулу того, что делает наш нейрон:

$$ y = f(\mathbf{w} \cdot \mathbf{x} + b) $$

Наш нейрон делает какую-то линейную операцию, которая задаётся двумя параметрами: **вектор весов $\mathbf{w}$** и **смещение $b$**. Соответственно, разделяющая поверхность задаётся уравнением, когда аргумент функции активации равен нулю:

$$ \mathbf{w} \cdot \mathbf{x} + b = 0 $$

Это уравнение задаёт прямую (в двумерном случае) или гиперплоскость (в многомерном). С одной стороны от этой прямой, функция активации равна единице, с другой стороны от этой прямой, функция активации равна нулю. Функция активации равна единице с той стороны от разделяющей поверхности, в которую показывает вектор $\mathbf{w}$. Соответственно, с другой стороны, функция активации равна нулю.

Таким образом, разделяющая поверхность — это какая-то плоскость (или гиперплоскость), которая задаётся вектором $\mathbf{w}$ и смещением $b$.

### сигмоида

Мы с вами рассмотрели пороговую функцию активации, однако это далеко не единственная функция, которая встречается в нейронных сетях. Мы с вами сейчас рассмотрим ещё одну функцию активации.

Итак, пороговая функция активации — это функция, которая принимает два значения $0$ и $1$, и значение ноль принимается, когда аргумент функции меньше либо равен нулю, и значение $1$ принимается в том случае, когда аргумент функции больше нуля. Это не очень хорошая функция, мы позже с вами разберём, почему. Но для того, чтобы улучшить некоторые характеристики этой функции, используется другая функция, у которой нет разрыва в точке 0.

Пороговая функция активации выглядит вот так, у неё есть разрыв в точке 0. Рассмотрим функцию, которая задаётся следующей формулой:

$$ \sigma(x) = \frac{1}{1 + e^{-x}} $$

Эта функция называется **сигмоидой**.

Проанализируем эту функцию:

1. **Когда аргумент стремится к $+\infty$:**  
   Когда $x$ очень большой, $e^{-x}$ — почти ноль. Соответственно, сигмоида принимает значение $1$:
   $$ \lim_{x \to +\infty} \sigma(x) = 1 $$

2. **Когда $x$ стремится к $-\infty$:**  
   В этом случае $e^{-x} = e^{|x|}$ — очень большое положительное число, значит:
   $$ \lim_{x \to -\infty} \sigma(x) = 0 $$

Таким образом, при $x \to -\infty$, $\sigma(x) \to 0$.

При этом стоит отметить, что сигмоида — всегда положительное число, потому что $e^{-x} > 0$, и в знаменателе всегда число больше 1. То есть здесь никак не может возникнуть отрицательного числа. Кроме того, мы должны отметить, что сигмоида меньше единицы, потому что $e^{-x} > 0$, соответственно, знаменатель $1 + e^{-x} > 1$, и таким образом, сигмоида не может быть больше единицы.

Когда $x = 0$ сигмоида равна $0.5$. Это достаточно просто проверить, просто подставив вместо $x$ ноль:

$$ \sigma(0) = \frac{1}{1 + e^{0}} = \frac{1}{1 + 1} = \frac{1}{2} $$

Довольно интересным свойством обладает эта функция. Если мы добавим параметр температуры $T$, то при параметре $T$, стремящимся к нулю, $\sigma(x/T)$ будет стремиться к ступенчатой функции. Рассмотрим сигмоиду с температурой:

$$ \sigma_T(x) = \frac{1}{1 + e^{-x/T}} $$

Действительно, если $T$ будет очень маленьким числом, то:

- Если $x > 0$ (даже небольшое), то $-x/T$ — очень большое отрицательное число, $e^{-x/T}$ почти ноль, и $\sigma_T(x) \approx 1$.
- Если $x < 0$ (даже небольшое), то $-x/T$ — очень большое положительное число, $e^{-x/T}$ огромно, и $\sigma_T(x) \approx 0$.

Чем меньше значения температуры, тем резче переход от нуля к единице.
