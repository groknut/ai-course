
## от нейрона к нейронной сети

Один нейрон нам даёт **линейную разделяющую поверхность**. Это значит, что между областью значений $0$ и $1$ при пороговой функции активации у нас прямая линия (или гиперплоскость в многомерном случае).  

Когда же мы будем соединять нейроны в более сложную конструкцию — а именно в **нейронную сеть** — то мы можем получать **нелинейные разделяющие поверхности**. Более того, эти поверхности могут формировать даже **несвязанные области**.  
Например, можно решать такую задачу и получить вот такую разделяющую поверхность с выделенной областью:

---

### Имеет ли смысл соединять несколько нейронов с **линейной** функцией активации?

Рассмотрим этот вопрос. Допустим, у нас есть три нейрона с **линейной функцией активации** $f(z) = z$ и два входа $x_1$ и $x_2$.  

**Структура сети:**  
- Первый слой: два нейрона  
  - Нейрон 1: $y_1 = f(w_{11} x_1 + w_{12} x_2 + b_1)$  
  - Нейрон 2: $y_2 = f(w_{21} x_1 + w_{22} x_2 + b_2)$  
- Второй слой: один нейрон  
  - Нейрон 3: $y = f(w_1 y_1 + w_2 y_2 + b_3)$  

**Что будет делать такая сеть?**  
Поскольку $f(z) = z$ (линейная функция), то:

$$
\begin{aligned}
y &= f(w_1 y_1 + w_2 y_2 + b_3) \\
  &= w_1 y_1 + w_2 y_2 + b_3 \\
  &= w_1 (w_{11} x_1 + w_{12} x_2 + b_1) + w_2 (w_{21} x_1 + w_{22} x_2 + b_2) + b_3 \\
  &= (w_1 w_{11} + w_2 w_{21}) x_1 + (w_1 w_{12} + w_2 w_{22}) x_2 + (w_1 b_1 + w_2 b_2 + b_3)
\end{aligned}
$$

**Итог:**  
Получилось выражение вида:

$$
y = \tilde{w}_1 x_1 + \tilde{w}_2 x_2 + \tilde{b}
$$

где $\tilde{w}_1$, $\tilde{w}_2$ и $\tilde{b}$ — новые константы.

---

### Вывод
**Не имеет смысла** скармливать результаты работы линейных нейронов в другой линейный нейрон, если каждый из первых линейных нейронов получал на вход все те же аргументы, что и остальные нейроны в этом же слое.  

Вся такая многослойная конструкция **сводится к одному линейному нейрону**, потому что композиция линейных функций остаётся линейной функцией.

Чтобы получить нелинейность и более выразительные разделяющие поверхности, необходимо использовать **нелинейные функции активации** (сигмоиду, ReLU, гиперболический тангенс и т.д.).
