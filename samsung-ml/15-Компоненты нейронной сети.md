
## Компоненты нейронной сети


### Настраиваемые параметры
Пусть в скрытом слое $M$ нейронов. Тогда параметры:

1. **Первый слой (скрытый):**
   - Веса $w^{(1)}_j$ — определяют "крутизну" сигмоиды $j$-го нейрона.
     - Большой вес → резкий переход от 0 к 1.
     - Малый вес → плавный переход.
   - Смещения $b^{(1)}_j$ — сдвигают сигмоиду вдоль оси $x$.
     - Положительное смещение → сдвиг влево.
     - Отрицательное смещение → сдвиг вправо.

2. **Второй слой (выходной):**
   - Веса $w^{(2)}_j$ — масштабируют выход $j$-й сигмоиды (определяют амплитуду).
     - Если сигмоида меняется от 0 до 1, то после умножения на $w^{(2)}_j$ она будет меняться от 0 до $w^{(2)}_j$.
   - Смещение $b^{(2)}$ — глобальное смещение выхода сети.

### Как сеть аппроксимирует функцию?
Выход сети:
$$
\tilde{y}(x) = b^{(2)} + \sum_{j=1}^{M} w^{(2)}_j \cdot \sigma\!\left(w^{(1)}_j x + b^{(1)}_j\right)
$$
где $\sigma(z) = \frac{1}{1+e^{-z}}$ — сигмоида.

Каждый нейрон скрытого слоя добавляет одну "настроенную" сигмоиду. Их сумма позволяет аппроксимировать сложные зависимости.

### Иллюстрация подбора параметров (ручной подход)
Рассмотрим графическую иллюстрацию:
1. **Первый нейрон** настраивается так, чтобы аппроксимировать левую часть зависимости.
   - Смещение $b^{(1)}_1$ определяет положение центра сигмоиды.
   - Вес $w^{(2)}_1$ задаёт амплитуду.
   - Вес $w^{(1)}_1$ регулирует крутизну.
2. **Второй нейрон** добавляется для коррекции формы в средней части.
3. **Третий нейрон** и далее уточняют аппроксимацию в остальных областях.

Таким образом, сумма сигмоид с разными параметрами постепенно приближается к целевой функции.

---

## Универсальная аппроксимация
**Теорема (Universal Approximation Theorem):**
Полносвязная нейронная сеть с одним скрытым слоем, содержащим достаточное количество нейронов с нелинейной функцией активации (например, сигмоидой), может приблизить **любую непрерывную функцию на компакте** с любой заданной точностью.

На практике это означает, что такие сети способны моделировать очень широкий класс зависимостей.

---

## Функция потерь
Для задачи восстановления зависимости наиболее популярна **среднеквадратичная ошибка (MSE)**:
$$
L = \frac{1}{N} \sum_{i=1}^{N} \left( \tilde{y}_i - y_i \right)^2
$$
где:
- $\tilde{y}_i$ — предсказание модели для $x_i$,
- $y_i$ — истинное значение (из обучающей выборки).

MSE измеряет среднее квадратичное отклонение предсказаний от истинных значений. Чем меньше MSE, тем лучше модель аппроксимирует данные.
