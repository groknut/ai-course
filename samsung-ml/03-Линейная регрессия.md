
## Линейная регрессия

Линейная регрессия - самый простой инструмент выявления зависимости между различными числовыми признакми. Часто линейную регрессию относят к методам машинного обучения, однако на самом деле - это не так.

Восстановить линейную регрессию - то есть линейную зависимость между различными числовыми данными - можно с помощью аналитического решения. Тем не менее, так как линейная ргрессия решает ту же задачу, что и машинное обучение - задачу восстановления зависимости между данными - для регрессии применяются те же термины и понятия, что и для машинного обучения.

### Базовые понятия, используемые в машинном обучении

Центральным понятием машинного обучения является _обучающая выборка_. Это примеры, на основе которых планируется строить общую закономерность. Она обозначается $X$ и состоит из $l$ пар объектов $x_{i}$ и известных ответов $y_{i}$:

$$X = (x_{i}, y_{i})^l_{i=1}.$$

Функция, отображающая пространство объектов $\mathbb{X}$ в пространство ответов $\mathbb{Y}$, позволяющая делать предсказания, называется _алгоритмом_ или _моделью_ и обозначается $a(x)$. Она принимает на вход объект и выдает ответ.

Отметим, что $ x_{i} = (x^{1}, x^{2}, ..., x^{d}) $. То есть каждый объект $x_{i}$ состоит из ряда различных значений. 

### Линейная регрессия

#### Линейная модель

Для начала введём проствую базовую линейную модель, записанную следующим образом:

$$a(x) = w_{0} + w_{1}x^{1} + ... + w_{d}x^{d} = w_{0}+\sum^{d}_{i=1}w_{i}x^{i}.$$

Параметрами модели $a(x)$ являются веса $w_{i}$. Вес $w_{0}$ называется _свободным коэффициентом_, _сдвигом_ или _смещением_. Оптимизация модели заключается в подборе оптимальных значений весов. Сумму в формуле также можно описать как скалярное произведение вектора признаков $x=(x^{1},...,x^{d})$ на вектор весов $w=(w_{1},...,w_{d})$:
 
$$a(x) = w_{0}+\left \langle w,x \right \rangle.$$
 
Чтобы сделать модель однородной и упростить оптимизацию вводится фиктивный признак $x^{0}$ всегда равный единице. Таким образом: 

$$a(x) = \left \langle w,x \right \rangle = \sum^{d}_{i=0}w_{i}x^{i} $$

#### Функционал ошибки

Для обучения модели и сравнения различных моделей между собой необходимо разработать математическую формулу и алгоритм расчёта ошибки модели. В рамках решения задачи оптимизации обычно ставится задача достижения минимального значения функционала ошибки. 

В качестве очевидного решения можно предложить такую формулу:
$Q(a,y)=a(x)-y$. Однако, у неё есть принципиальная проблема: у такой функции не существует минимума. 

С учётом этого, логичным кажется решение использовать в качестве функции для расчёта ошибки модуль отклонения $Q(a,y)=|a(x)-y|$. Соответствующий функционал ошибки называется средним абсолютным отклонением (mean absolute error, MAE):

$$Q(a,x) = \frac{1}{l}\sum^{l}_{i=1}|a(x_{i})-y_{i}|.$$

Однако, мы уже разбирали, что функционал ошибки или функция потерь, должны быть дифференцируемы, а значит, необходимо предложить другой вариант базовой функции для расчёта ошибки: $Q(a,y)=(a(x)-y)^{2}$. Такая функция является гладкой и имеет производную в каждой точке, а ее минимум достигается при равенстве истинного ответа $y$ и прогноза $a(x)$.

Основанный на этой функции функционал ошибки называется _среднеквадратичным отклонением_ (mean squared error, MSE):

$$Q(a,x) = \frac{1}{l}\sum^{l}_{i=1}(a(x_{i})-y_{i})^{2}.$$

#### Вывод аналитической формулы для решения линейной регрессии 

Таким образом, задача расчёт оптимальных коэффициентов математически может быть записана следующим образом.

$$Q(w,x) = \frac{1}{l}\sum^{l}_{i=1}(\left \langle w,x_{i} \right \rangle-y_{i})^{2} \rightarrow \underset{w}{\text{min}}.$$

Если вас смущает возникшая, казалось бы путаница, между различными записями, то целесообразно ввести матричную запись признаков:

$$X = \begin{pmatrix}
x_{11} & ... & x_{1d}\\ 
... & ... & ...\\ 
x_{l1} & ... & x_{ld}
\end{pmatrix},$$

$$y = \begin{pmatrix}
y_{1}\\ 
...\\ 
y_{l}
\end{pmatrix}.$$

Таким образом, задача начинает выглядеть следующим образом

$$Q(w, X) = \frac{1}{l}||Xw-y||^{2}\rightarrow \underset{w}{\text{min}},$$

Взяв производную (от матрицы (!)) и прировняв её к нулю (точка экструма квадратинчной функции) можно получить значения коэффициентов:

$$w = (X^{T}X)^{-1}X^{T}y.$$

Если вам интересна более детальная математическая подоплёка смотрите её тут: [Вывод аналитической формулы решения уравнения линейной регрессии](https://habr.com/ru/company/ods/blog/323890/#metod-naimenshih-kvadratov) (см. пункт 1.2)

#### Практика

```python
import numpy as np 
from sklearn.linear_model import LinearRegression # именно в библиотеке sklearn содержится наиболее оптмаильный метод для вычисления линейной регрессии

x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
y = np.array([5, 20, 14, 32, 22, 38])

import matplotlib.pyplot as plt
fig, ax = plt.subplots()
ax.scatter(x, y)
plt.show()
```
Создаем модель линейной регрессии - т.е. прямой которая будет проходить через наши точки \
С помощью метода fit() применяем нашу модель к нашему набору данных - обучем модель \
С помощью .fit() вычисляются оптимальные значение весов w₀ и w₁ и т.д, используя существующие вход и выход (x и y) в качестве аргументов. 

```python
model = LinearRegression()
model.fit(x, y)
```
Получить значения коэффициентов `w0` и `w1`
```python
w0 = model.intercept_
w1 = model.coef_
```

$$ y = w_{0} + w_{1}x $$
$$ y = 5.63 + 0.54*x $$

Прямые вычисления
```python
np.linalg.inv(x.T@x)@x.T@y
```
В массив надо добавить 1 в каждую пару
```python
np.linalg.inv(x1.T@x1)@x1.T@y
```
Визуализируем результат
```python
# предсказание результатов 
y_pred = model.predict(x)
# y = np.array([5, 20, 14, 32, 22, 38])
plt.scatter(x, y)
plt.plot(x, y_pred, color='red', linewidth=2);
plt.show()
```

**Как работает метод `predict` в `sklearn`** \
для одномерной регрессии
```python
y_pred = model.intercept_ + model.coef_[0] * x
```
для многомерной регрессии
```python
# Для нескольких переменных
y_pred = model.intercept_ + model.coef_[0]*x1 + model.coef_[1]*x2 + ... + model.coef_[n]*xn

# Или в векторной форме:
y_pred = model.intercept_ + np.dot(X, model.coef_)
```


### Почему регрессия "Линейная"

Линейная регрессия никогда не уловит квадратичную зависимость в данных.

Метод линейной регресси используется для проверки гипотезы наличия линейной связи между двумя или несколькими переменными. Часто при первых использованиях не придается достаточной значимости тому, что
- оценивается имеено линейная связь
- выбросы сильно влияют на результат оценивания


1. устанавливает линейную зависимость в данных
2. может быть решена без методов МО
3. чувствительна к выбросам
4. не ухватывает нелинейные зависимости
5. вертикальные зависимости регрессия никогда не увидит
