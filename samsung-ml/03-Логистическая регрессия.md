
## Логистическая регрессия

### Теоретические основы бинарной классификации

Простейшим случаем является _бинарная классификация_, то есть случай, когда у нас имеется два класса. Ключевое отличие от линейной регресси состоит в том, что пространство ответов состоит из двух элементов, например, возьмём $\mathbb{Y} = \{-1,1\}$, где -1 и 1 означают принадлежность к первому или второму классу, соответственно. Для задачи распознавания спам-писем эти значиния могли бы занчить: -1 означало, что письмо не является спамом, а 1 - что является.

В случае, если мы используем линейную модель, то для определения отношения к одному из двух классов достаточно взять знак результата работы этой модели. То есть на самом деле мы используем практически ту же модель, что и в случае с решением задачи регрессии:
$$ a(x) = \text{sign} \left ( \sum^{d}_{i=0}w_{i}x^{i} \right ) = \text{sign} \left ( \left \langle w,x \right \rangle \right ).$$ 

Множество точек $\left \langle w,x \right \rangle = 0$ образует _гиперплоскость_ в пространстве признаков и делит его на две части. Объекты, расположенние по разные стороны от нее, относятся к разным классам. 

_Гиперплоскость_ - это обобщённое название плоскости для многомерного пространство (в случае двумерного пространство плоскость - это _прямая_). А часть математических оснований того, что мы будем здесь и далее обсуждать будет относится к разедам теории вероятности и линейной алгебры, которую изучают на 1-2 курсе университета. Разделяющая плоскость на рисунке ниже и будет называться гиперплоскостью в 3-ёх мерном пространстве. 

На основании законов линейной алгебры, можно вывести, что расстояние до гиперплоскости будет равняться $\frac{| \left \langle w,x \right \rangle |}{||w||}$, соответственно, при классификации важен не только знак скалярного произведения $\left \langle w,x \right \rangle$, но и его значение: чем больше значение, тем больше будет расстояние от объекта до гиперплоскости, что будет означать, что алгоритм более уверен в отнесении объекта к данному классу. 

Для корректного расчёта положения гиперплоскости значение _смещения_ должно быть равно скалярному произведению вектора весов $w$ на вектор признаков $x$, умноженному на истинное значение ответа $y$, которое, как мы помним, принимает значения -1 и 1:

$$M_{i}=y_{i}\left \langle w,x_{i} \right \rangle.$$

Если скалярное произведение отрицательно, и истинный ответ равен -1, смещение больше нуля. Если скалярное произведение положительно, и истинный ответ равен 1, смещение также будет положительным. То есть $M_{i}>0$, если классификатор даёт верный ответ, и $M_{i}<0$, когда классификатор ошибается. Смещение характеризует корректность ответа, а его абсолютное значение свидетельствует о расстоянии от разделяющей гиперплоскости.

Функционал ошибки задаётся для классификации очень просто и естественно. По существу - необходимо минимизировать минимальное число ответов. 

$$Q(a, X) = \frac{1}{l}\sum^{l}_{i=1}[a(x_{i}) \neq y_{i})]$$

или, используя понятие смещения,

$$Q(a, X) = \frac{1}{l}\sum^{l}_{i=1}[M_{i}<0] = \frac{1}{l}\sum^{l}_{i=1}[y_{i}\left \langle w,x_{i} \right \rangle < 0)].$$

### Визуализация функции потерь

```python
import matplotlib.pyplot as plt
import numpy as np

def loss_function(x):
    return 0 if x > 0 else 1
data = np.linspace(-1, 1, 100)
loss = [loss_function(x) for x in data]

plt.xlabel('Пороговая функция')
plt.xlim(-1, 1)
plt.plot(data, loss)
plt.show()
```

### Проблемы пороговых функций

Известно, что пороговая функция недифференцируема, то есть от неё нельзя взять производную, а значит и нельзя использовать градиентный спуск. В практике используют функции, которые дифференцируемы и больше по значению, чем функция потерь. То есть:

$$[M_{i}<0] \leq \tilde{L}(M_{i}).$$

Тогда минимизировать уже надо эту новую функцию:

$$Q(a, X) \leq \tilde Q(a, X) = \frac{1}{l}\sum^{l}_{i=1}\tilde{L}(M_{i}) \rightarrow \underset{w}{\text{min}}.$$

Примеры:

- _экспоненциальная функция потерь_ $\tilde{L}(M_{i}) = \text{exp}(- M_{i})$;


- _квадратичная функция потерь_ $\tilde{L}(M_{i}) = (1- (M_{i}))^{2}$;


- _логистическая функция потерь_ $\tilde{L}(M_{i}) = \text{log}_{2}(1 + \text{exp}(- M_{i}))$;

```python
def exp_loss_func(x):
    return np.exp(-x)

def square_loss(x):
    return (1 - x) ** 2

def logistic_loss(x):
    return np.log2(1 + np.exp(-x))
```

```python
exp_loss = [exp_loss_func(x) for x in data]
logistic_loss = [logistic_loss(x) for x in data]
square_loss = [square_loss(x) for x in data]

plt.xlabel('Функции потерь')
plt.xlim(-1, 1)
plt.plot(data, loss)
plt.plot(data, exp_loss)
plt.plot(data, square_loss)
plt.plot(data, logistic_loss)
plt.legend(['Пороговая функция', 'Экпоненциальная функция потерь', 'Квадратичная функция потерь', 'Логистическая функция потерь'])
plt.show()
```

### Логистическая регрессия

_Логистическая регрессия_ — частный случай линейного классификатора, обладающий одной полезной особенностью — помимо отнесения объекта к определённому классу она умеет прогнозировать вероятность $P$ того, что объект относится к этому классу. Логистическая регрессия потому называется регрессией, что выводится из линейной регрессии.

Однако, _логистическая регрессия_ вовсе не _регрессия_ по своей природе, а _классификатор_. Не перепутайте =)

Пусть в каждой точке пространства объектов $\mathbb{X}$ задана вероятность того, что объект $x$ будет принадлежать к классу «+1» $P(y=1|x)$. Она будет принимать значения от 0 до 1 - наша задача её предсказывать. Решение этой задачи регрессионным методом выглядит так: $b(x)=\left \langle w,x_{i} \right \rangle$. 

Но у этого решения есть проблема, связанная с тем, что скалярное произведение $\left \langle w,x_{i} \right \rangle$ не всегда возвращает значения в отрезке [0, 1]. 

Чтобы достичь такого условия, можно использовать некую функцию $\sigma:\mathbb{R} \rightarrow [0,1]$, которая станет переводить полученное в скалярном произведении значение в вероятность, а пределы такой вероятности будут лежать в промежутке от 0 до 1. В модели логистической регрессии в качестве такой функции берётся сигмоида, которая имеет вид:

$$\sigma(z) = \frac{1}{1 + exp(-z)}.$$

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

data = np.linspace(-10, 10, 100)
sigmoid_value = list(map(sigmoid, data))

plt.xlabel('x')
plt.ylabel('Значение сигмоидальной функции')
plt.plot(data, sigmoid_value)
plt.show()
```

$P(y=1|x)$, для краткости обозначим $p_{+}$, будет равняться

$$p_{+} = \frac{1}{1 + exp(-\left \langle w,x_{i} \right \rangle)}$$

Чем больше будет скалярное произведение $\left \langle w,x_{i} \right \rangle$, тем выше окажется предсказанная вероятность.

При этом мы знаем как рассчитать скалярное производение, но не знаем, как рассчитать вероятность. Выведем необходимые значения:

$$\left \langle w,x_{i} \right \rangle = \text{ln} \frac{p_{+}}{1 - p_{+}}.$$

Выражение под логарифмом называется _риском_ (отношение вероятности положительного события к, по сути, вероятности отрицательного события), а вместе с логарифмом это выражение называется _логитом_. Поэтому метод и называется логистической регрессией: мы приближаем логит линейной комбинацией признаков и весов.

Для обучения этой модели (а точнее, подбора расчётной функции, которую можно использовать для обучения) нам потребуется использовать _метод максимального правдоподобия_ю Его сущность заключается в выборе гипотезы, при которой вероятность получить имеющееся наблюдение максимальна.

С точки зрения реализуемого алгоритма, вероятность того, что в выборке встретится объект $x_{i}$ c классом $y_{i}$, равна

$$P(y=y_{i}|x_{i}) = p_{+}^{[y_{i}=+1]}(1-p_{+})^{[y_{i}=-1]}.$$

Для всей выборки это значение будет равно производению её элементов:

$$P(y|X) = L(X) = \prod^{l}_{i=1} p_{+}^{[y_{i}=+1]}(1-p_{+})^{[y_{i}=-1]}.$$

Проблема производения в том, что его крайне тяжело оптимизировать. Для решения проблемы "произведения" рационально использовать логарифмирование:

$$-\text{ln}L(X) = -\sum^{l}_{i=1}([y_{i} = +1] \text{ln}p_{+} + [y_{i} = -1]\text{ln}(1 - p_{+})).$$

Эта функция потерь называется _логарифмической функцией потерь (log loss),_ или _кросс-энтропией_.

Обратите внимание на знак "-" перед логарифмом. Он необходим для того, чтобы перейти от задачи "максимального" праводоподобия к задаче "минимального" правдоподобия.

Когда $y$ принимает значения 0 и 1, log loss запишется как

$$-\text{ln}L(X) = -\sum^{l}_{i=1} (y_{i} \text{ln}\frac{1}{1 + exp(-\left \langle w,x_{i} \right \rangle)} + (1 - y_{i})\text{ln} (1-\frac{1}{1 + exp(-\left \langle w,x_{i} \right \rangle)})).$$

$$-\text{ln}L(X) = -\sum^{l}_{i=1} (y_{i} \text{ln}(\sigma) + (1 - y_{i})\text{ln} (1-\sigma)).$$

### Практическая реализация

```python
from sklearn import datasets
from matplotlib.colors import ListedColormap

# сгеренируем данные
classes = datasets.make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_classes=2, random_state=1)

# и изобразим их на графике
colors = ListedColormap(['red', 'blue'])

plt.figure(figsize=(8, 8))
plt.scatter([x[0] for x in classes[0]], [x[1] for x in classes[0]], c=classes[1], cmap=colors)

plt.show()
```
При работе с набором данных их рекомендуется перемешивать перед тем, как разделять на обучающую и тестовые выборки. 

```python
# перемешивание датасета
np.random.seed(12) # это число позволяет постоянно получать одну и ту же "случайность"
shuffle_index = np.random.permutation(classes[0].shape[0])
X_shuffled, y_shuffled = classes[0][shuffle_index], classes[1][shuffle_index]

# разбивка на обучающую и тестовую выборки
train_proportion = 0.7
train_test_cut = int(len(classes[0]) * train_proportion)

X_train, X_test, y_train, y_test = \
    X_shuffled[:train_test_cut], \
    X_shuffled[train_test_cut:], \
    y_shuffled[:train_test_cut], \
    y_shuffled[train_test_cut:]
    
print("Размер массива признаков обучающей выборки", X_train.shape)
print("Размер массива признаков тестовой выборки", X_test.shape)
print("Размер массива ответов для обучающей выборки", y_train.shape)
print("Размер массива ответов для тестовой выборки", y_test.shape)
```

Далее транспонируем матрицы данных, так как нам удобнее работать со строками.

```python
X_train_tr = X_train.transpose()
y_train_tr = y_train.reshape(1, y_train.shape[0])
X_test_tr = X_test.transpose()
y_test_tr = y_test.reshape(1, y_test.shape[0])
```

Оптимизируем функционал ошибки, используя градиентный спуск, его вид в случае использования такой функции потерь будет:

$$w_{n+1} = w_{n} - \eta \frac{1}{l}X( \frac{1}{1 + exp(-\left \langle w,x_{i} \right \rangle)}-Y)^{T},$$

```python
def log_loss(w, X, y):
    m = X.shape[1]

    # используем функцию сигмоиды, написанную ранее
    A = sigmoid(np.dot(w.T, X))
    
    loss = -1.0 / m * np.sum(y * np.log(A) + (1 - y) * np.log(1 - A))
    loss = np.squeeze(loss)
    grad = 1.0 / m * np.dot(X, (A - y).T)
    
    return loss, grad
```
```python
def optimize(w, X, y, n_iterations, eta):
#     потери будем записывать в список для отображения в виде графика
    losses = []
    
    for i in range(n_iterations):        
        loss, grad = log_loss(w, X, y)
        w = w - eta * grad

        losses.append(loss)
        
    return w, losses
```

```python
def predict(w, X, b=0.5):
    
    m = X.shape[1]
    
    y_predicted = np.zeros((1, m))
    w = w.reshape(X.shape[0], 1)
    
    A = sigmoid(np.dot(w.T, X))
    
#     За порог отнесения к тому или иному классу примем вероятность 0.5
    for i in range(A.shape[1]):
        if (A[:,i] > b): 
            y_predicted[:, i] = 1
        elif (A[:,i] <= b):
            y_predicted[:, i] = 0
    
    return y_predicted
```

```python
# инициализируем начальный вектор весов
w0 = np.zeros((X_train_tr.shape[0], 1))

n_iterations = 1000
eta = 0.05

w, losses = optimize(w0, X_train_tr, y_train_tr, n_iterations, eta)

y_predicted_test = predict(w, X_test_tr)
y_predicted_train = predict(w, X_train_tr)

# В качестве меры точности возьмём долю правильных ответов
train_accuracy = 100.0 - np.mean(np.abs(y_predicted_train - y_train_tr)*100.0)
test_accuracy = 100.0 - np.mean(np.abs(y_predicted_test-y_test_tr)*100.0)

print(f"Итоговый вектор весов w: {w}")
print(f"Точность на обучающей выборке: {train_accuracy:.3f}")
print(f"Точность на тестовой выборке: {test_accuracy:.3f}")
```
```python
data = np.linspace(-2, 2, 100)

plt.figure(figsize=(8, 8))
plt.scatter([x[0] for x in classes[0]], [x[1] for x in classes[0]], c=classes[1], cmap=colors)
plt.plot(data, w[0,0]*data+w[1,0])

plt.show()
```
```python
plt.title('Log loss')
plt.xlabel('iterations')
plt.ylabel('loss')
plt.plot(range(len(losses)), losses)

plt.show()
```

В заверешении подберём различные значения порога для определения точности
```python
test_acc = []
b_l = []

for b in range(0, 101):
  y_predicted_test = predict(w, X_test_tr, b/100)
  y_predicted_train = predict(w, X_train_tr, b/100)

  # В качестве меры точности возьмём долю правильных ответов

  test_accuracy = 100.0 - np.mean(np.abs(y_predicted_test-y_test_tr)*100.0)

  b_l.append(b/100)
  test_acc.append(test_accuracy)

plt.plot(b_l, test_acc)
plt.show()
```

### Самостоятельная реализация
```python
# линейный классификатор
np.sign(np.dot(x, w) + b)

# сигмоида
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def fit(x, y, lr, n_iter):
    n, m = x.shape
    
    x_aug = np.c_[np.ones(m), x.T]
    
    w = np.zeros((n+1, 1))
    
    for _ in range(n_iter):
        A = sigmoid(x_aug @ w).T
        grad = 1.0 / m * (x_aug.T @ (A - y).T)
        w -= lr * grad
    return w

def predict(w, x, threshold=0.5):
    m = x.shape[1]
    x_aug = np.c_[np.ones(m), x.T]
    A = sigmoid(x_aug @ w).T
    
    return (A > threshold).astype(int)
```
