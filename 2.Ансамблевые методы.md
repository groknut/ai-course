# Ансамблевые методы

## 1. Мотивация использования ансамблей

**Ключевое ограничение решающего дерева:**

- на выходе всегда формируется **ограниченное число состояний (классов)**

Следствия:

- решающие деревья хорошо подходят для **классификации** (например: кошка / собака / велосипед)
- плохо подходят для задач **непрерывного прогноза** (например: температура)

**Идея повышения точности:**
- использовать не одно решающее дерево, а **набор независимых моделей**
- итоговый прогноз получать на основе их совместного решения

---

## 2. Общая идея ансамблевых методов

**Ансамблевые методы** — это методы, в которых:
- целевая функция прогнозируется
- не одним алгоритмом
- а **набором независимых решающих алгоритмов**

После этого:
- результаты отдельных моделей **комбинируются**
- по фиксированному правилу

Ансамблевые методы применимы:
- в задачах классификации
- в задачах прогноза (регрессии)

---

## 3. Основные типы ансамблевых методов

Выделяют три основных подхода:

1. **Bagging**
    - самый быстрый в обучении
2. **Stacking**
    - более медленный
    - обычно даёт более точный прогноз
3. **Boosting**
    - самый мощный из трёх
    - предпочтителен при достаточных вычислительных ресурсах

Все ансамблевые методы:
- являются **двухэтапными**
- последние два (stacking и boosting) явно используют двухэтапное обучение

---

## 4. Обеспечение независимости моделей в ансамбле

В ансамблевых схемах строится **несколько независимых решающих деревьев**.

Независимость достигается:
- разными схемами обрезки
- разными алгоритмами построения деревьев
- обучением на различных подвыборках данных

---

## 5. Способы разбиения исходного датасета

### 5.1. Случайное разбиение без пересечений

- данные делятся на непересекающиеся поддатасеты
- один и тот же объект не встречается в разных наборах

### 5.2. Случайное разбиение с пересечениями

- поддатасеты формируются случайной выборкой
- одни и те же объекты могут входить в разные наборы

### 5.3. Разбиение по признакам

- разные модели обучаются на разных подмножествах признаков
- возможны варианты:
    - с пересечением признаков
    - без пересечения признаков

---

## 6. Bagging (Bootstrap Aggregating)

### 6.1. Основная идея

В контексте решающих деревьев **bagging** часто называют **случайным лесом**.

Процесс:

- строится множество решающих деревьев
- каждое дерево обучается **независимо**
- деревья различаются:
    - обучающими данными
    - ограничениями
    - параметрами построения

Каждое дерево выдаёт **собственный прогноз**.

---

### 6.2. Формирование итогового прогноза

Результирующее решение получается путём:

- усреднения прогнозов
- взятия медианы
- выбора наиболее вероятного значения

То есть применяется **статистическая агрегация** результатов отдельных деревьев.

Обычно bagging эффективен, когда:
- число деревьев велико (100 и более)

---

### 6.3. Схема обучения и прогноза

- обучение проводится **в один этап**
- все деревья строятся независимо
- при прогнозе:
    - результаты всех деревьев подаются на сумматор
    - сумматор выдаёт итоговое решение

---

### 6.4. Преимущества и недостатки bagging

**Преимущества:**
- хорошая параллелизация обучения
- хорошая параллелизация прогнозирования

**Недостатки:**
- на малом числе деревьев точность часто невысока
- не гарантирует превосходство над одиночным деревом

---

## 7. Разновидности bagging по способу выборки данных

- **Pasting**  
    выборка без возвращения (непересекающиеся датасеты)
- **Bagging**  
    выборка с возвращением (пересекающиеся датасеты)
- **Random Subspaces**  
    выборка различных компонент входного вектора
- **Random Patches**  
    выборка как компонент вектора, так и частей датасета
## 8. Stacking (стекинг)

### 8.1. Идея метода

**Stacking** — ансамблевый метод, ориентированный на эффективную работу с большими объёмами данных и распределёнными вычислениями.

Основная мотивация:
- наличие большого объёма данных
- наличие множества независимых вычислительных узлов
- данные физически распределены (облачные хранилища)

Принцип:
- каждый вычислительный узел обучает свою модель
- используется только локальный фрагмент данных
- результаты моделей объединяются на следующем этапе

---

### 8.2. Отличие от bagging

Ключевое свойство stacking:

- **гарантированное неухудшение качества прогноза**

Следствия:

- ансамбль не может работать хуже, чем одиночное дерево
- даже ансамбль из двух деревьев не даёт результат хуже любого из них
- если одно дерево обучено на полном датасете — качество ансамбля не ухудшается

---

### 8.3. Двухэтапное обучение

**Особенность stacking — двухпроходное обучение.**

#### Первый этап:
- обучаются независимые решающие деревья первого слоя
- каждое дерево оптимизирует собственный прогноз

#### Второй этап:

- формируется новый датасет из прогнозов первого слоя
- обучается итоговое решающее дерево
- его цель — найти оптимальную комбинацию прогнозов базовых моделей

---

### 8.4. Логика принятия решения

При прогнозировании:
- используются обученные деревья первого слоя
- их прогнозы подаются на вход дерева второго слоя

Итоговое дерево:
- не просто усредняет ответы
- а **выбирает наиболее точных «экспертов»**
- и их комбинации

---

### 8.5. Интерпретация метода

Stacking можно описать следующим образом:

> строим и обучаем несколько решающих схем,  
> затем обучаем решающую схему,  
> которая по результатам их прогнозов строит оптимальный итоговый прогноз

---

### 8.6. Схема обучения и прогноза

- первый этап аналогичен bagging
- второй этап:
    - формирование датасета прогнозов
    - обучение решающего дерева второго слоя

При прогнозе:
- используются модели обоих этапов
- итоговый решатель — обученная модель, а не простое голосование

---

### 8.7. Преимущества и недостатки

**Преимущества:**
- не хуже одиночного дерева
- не хуже bagging
- эффективно работает в распределённых системах

**Недостатки:**
- двухпроходное обучение
- требуется повторная обработка обучающих данных
- более высокая вычислительная стоимость

## 9. Boosting (бустинг)

### 9.1. Идея метода

**Boosting** — более эффективный ансамблевый метод по сравнению со stacking.

Ключевое отличие от stacking:
- на вход итогового решающего дерева подаются:
    - прогнозы решающих деревьев первого слоя
    - **исходные входные данные**

То есть итоговая модель обучается **одновременно**:
- на результатах других моделей
- и на исходных признаках

Следствие:
- метод более ресурсоёмкий
- но при выборе между stacking и boosting предпочтение отдаётся boosting

---

### 9.2. Обобщённое описание

Boosting можно описать так:

> строим и обучаем несколько решающих схем,  
> затем строим и обучаем решающую схему,  
> которая по прогнозам базовых моделей и исходным данным  
> строит общий оптимальный прогноз

---

### 9.3. Схема обучения

Обучение проводится в **два этапа**.

#### Первый этап:
- аналогичен bagging и stacking
- данные разбиваются на датасеты
- каждое дерево обучается независимо

#### Второй этап:
- формируется новый датасет
- в него входят:
    - прогнозы, полученные на первом этапе
    - исходные входные данные
- на этом расширенном датасете обучается итоговое решающее дерево второго слоя

---

### 9.4. Логика работы метода

Особенность boosting:
- модель второго слоя видит:
    - где именно базовые модели ошибаются
    - какие входные данные приводят к ошибкам

Следствие:
- метод выделяет плохо спрогнозированные области
- строит для них более точный прогноз

---

### 9.5. Прогнозирование

При прогнозе методом boosting:
- используются деревья обоих этапов

Результирующая схема:
- похожа на stacking
- но усилена учётом исходных данных

---

### 9.6. Преимущества и недостатки

**Преимущества:**
- не менее точен, чем stacking
- обычно даёт более точный прогноз

**Недостатки:**
- самый ресурсоёмкий из ансамблевых методов
- высокая стоимость обучения

## 10. Примеры реализации ансамблевых методов в библиотеке sklearn

Ансамблевые методы реализованы в специализированных библиотеках машинного обучения. Ниже приведены примеры их использования в библиотеке **sklearn**.

---

### 10.1. Реализация Bagging в sklearn

**Пример 15. Реализация метода Bagging**
``` python
from sklearn.ensemble import BaggingClassifier
from sklearn import tree

bagging_clf = BaggingClassifier(
tree.DecisionTreeClassifier(max_depth=2),
n_estimators=10
)

bagging_clf = bagging_clf.fit(XARR, YARR)
```
В данном примере:

- все базовые решающие схемы имеют одинаковый тип (DecisionTreeClassifier)
- различие между деревьями обеспечивается **разными подвыборками датасета**

---

### 10.2. Параметры разбиения данных в BaggingClassifier

Основные параметры, определяющие способ формирования поддатасетов:

- **max_samples** — доля объектов датасета, используемая для обучения одного дерева
- **max_features** — доля компонент входного вектора
- **bootstrap** — использовать ли выборку с возвращением для объектов
- **bootstrap_features** — использовать ли выборку с возвращением для признаков

Комбинации этих параметров позволяют реализовать различные схемы ансамблей:

- Pasting
- Bagging
- Random Subspaces
- Random Patches

---

### 10.3. Реализация Boosting (AdaBoost) в sklearn

**Пример 16. Реализация AdaBoost**
```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.datasets import make_classification
from sklearn.tree import DecisionTreeClassifier

X, y = make_classification(
n_samples=1000,
n_features=4,
n_informative=2,
n_redundant=0,
random_state=0,
shuffle=False
)

clf = AdaBoostClassifier(
DecisionTreeClassifier(max_depth=1),
n_estimators=100,
random_state=0
)

clf.fit(X, y)
clf.predict([[0, 0, 0, 0]])
clf.score(X, y)
```
---

### 10.4. Особенности AdaBoost

В отличие от рассмотренного ранее двухслойного бустинга:
- AdaBoost использует **многослойный бустинг**
- на каждом слое располагается **одна решающая схема**
- количество слоёв определяется параметром `n_estimators`

Особенности обучения:
- на вход каждой последующей модели подаются:
    - прогноз предыдущей модели
    - весь исходный датасет
- на каждом этапе обучения:
    - уменьшается вклад хорошо предсказываемых объектов
    - усиливается вклад плохо предсказываемых объектов
### 10.5 Stacking в sklearn

**Идея:** строим несколько базовых моделей (1-й слой), затем обучаем _итоговую_ модель (2-й слой), которая получает **прогнозы базовых моделей** и на их основе выдаёт общий прогноз.
**Ключевой момент (важный для экзамена):** в scikit-learn базовые модели обучаются на всём `X`, а итоговая модель обучается на **кросс-валидационных прогнозах** базовых моделей (через `cross_val_predict`), чтобы не “подглядывать” в ответы.
### Реализация для классификации: `StackingClassifier`
``` python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

estimators = [
    ("dt", DecisionTreeClassifier(max_depth=3)),
    ("svm", SVC(probability=True)),
]

stack = StackingClassifier(
    estimators=estimators,
    final_estimator=LogisticRegression(),
    cv=5,              # прогнозы базовых моделей берутся по CV
    passthrough=False  # если True — итоговая модель видит и X тоже
)

stack.fit(X_train, y_train)
y_pred = stack.predict(X_test)
```
### Реализация для регрессии: `StackingRegressor`
```python
from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import RidgeCV
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR

estimators = [
    ("dt", DecisionTreeRegressor(max_depth=4)),
    ("svr", SVR()),
]

stack = StackingRegressor(
    estimators=estimators,
    final_estimator=RidgeCV(),
    cv=5,
    passthrough=False
)

stack.fit(X_train, y_train)
y_pred = stack.predict(X_test)
```

### Параметры, которые обычно упоминают

- `estimators`: список базовых моделей `(name, estimator)`
- `final_estimator`: модель 2-го слоя (если не задана — есть дефолт)
- `cv`: схема CV для получения честных прогнозов базовых моделей
- `passthrough`: если `True`, итоговая модель получает **и прогнозы, и исходные признаки X**