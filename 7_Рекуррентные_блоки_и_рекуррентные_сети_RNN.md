
---
# Билет: Рекуррентные блоки и рекуррентные сети (RNN)

## 0) Зачем вообще RNN
**Семейство задач:** вход — **последовательность неопределённой длины**.
- текст: слово / предложение / документ;
- аудио: сигнал во времени;
- временные ряды: числовые последовательности (датчики, финансы и т.п.).

**Причина, почему обычная FFN неудобна:** стандартные сети не имеют “внутреннего состояния”, т.е. не несут контекст между шагами последовательности.

---

## 1) Ключевая идея RNN: скрытое состояние (память)
Рекуррентный блок называется рекуррентным потому что:
- после обработки элемента последовательности часть выхода возвращается на вход блока,
- формируя **скрытое состояние** `h_t`, которое переносит контекст.

**Смысл:**
- выход после последнего элемента зависит от всей истории входа,
- зависимость определяется **фиксированным набором параметров** (веса не растут с длиной последовательности),
- аналогия с **БИХ-фильтром**: механизм обновления состояния одинаковый на каждом шаге.

---

## 2) Vanilla RNN (сеть 1-к-1)
### 2.1 Структура элемента
У Vanilla-элемента:
- вход `x_t`,
- вход памяти `h_t`,
- выход памяти `h_{t+1}`,
- (опционально) выход `y_t`.

Базовые преобразования:

$$h_{t+1} = f(h_t, x_t)  $$
$$y_t = g(h_{t+1})$$

Если `f, g` реализованы нейронами:

$$
f(h, x) = φ( W0·h + W1·x + b0 )  $$
$$g(h) = ϕ( W2·h + b1 )$$


### 2.2 Что важно про параметры
- веса `W0, W1, W2` **одни и те же на всех шагах времени**,
- длина последовательности влияет на число шагов вычисления, но **не увеличивает число параметров**.

---

## 3) Недостатки Vanilla RNN (по смыслу, почему их “ломают” LSTM/GRU)
1) **Параллелизация слабая:** нужно пройти шаги последовательно → медленнее CNN/Transformer.
2) **Сложно балансировать мощность и сходимость:** обучение чувствительно к настройкам.
3) **Критичная проблема:** затухание/взрыв градиента при длинных зависимостях.

---

## 4) Проблема затухания и взрыва градиента
Рассмотрим упрощённый скалярный случай:
$$
f(h, x) = f(W0·h + W1·x)  $$
$$g(h) = W2·h$$


При вычислении градиента по параметру `w` (коэффициенту, влияющему на рекурсию) возникает произведение производных по шагам:

- градиент содержит фактор вида:
$$
∏ (df_j(arg)/darg)

$$
### 4.1 Взрыв градиента
Если:
$$
|df/darg| > 1
$$
то произведение растёт **геометрически** → градиент огромный, обучение нестабильно.
Эффект: ранние элементы могут “перезадавить” всё остальное, обучение становится кривым (вплоть до divergence).

Частичная защита:
- **gradient clipping** (ограничение ∂LOSS/∂w по порогу), но это “костыль”, не исправляет природу проблемы.

### 4.2 Затухание градиента
Если:
$$
|df/darg| < 1
$$
(типично для `tanh`, `sigmoid`), произведение падает геометрически → градиент стремится к 0.
Эффект: сеть “забывает” ранние элементы последовательности → плохо учит дальние зависимости → недообучение.

### 4.3 Вывод
Для длинных последовательностей Vanilla RNN плохо держит “долгую память”.
Нужно сделать так, чтобы “путь градиента” мог оставаться **по модулю близким к 1**.

---

## 5) Решение: модифицированные рекуррентные элементы

## 5.1 LSTM (Long Short-Term Memory)
**Идея:** вводится отдельная долговременная память `c_t` (cell state), управление которой делается воротами (gates).

Ключевое линейное обновление:
$$
c_t = f_t · c_{t-1} + i_t · g_t
$$

где:
- направление внутреннего состояния:

$$g_t = tanh( W0·x_t + W1·h_{t-1} + b0 ) , -1 < g_t < 1$$

- “впуск” (input gate):

$$i_t = σ( W2·x_t + W3·h_{t-1} + b1 ) , 0 < i_t < 1$$

- “забывание” (forget gate):

$$f_t = σ( W4·x_t + W5·h_{t-1} + b2 ) , 0 < f_t < 1$$


Выход `h_t` формируется как функция от `c_t` и текущих сигналов (деталь зависит от реализации).

**Почему это помогает:**
- память `c_t` содержит “сквозной” путь (по сути, контролируемая линейность),
- градиент может проходить через `c_t` без убийственного затухания/взрыва,
- сеть лучше учит дальние зависимости.

**Минус:** много параметров:
$$
LSTM: 8 d^2 + 4 d
$$
где `d` — размерность входа.

---

## 5.2 GRU (Gated Recurrent Unit)
**Цель:** приблизить мощность LSTM, но с меньшим числом параметров.
- объединяет состояние и выход, сокращает линии памяти.

Упрощённая форма (идея балансирования памяти входом):

$$h_t = z_t·h_{t-1} + (1 - z_t)·g_t  $$
$$z_t = W·x_t
$$

Число параметров:
$$
GRU: 6 d^2 + 6 d
$$
---

## 5.3 SRU (Simple Recurrent Unit)
**Цель:** ещё меньше параметров + частичная параллелизация.
- состояние и выход разведены,
- часть вычислений можно распараллелить.

Число параметров:
$$
SRU: 3 d^2 + 4 d
$$
---

## 6) Формы соединения RNN-блоков (X-к-Y)
Обозначение: `X-to-Y`, где:
- X — размерность входа (скаляр/вектор),
- Y — размерность выхода.

Основные схемы:
1) **1-to-1 (Vanilla):** один вход → один выход на шаге.
2) **1-to-many:** один вход → последовательность выходов (генерация).
3) **many-to-1:** последовательность → один итоговый выход (классификация текста).
4) **many-to-many:** последовательность → последовательность (seq2seq, разметка).

### Бинаправленная RNN (Bidirectional)
Два независимых RNN:
- одна идёт слева направо,
- вторая справа налево,
- их выходы комбинируются (склейка/сумма/функция).

Смысл:
- зависимость выхода не только от прошлого, но и от будущего контекста.

---

## 7) Как RNN используют в задачах NLP и временных рядов

## 7.1 Векторизация входа
Для текста входные элементы сначала переводятся в числа, обычно через эмбеддинги:
- предобученные (Word2Vec/GloVe/FastText),
- или обучаемые с нуля embedding-слоем.

---

## 7.2 Классификация последовательности (many-to-1)
**Цель:** всей последовательности назначить одну метку (тональность, тема, язык).

Схема:
1) Токены → эмбеддинги `x_1...x_T`.
2) RNN обновляет состояния `h_1...h_T`.
3) Получаем “итоговое представление” последовательности (агрегация):
   - `h_T` (последнее состояние),
   - pooling по всем `h_t` (mean/max),
   - attention (взвешенная сумма состояний).
4) Полносвязный слой + softmax → вероятности классов.

---

## 7.3 Генерация последовательностей (many-to-many / autoregressive)
### Языковое моделирование (next-token prediction)
Обучение (teacher forcing):
- на вход: `[<SOS>, tok1, tok2, ...]`
- на цель: `[tok1, tok2, tok3, ..., <EOS>]`

Сеть учит:
```
P(next_token | previous_tokens)
```

Инференс (генерация):
- стартуем с `<SOS>`,
- предсказываем распределение по токенам,
- выбираем токен:
  - greedy (max),
  - sampling (по распределению),
  - beam search (N-гипотез),
- повторяем до `<EOS>` или лимита длины.

---

## 8) Обучение RNN: BPTT (Backpropagation Through Time)
Обычный backprop для сети по слоям превращается в backprop “по времени”:
1) последовательность прогоняется вперёд,
2) сеть разворачивается во времени как цепочка одинаковых блоков,
3) ошибка распространяется назад по этой цепочке.

Практический момент:
- длинные последовательности часто обучают “окнами” (20–50 шагов),
- скрытое состояние переносят между окнами (чтобы сохранять контекст).

---

## 9) Плюсы и ограничения RNN (как итог билета)
### Плюсы
- естественно подходят для последовательностей,
- используют контекст через скрытое состояние,
- могут работать в потоковом режиме (по шагам),
- LSTM/GRU держат долгие зависимости лучше Vanilla.

### Ограничения
- Vanilla плохо держит длинные зависимости (vanishing/exploding gradients),
- последовательность → хуже параллелится → медленнее современных архитектур,
- сложнее настройка (архитектура, шаг обучения, стабилизация),
- более сложные блоки = больше параметров и времени.

---

## 10) Важная связь с тем, что уже было в конспекте (классификация)
RNN в задачах текста часто заканчивается обычным классификатором:
- если бинарная метка → sigmoid + BCE,
- если N классов → softmax + CE.

То есть RNN решает “как получить представление последовательности”, а **финальная голова** решает “как классифицировать”.

---