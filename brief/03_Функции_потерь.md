
---

## üìÑ `03_–§—É–Ω–∫—Ü–∏–∏_–ø–æ—Ç–µ—Ä—å.md`

---
tags: [loss, machine-learning]
---

# üéØ –§—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å

–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏–∑–º–µ—Ä—è–µ—Ç –æ—à–∏–±–∫—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏.

## üîπ MSE ‚Äî Mean Squared Error
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏:
$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n}(y_i - \hat{y}_i)^2
$$

## üîπ BCE ‚Äî Binary Cross Entropy
–î–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:
$$
\text{BCE} = -\frac{1}{n} \sum
\left[
y \ln(\hat{y}) + (1 - y)\ln(1 - \hat{y})
\right]
$$

## üîπ Cross-Entropy
–î–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏  
–æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å Softmax

$$
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
$$
