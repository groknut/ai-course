
---

# Машинное обучение и работа с изображениями: Понятный гид

## 1. Градиентный спуск: Как компьютер «учится»
**Теория:** Представьте, что вы стоите на вершине горы в густом тумане, и ваша задача — спуститься в самую низкую долину. Вы не видите всей картины, но чувствуете наклон поверхности под ногами. Вы делаете шаг в сторону наибольшего спуска, затем снова оцениваете наклон и так до тех пор, пока не окажетесь внизу. 

В математике этот наклон называется **производной** (для одной переменной) или **градиентом** (для многих переменных). Чтобы двигаться к минимуму, мы вычитаем значение производной из текущей координаты, используя **скорость обучения ($\lambda$ или $\eta$)** — коэффициент, определяющий размер нашего шага.

```python
# Реализация спуска для функции f(x) = x^2 - 6x + 5
def dfdx(x): 
    return 2*x - 6 # Это формула наклона (производная)

xc = -4  # Наша случайная начальная точка на горе
lr = 0.1 # Размер шага (скорость обучения)

for _ in range(200):
    # Обновляем положение: текущая точка минус (шаг * наклон)
    xc = xc - lr * dfdx(xc) 

print(f'Мы спустились в точку: {xc}') # Ожидаем результат близкий к 3
```

## 2. Линейная регрессия: Поиск закономерностей
**Теория:** Это поиск прямой линии, которая лучше всего проходит через облако точек. Мы предполагаем, что ответ ($y$) линейно зависит от признаков ($x$). Чтобы понять, насколько хороша наша линия, мы используем **среднеквадратичную ошибку (MSE)**: считаем разницу между предсказанием и реальностью, возводим в квадрат и усредняем.

```python
from sklearn.linear_model import LinearRegression
import numpy as np

# Данные (например, рост и вес)
x = np.array().reshape((-1, 1))
y = np.array()

model = LinearRegression() # Создаем "пустую" модель
model.fit(x, y)            # Модель подбирает веса так, чтобы линия была идеальной

# Получаем коэффициенты прямой y = w1*x + w0
w0, w1 = model.intercept_, model.coef_
```

## 3. Оценка классификации: Как не обмануть себя
**Теория:** Просто считать долю правильных ответов (**Accuracy**) опасно, особенно если данных одного класса намного больше. Для этого используют **Матрицу ошибок**, которая делит ответы на:
*   **TP**: Верно угадали "положительный" объект.
*   **FP**: Ошибочно назвали "отрицательный" объект "положительным".
*   **FN**: Пропустили "положительный" объект.
*   **TN**: Верно угадали "отрицательный" объект.

**Основные метрики:**
*   **Precision (Точность):** Можно ли верить модели, когда она говорит «да»?
*   **Recall (Полнота):** Все ли объекты этого класса мы нашли?
*   **F-мера:** Золотая середина между точностью и полнотой.

## 4. Логистическая регрессия: Классификация через вероятность
**Теория:** Несмотря на название, это метод **классификации**. Модель предсказывает не число, а вероятность того, что объект принадлежит к классу (например, «спам» или «не спам»). Для этого используется **сигмоида** — функция, которая превращает любое число в значение от 0 до 1.

## 5. Деревья решений и Ансамбли
**Теория:** 
*   **Дерево решений** — это цепочка вопросов (если признак $X > 5$, то идем влево). Они понятны, но легко «зазубривают» обучающие данные (переобучаются), выдавая плохие результаты на новых примерах.
*   **Случайный лес (Random Forest)** — это когда мы берем много деревьев и заставляем их голосовать. Чтобы деревья были разными, каждое обучается на своем случайном наборе данных (**бутстрап**) и случайных признаках.
*   **Градиентный бустинг** — деревья строятся по очереди: каждое новое дерево исправляет ошибки предыдущих.

```python
from sklearn.ensemble import RandomForestClassifier

# Создаем лес из 100 деревьев
rfc = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42)
# Обучение модели
# rfc.fit(X_train, y_train) 
```

## 6. Кластеризация (K-means): Группировка без подсказок
**Теория:** Это обучение «без учителя», когда у нас нет правильных ответов. Алгоритм сам находит «сгустки» (кластеры) данных.
1. Выбираем количество групп ($k$) и ставим случайные центры (центроиды).
2. Каждая точка примыкает к ближайшему центру.
3. Центры перемещаются в середину своих новых групп.
4. Повторяем, пока центры не перестанут двигаться.

## 7. Работа с изображениями (Computer Vision)
**Теория:** Изображение для компьютера — это просто большая таблица (матрица) с числами. Каждое число — яркость пикселя. В цветном фото таких таблиц три: для Красного, Зеленого и Синего цветов (RGB).
Важный инструмент — **свертка**. Это маленькая матрица-фильтр, которая «скользит» по фото и подчеркивает детали: границы, углы или размытие.

```python
import cv2
import matplotlib.pyplot as plt

# Загружаем фото
img = cv2.imread('image.jpg')
# OpenCV читает в формате BGR, меняем на привычный RGB для отображения
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# Размытие по Гауссу (делает картинку мягче)
blur = cv2.GaussianBlur(img_rgb, (15, 15), 0)

# Выделение границ с помощью фильтра Собеля
edges = cv2.Sobel(img_rgb, dx=1, dy=0, ksize=3, ddepth=-1)

plt.imshow(edges)
plt.show()
```

---

Вот компактная шпаргалка, которая поможет вам и вашим друзьям быстро переходить от теории к практике. В ней собраны ключевые формулы и примеры кода для основных задач машинного обучения и обработки изображений.

---

# Шпаргалка по машинному обучению и Computer Vision

## 1. Базовая оптимизация: Градиентный спуск
Используется для минимизации функции потерь путем движения в сторону, противоположную градиенту.

*   **Формула:** $x_{n+1} = x_n - \lambda \cdot f'(x_n)$, где $\lambda$ — скорость обучения (learning rate).
*   **Проблема:** Может застрять в локальном минимуме вместо глобального.

```python
# Один итерационный шаг обновления параметра
xc = xc - lr * dfdx(xc) # xc — текущее значение, lr — шаг, dfdx — производная
```

## 2. Регрессия и Классификация
| Алгоритм | Суть | Формула / Метрика |
| :--- | :--- | :--- |
| **Линейная регрессия** | Поиск прямой $y = w_0 + w_1x$. | **MSE:** $\frac{1}{l}\sum (a(x_i)-y_i)^2$. |
| **Логистическая регрессия** | Предсказание вероятности класса. | **Сигмоида:** $\sigma(z) = \frac{1}{1 + e^{-z}}$. |
| **kNN** | Класс определяется по $k$ ближайшим соседям. | **Евклидова метрика:** $\sqrt{\sum(x_i-y_i)^2}$. |

**Метрики качества классификации:**
*   **Accuracy:** Доля верных ответов (плохо работает на несбалансированных данных).
*   **Precision (Точность):** $\frac{TP}{TP+FP}$ (доверие к ответам модели).
*   **Recall (Полнота):** $\frac{TP}{TP+FN}$ (способность находить объекты класса).
*   **F-мера:** $2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$ (баланс между P и R).

## 3. Деревья решений и Ансамбли
*   **Дерево решений:** Строится путем разбиения данных по условию $[x^j \leq t]$ так, чтобы минимизировать критерий информативности (например, индекс Джини).
*   **Случайный лес (Random Forest):** Ансамбль независимых деревьев. Использует **Bootstrap** (случайные выборки с возвращением) и случайные подмножества признаков для каждого разбиения.
*   **Градиентный бустинг (Boosting):** Последовательное построение деревьев, где каждое новое исправляет ошибки (остатки) предыдущих.

```python
from sklearn.ensemble import RandomForestClassifier
# Пример инициализации с ключевыми параметрами
model = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_leaf=3)
```

## 4. Кластеризация: K-means (Обучение без учителя)
Разбивает данные на $k$ групп («сгустков»).
1. Случайная инициализация центроидов.
2. Привязка точек к ближайшему центру.
3. Пересчет центров как среднего арифметического точек кластера.

## 5. Обработка изображений (OpenCV)
Изображение — это матрица $H \times W \times C$ (Высота, Ширина, Каналы).

*   **Цветовые пространства:** RGB, BGR (в OpenCV), GrayScale, HSV.
*   **Свертка:** Операция фильтрации изображения ядром $K$ для выделения признаков.

```python
import cv2
img = cv2.imread('file.jpg') # Загрузка в BGR
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Конвертация

# Полезные инструменты:
edges = cv2.Sobel(img_rgb, dx=1, dy=0, ksize=3, ddepth=-1) # Выделение границ
blur = cv2.GaussianBlur(img_rgb, (15, 15), 0) # Размытие
resized = cv2.resize(img_rgb, (100, 100)) # Изменение размера
```

---
